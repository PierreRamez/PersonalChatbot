{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8EsYo9Z2rRc"
      },
      "source": [
        "##Run those cells if you have any problem with the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KqF34Bq6-LXI"
      },
      "outputs": [],
      "source": [
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JgXcyeYT9wyX"
      },
      "outputs": [],
      "source": [
        "# # # First, clear the current model from memory\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# import gc\n",
        "# gc.collect()\n",
        "\n",
        "# # # Check memory freed\n",
        "# !nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83HI6praRlmZ"
      },
      "source": [
        "# this is our start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-gyw-1fcih3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98314071-c5b0-4094-e2e1-211a2a83e86c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WORKDIR: /content/personalized_chatbot\n",
            "DATA_DIR: /content/personalized_chatbot/data\n",
            "FINETUNE_PREP: /content/personalized_chatbot/finetune_prep\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------\n",
        "# CLEAN PROJECT FOLDER SETUP\n",
        "# ---------------------------\n",
        "# Why this is here:\n",
        "# - Every Colab session is clean.\n",
        "# - We need a stable folder to store data, logs, HITL feedback.\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "WORKDIR = Path(\"/content/personalized_chatbot\")\n",
        "WORKDIR.mkdir(exist_ok=True)\n",
        "\n",
        "DATA_DIR = WORKDIR / \"data\"\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "HITL_FILE = WORKDIR / \"feedback.jsonl\"      # where Human-in-the-Loop corrections go\n",
        "FINETUNE_PREP = WORKDIR / \"finetune_prep\"\n",
        "FINETUNE_PREP.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"WORKDIR:\", WORKDIR)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"FINETUNE_PREP:\", FINETUNE_PREP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lLoSpVcaJ3e-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "840bfd2f-6637-4b15-c77a-99f079830c21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extended MLOps directories:\n",
            "  Processed IDs: /content/personalized_chatbot/processed_feedback_ids.json\n",
            "  Training batches: /content/personalized_chatbot/finetune_prep\n"
          ]
        }
      ],
      "source": [
        "# Cell 1A: Extended directory structure for MLOps pipeline (FIXED)\n",
        "# Place this RIGHT AFTER your existing Cell 1 (folder setup)\n",
        "\n",
        "# Create additional directories for pipeline\n",
        "PROCESSED_IDS_FILE = WORKDIR / \"processed_feedback_ids.json\"\n",
        "\n",
        "# Use FINETUNE_PREP from your original notebook instead of creating BATCH_DIR\n",
        "if 'BATCH_DIR' not in globals():\n",
        "    BATCH_DIR = FINETUNE_PREP  # Use existing directory\n",
        "\n",
        "BATCH_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Create empty processed IDs file if doesn't exist\n",
        "if not PROCESSED_IDS_FILE.exists():\n",
        "    import json\n",
        "    with open(PROCESSED_IDS_FILE, 'w') as f:\n",
        "        json.dump([], f)\n",
        "\n",
        "print(\"Extended MLOps directories:\")\n",
        "print(f\"  Processed IDs: {PROCESSED_IDS_FILE}\")\n",
        "print(f\"  Training batches: {BATCH_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fx9Mn_AUgskz"
      },
      "outputs": [],
      "source": [
        "# LLaMA inference (transformers + bitsandbytes)\n",
        "# LoRA (peft)\n",
        "# dataset management (datasets)\n",
        "# orchestration (langchain)\n",
        "!pip install -q transformers accelerate bitsandbytes peft datasets langchain sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Lv7N_Lf4KB5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e8ce86-bfd6-4a2b-a76a-8d53fa41d628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Pipeline dependencies installed\n",
            "  Note: Using lightweight pipeline tracking instead of Prefect\n"
          ]
        }
      ],
      "source": [
        "# Cell 2A: Install MLOps dependencies (FIXED for Colab)\n",
        "# Place this RIGHT AFTER your existing Cell 2 (pip install)\n",
        "\n",
        "# Prefect has dependency issues on Colab, so we'll use a simpler orchestration approach\n",
        "# We'll track pipeline runs manually with timestamps and logs\n",
        "\n",
        "# Install only what we absolutely need\n",
        "!pip install -q datasets pandas\n",
        "\n",
        "print(\"✓ Pipeline dependencies installed\")\n",
        "print(\"  Note: Using lightweight pipeline tracking instead of Prefect\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dWPoUUq_TLk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "956abc40-8a35-4de1-c119-49184420c5f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Pipeline orchestration ready\n",
            "  Logs directory: /content/personalized_chatbot/pipeline_logs\n"
          ]
        }
      ],
      "source": [
        "# Cell 2B: Simple Pipeline Orchestration (Prefect Alternative)\n",
        "# Place after Cell 2A\n",
        "\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Callable\n",
        "import traceback\n",
        "\n",
        "class PipelineRun:\n",
        "    \"\"\"\n",
        "    Lightweight pipeline orchestration without external dependencies.\n",
        "\n",
        "    Tracks:\n",
        "    - Task execution times\n",
        "    - Success/failure status\n",
        "    - Error messages\n",
        "    - Run metadata\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name: str, log_dir: Path):\n",
        "        self.name = name\n",
        "        self.log_dir = log_dir\n",
        "        self.log_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        self.run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.start_time = time.time()\n",
        "        self.tasks = []\n",
        "        self.status = \"running\"\n",
        "\n",
        "    def run_task(self, task_name: str, task_func: Callable, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Execute a task with error handling and logging.\n",
        "\n",
        "        Args:\n",
        "            task_name: Human-readable task name\n",
        "            task_func: Function to execute\n",
        "            *args, **kwargs: Arguments to pass to task_func\n",
        "\n",
        "        Returns:\n",
        "            Task result if successful, None if failed\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Task: {task_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        task_start = time.time()\n",
        "        task_record = {\n",
        "            \"name\": task_name,\n",
        "            \"start_time\": task_start,\n",
        "            \"status\": \"running\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Execute task\n",
        "            result = task_func(*args, **kwargs)\n",
        "\n",
        "            # Record success\n",
        "            task_record[\"status\"] = \"success\"\n",
        "            task_record[\"duration\"] = time.time() - task_start\n",
        "            task_record[\"result_summary\"] = str(result)[:200] if result else \"None\"\n",
        "\n",
        "            print(f\"Task completed in {task_record['duration']:.2f}s\")\n",
        "\n",
        "            self.tasks.append(task_record)\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            # Record failure\n",
        "            task_record[\"status\"] = \"failed\"\n",
        "            task_record[\"duration\"] = time.time() - task_start\n",
        "            task_record[\"error\"] = str(e)\n",
        "            task_record[\"traceback\"] = traceback.format_exc()\n",
        "\n",
        "            print(f\"Task FAILED: {e}\")\n",
        "\n",
        "            self.tasks.append(task_record)\n",
        "            self.status = \"failed\"\n",
        "\n",
        "            return None\n",
        "\n",
        "    def finish(self):\n",
        "        \"\"\"Complete the pipeline run and save log\"\"\"\n",
        "        self.duration = time.time() - self.start_time\n",
        "\n",
        "        if self.status != \"failed\":\n",
        "            self.status = \"completed\"\n",
        "\n",
        "        # Save run log\n",
        "        log_file = self.log_dir / f\"run_{self.run_id}.json\"\n",
        "\n",
        "        run_log = {\n",
        "            \"run_id\": self.run_id,\n",
        "            \"pipeline\": self.name,\n",
        "            \"status\": self.status,\n",
        "            \"start_time\": datetime.fromtimestamp(self.start_time).isoformat(),\n",
        "            \"duration_seconds\": self.duration,\n",
        "            \"tasks\": self.tasks\n",
        "        }\n",
        "\n",
        "        with open(log_file, 'w') as f:\n",
        "            json.dump(run_log, f, indent=2)\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"PIPELINE SUMMARY\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Run ID: {self.run_id}\")\n",
        "        print(f\"Status: {self.status.upper()}\")\n",
        "        print(f\"Duration: {self.duration:.2f}s\")\n",
        "        print(f\"Tasks: {len(self.tasks)}\")\n",
        "\n",
        "        for task in self.tasks:\n",
        "            status_label = \"[SUCCEEDED]\" if task[\"status\"] == \"success\" else \"[FAILED]\"\n",
        "            print(f\"  {status_label} {task['name']}: {task.get('duration', 0):.2f}s\")\n",
        "\n",
        "        print(f\"\\nLog saved: {log_file}\")\n",
        "\n",
        "        return run_log\n",
        "\n",
        "# Create pipeline logs directory\n",
        "PIPELINE_LOGS = WORKDIR / \"pipeline_logs\"\n",
        "PIPELINE_LOGS.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"✓ Pipeline orchestration ready\")\n",
        "print(f\"  Logs directory: {PIPELINE_LOGS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we will be using the 3B model since we are working on a CPU backend"
      ],
      "metadata": {
        "id": "jaLK5viGO2MO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NZTNJuXMg3Wu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "619e16d6e21f41bcbd85a1518b0ee4af",
            "940745e973384a639f83c23ef2f4e589",
            "cf696f33d4224c8ca918f308d290b9e7",
            "d30cde73f3a045fe9d9c9f9ce3c084d6",
            "5748def886f94e359b0951ad15357306",
            "a9173a9fffd04d89a6e159c58ec5c791",
            "c49cc03945a145b29a6ab6bcc5f129e5",
            "535c6abde8df4edba6e7eba5769c4db0",
            "c52f7d8cfba942488ee722a57ec0513e",
            "c2c6b9b47af44127825d688d1d972ab9",
            "9e885cb387a0436b8f7d3b21867681cf"
          ]
        },
        "outputId": "1e0b7602-4f8e-4d23-d3c7-cef89be2bcda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "619e16d6e21f41bcbd85a1518b0ee4af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "\n",
        "MODEL = 'pierreramez/Llama-3.2-3B-Instruct-bnb-4bit_finetuned' # 3B model\n",
        "# MODEL = 'pierreramez/llama3.1-finetuned-v2' # 8B model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=False)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True # Enable CPU offload for 32-bit modules\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "                                             MODEL,\n",
        "                                             quantization_config=bnb_config, # Use BitsAndBytesConfig for quantization\n",
        "                                             device_map={ '': 0 }, # Explicitly map all layers to GPU 0\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             trust_remote_code=True #required for llama\n",
        "                                            )\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# text gen pipeline\n",
        "pipe = pipeline(\n",
        "    'text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.2, # low temp to make it more deterministic\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "print('Model loaded successfully!\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7hgW0mB2jk47"
      },
      "outputs": [],
      "source": [
        "def generate_reply(user_input, history, max_turns=4):\n",
        "    \"\"\"\n",
        "    history = list of dicts: [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}, ...]\n",
        "    \"\"\"\n",
        "    # Truncate history to last max_turns exchanges\n",
        "    truncated_history = history[-(max_turns * 2):]\n",
        "\n",
        "    # Add new user message\n",
        "    messages = truncated_history + [{\"role\": \"user\", \"content\": user_input}]\n",
        "\n",
        "    # Apply the SAME chat template used during training\n",
        "    # Ensure inputs are moved to the model's device\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    attention_mask = (inputs != tokenizer.pad_token_id).long()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids=inputs,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=128,\n",
        "            use_cache=True,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    # Decode only new tokens\n",
        "    response = tokenizer.decode(output[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A3OM2VFQq7o0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fca3f3d-eec9-4f5c-b73a-326b76dee7e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'prompt': 'user: Explain normalization.',\n",
              "  'response': 'Normalization rescales features to stable ranges.'},\n",
              " {'prompt': 'user: Explain normalization. assistant: Normalization rescales features to stable ranges. user: Show formula.',\n",
              "  'response': 'z = (x - µ) / σ'}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import re, json, html\n",
        "\n",
        "def clean_text(s):\n",
        "  s = html.unescape(s)\n",
        "  s= re.sub(r'\\s+',' ',s).strip()\n",
        "  return s\n",
        "\n",
        "def chat_to_pairs(chat_log, max_user_context=4):\n",
        "  '''\n",
        "  chat_log is like:\n",
        "  [\n",
        "    {role: 'user', content: '...'},\n",
        "    {role: 'assistant', content: '...'},\n",
        "    ...\n",
        "  ]\n",
        "\n",
        "  We convert multi-turn chat into supervised training pairs.\n",
        "  '''\n",
        "\n",
        "  pairs = []\n",
        "  for i in range(len(chat_log) - 1):\n",
        "      if chat_log[i][\"role\"] == \"user\" and chat_log[i+1][\"role\"] == \"assistant\":\n",
        "          # Build concise prompt\n",
        "          ctx_start = max(0, i - max_user_context*2)\n",
        "          ctx = chat_log[ctx_start:i+1]\n",
        "\n",
        "          prompt = \" \".join(f\"{t['role']}: {clean_text(t['content'])}\" for t in ctx)\n",
        "          response = clean_text(chat_log[i+1][\"content\"])\n",
        "\n",
        "          pairs.append({\"prompt\": prompt, \"response\": response})\n",
        "\n",
        "  return pairs\n",
        "\n",
        "example_chat = [\n",
        "    {\"role\":\"user\",\"content\":\"Explain normalization.\"},\n",
        "    {\"role\":\"assistant\",\"content\":\"Normalization rescales features to stable ranges.\"},\n",
        "    {\"role\":\"user\",\"content\":\"Show formula.\"},\n",
        "    {\"role\":\"assistant\",\"content\":\"z = (x - µ) / σ\"}\n",
        "]\n",
        "\n",
        "pairs = chat_to_pairs(example_chat)\n",
        "pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nzyhgaU1uRf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3317f221-ca04-4b43-c6ec-8759b6e9d84a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved training pairs to: /content/personalized_chatbot/data/train_pairs.jsonl\n"
          ]
        }
      ],
      "source": [
        "out_path = DATA_DIR / \"train_pairs.jsonl\"\n",
        "\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for p in pairs:\n",
        "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Saved training pairs to:\", out_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgG3QBCJvwI3"
      },
      "source": [
        "## Human in the loop (HITL) pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BExu6FeauabM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c659160-69ef-474b-af8e-ced649fd0650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HITL pipeline ready!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def save_interaction(user_input, model_reply, user_correction=None, reason=None):\n",
        "    \"\"\"\n",
        "    Append a single interaction to feedback.jsonl\n",
        "    The model learns from mistakes later.\n",
        "    \"\"\"\n",
        "    rec = {\n",
        "        \"time\": time.time(),\n",
        "        \"user_input\": user_input,\n",
        "        \"model_reply\": model_reply,\n",
        "        \"user_correction\": user_correction,\n",
        "        \"accepted\": user_correction is None,\n",
        "        \"reason\": reason,\n",
        "    }\n",
        "\n",
        "    with open(HITL_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return rec\n",
        "\n",
        "print(\"HITL pipeline ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ktk_1oeuKMZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36dd22a6-47f0-4c83-cf69-4af46de7149a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Fixed FeedbackManager initialized\n",
            "   Tracking: /content/personalized_chatbot/feedback.jsonl\n",
            "   Already processed: 11 interactions\n"
          ]
        }
      ],
      "source": [
        "# Cell 7A: FIXED FeedbackManager (REPLACE the old one completely)\n",
        "# Place after Cell 7 (save_interaction)\n",
        "\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "class FeedbackManager:\n",
        "    \"\"\"\n",
        "    Fixed version that separates reading from processing.\n",
        "\n",
        "    Key fix: get_new_corrections() now does NOT mark items as processed.\n",
        "    Only mark_as_processed() does that, which we call AFTER successful training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feedback_file: Path, processed_ids_file: Path):\n",
        "        self.feedback_file = feedback_file\n",
        "        self.processed_ids_file = processed_ids_file\n",
        "        self.processed_ids = self._load_processed_ids()\n",
        "\n",
        "    def _load_processed_ids(self) -> set:\n",
        "        \"\"\"Load IDs of feedback already used for training\"\"\"\n",
        "        if not self.processed_ids_file.exists():\n",
        "            return set()\n",
        "\n",
        "        try:\n",
        "            with open(self.processed_ids_file, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                return set(data) if isinstance(data, list) else set()\n",
        "        except:\n",
        "            return set()\n",
        "\n",
        "    def _save_processed_ids(self):\n",
        "        \"\"\"Persist processed IDs to disk\"\"\"\n",
        "        with open(self.processed_ids_file, 'w') as f:\n",
        "            json.dump(list(self.processed_ids), f)\n",
        "\n",
        "    def _generate_feedback_id(self, interaction: Dict) -> str:\n",
        "        \"\"\"Create unique ID from interaction\"\"\"\n",
        "        content = f\"{interaction['user_input']}{interaction['time']}\"\n",
        "        return hashlib.md5(content.encode()).hexdigest()\n",
        "\n",
        "    def get_new_corrections(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Extract NEW corrections (not yet processed).\n",
        "\n",
        "        CRITICAL: This does NOT mark them as processed!\n",
        "        Call mark_as_processed() after successful training.\n",
        "        \"\"\"\n",
        "        if not self.feedback_file.exists():\n",
        "            return []\n",
        "\n",
        "        new_corrections = []\n",
        "\n",
        "        with open(self.feedback_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    interaction = json.loads(line.strip())\n",
        "\n",
        "                    # Generate ID\n",
        "                    interaction_id = self._generate_feedback_id(interaction)\n",
        "\n",
        "                    # Skip if already processed\n",
        "                    if interaction_id in self.processed_ids:\n",
        "                        continue\n",
        "\n",
        "                    # Only corrections (accepted=False means user corrected)\n",
        "                    if interaction.get('accepted') is False and interaction.get('user_correction'):\n",
        "                        new_corrections.append({\n",
        "                            'id': interaction_id,\n",
        "                            'prompt': interaction['user_input'],\n",
        "                            'response': interaction['user_correction'],\n",
        "                            'timestamp': interaction['time'],\n",
        "                            'reason': interaction.get('reason', 'user_correction')\n",
        "                        })\n",
        "\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "        return new_corrections\n",
        "\n",
        "    def mark_as_processed(self, correction_ids: List[str]):\n",
        "        \"\"\"\n",
        "        Mark corrections as processed after successful training.\n",
        "\n",
        "        Call this ONLY after training completes successfully.\n",
        "        \"\"\"\n",
        "        self.processed_ids.update(correction_ids)\n",
        "        self._save_processed_ids()\n",
        "        print(f\"Marked {len(correction_ids)} corrections as processed\")\n",
        "\n",
        "# Initialize feedback manager\n",
        "feedback_mgr = FeedbackManager(\n",
        "    feedback_file=HITL_FILE,\n",
        "    processed_ids_file=PROCESSED_IDS_FILE\n",
        ")\n",
        "\n",
        "print(\"  Fixed FeedbackManager initialized\")\n",
        "print(f\"   Tracking: {HITL_FILE}\")\n",
        "print(f\"   Already processed: {len(feedback_mgr.processed_ids)} interactions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "e2IIlDspKZVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00d3d8ce-27f1-4c6d-9b39-4a0b03822f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training data preparator READY\n",
            "  Output directory: /content/personalized_chatbot/finetune_prep\n"
          ]
        }
      ],
      "source": [
        "# Cell 7B: Training Data Preparation\n",
        "# Place RIGHT AFTER Cell 7A (Feedback Manager)\n",
        "\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "class TrainingDataPreparator:\n",
        "    \"\"\"\n",
        "    Converts feedback corrections into model training format.\n",
        "\n",
        "    Critical: Must match the instruction format your model was trained on.\n",
        "    Your model expects: instruction + input + output structure.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dir: Path):\n",
        "        self.output_dir = output_dir\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    def prepare_training_batch(self, corrections: List[Dict]) -> Optional[Path]:\n",
        "        \"\"\"\n",
        "        Transform corrections into training-ready format.\n",
        "\n",
        "        Format:\n",
        "        - instruction: The user's question/prompt\n",
        "        - input: Empty for conversational models\n",
        "        - output: The corrected response\n",
        "        \"\"\"\n",
        "        if not corrections:\n",
        "            print(\"No corrections to prepare\")\n",
        "            return None\n",
        "\n",
        "        # Convert to instruction-following format\n",
        "        training_examples = []\n",
        "        for corr in corrections:\n",
        "            training_examples.append({\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"user\", \"content\": corr[\"prompt\"]},\n",
        "                    {\"role\": \"assistant\", \"content\": corr[\"response\"]}\n",
        "                ],\n",
        "                \"metadata\": {\n",
        "                    \"id\": corr[\"id\"],\n",
        "                    \"timestamp\": corr[\"timestamp\"],\n",
        "                    \"reason\": corr.get(\"reason\", \"user_correction\")\n",
        "                }\n",
        "            })\n",
        "\n",
        "        # Save as JSONL with timestamp\n",
        "        batch_file = self.output_dir / f\"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\"\n",
        "\n",
        "        with open(batch_file, 'w', encoding='utf-8') as f:\n",
        "            for example in training_examples:\n",
        "                f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        print(f\"✓ Prepared {len(training_examples)} training examples\")\n",
        "        print(f\"  Saved to: {batch_file}\")\n",
        "\n",
        "        return batch_file\n",
        "\n",
        "    def create_huggingface_dataset(self, jsonl_file: Path) -> Dataset:\n",
        "        \"\"\"\n",
        "        Load prepared JSONL into HuggingFace Dataset.\n",
        "        This is what the Trainer expects.\n",
        "        \"\"\"\n",
        "        data = []\n",
        "        with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                data.append(json.loads(line))\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        dataset = Dataset.from_pandas(df)\n",
        "\n",
        "        print(f\"✓ HuggingFace dataset: {len(dataset)} examples\")\n",
        "        return dataset\n",
        "\n",
        "# Initialize preparator\n",
        "data_prep = TrainingDataPreparator(output_dir=BATCH_DIR)\n",
        "\n",
        "print(\" Training data preparator READY\")\n",
        "print(f\"  Output directory: {BATCH_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YaVNxwItKdFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00657ce2-d1cb-4aa4-f986-ccb1e56c181a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Pipeline tasks defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 7C: Pipeline Tasks (Updated for lightweight orchestration)\n",
        "# Place after Cell 7B\n",
        "\n",
        "def collect_new_feedback() -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Task: Extract new corrections from feedback log.\n",
        "    \"\"\"\n",
        "    print(\" Collecting feedback...\")\n",
        "    corrections = feedback_mgr.get_new_corrections()\n",
        "\n",
        "    if corrections:\n",
        "        print(f\"✓ Found {len(corrections)} new corrections\")\n",
        "        for i, corr in enumerate(corrections[:3], 1):\n",
        "            print(f\"  {i}. {corr['prompt'][:60]}...\")\n",
        "    else:\n",
        "        print(\"ℹ️  No new corrections found\")\n",
        "\n",
        "    return corrections\n",
        "\n",
        "def prepare_data_for_training(corrections: List[Dict]) -> Optional[Path]:\n",
        "    \"\"\"\n",
        "    Task: Format corrections into training dataset.\n",
        "    \"\"\"\n",
        "    if not corrections:\n",
        "        print(\"  Skipping - NO CORRECTIONS\")\n",
        "        return None\n",
        "\n",
        "    print(\"Preparing training data...\")\n",
        "    batch_file = data_prep.prepare_training_batch(corrections)\n",
        "    return batch_file\n",
        "\n",
        "def validate_training_dataset(batch_file: Optional[Path]) -> bool:\n",
        "    if batch_file is None:\n",
        "        return False\n",
        "\n",
        "    print(\"  Validating dataset...\")\n",
        "\n",
        "    try:\n",
        "        with open(batch_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        if len(lines) == 0:\n",
        "            print(\"Dataset is empty\")\n",
        "            return False\n",
        "\n",
        "        # Check for messages format\n",
        "        first_ex = json.loads(lines[0])\n",
        "        if \"messages\" not in first_ex:\n",
        "            print(f\" Missing 'messages' field. Has: {first_ex.keys()}\")\n",
        "            return False\n",
        "\n",
        "        # Verify messages structure\n",
        "        messages = first_ex[\"messages\"]\n",
        "        if not isinstance(messages, list) or len(messages) < 2:\n",
        "            print(f\" INVALID MESSAGES STRUCTURE\")\n",
        "            return False\n",
        "\n",
        "        print(f\" Dataset VALID ({len(lines)} examples)\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Validation FAILED: {e}\")\n",
        "        return False\n",
        "\n",
        "print(\"✓ Pipeline tasks defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ru15jZHsKd0L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd48e21-6f5e-44ef-e6f6-463ef7fecb8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Data pipeline READY (with proper processing)\n",
            "\n",
            " To run: result = run_data_pipeline()\n"
          ]
        }
      ],
      "source": [
        "# Cell 7D: Data Pipeline Execution (UPDATED)\n",
        "# REPLACE the old Cell 7D with this\n",
        "\n",
        "def run_data_pipeline() -> Dict:\n",
        "    \"\"\"\n",
        "    Execute the data preparation pipeline.\n",
        "\n",
        "    Now marks corrections as processed AFTER successful preparation.\n",
        "    \"\"\"\n",
        "    pipeline = PipelineRun(\n",
        "        name=\"data_preparation\",\n",
        "        log_dir=PIPELINE_LOGS\n",
        "    )\n",
        "\n",
        "    print(f\"\\nSTARTING DATA PREPARATION PIPELINE\")\n",
        "    print(f\"Run ID: {pipeline.run_id}\\n\")\n",
        "\n",
        "    # Task 1: Collect feedback (does NOT mark as processed yet)\n",
        "    corrections = pipeline.run_task(\n",
        "        \"collect_feedback\",\n",
        "        collect_new_feedback\n",
        "    )\n",
        "\n",
        "    if not corrections:\n",
        "        pipeline.finish()\n",
        "        return {\n",
        "            \"status\": \"no_data\",\n",
        "            \"dataset_path\": None,\n",
        "            \"num_examples\": 0\n",
        "        }\n",
        "\n",
        "    # Task 2: Prepare training data\n",
        "    batch_file = pipeline.run_task(\n",
        "        \"prepare_training_data\",\n",
        "        prepare_data_for_training,\n",
        "        corrections\n",
        "    )\n",
        "\n",
        "    # Task 3: Validate\n",
        "    is_valid = pipeline.run_task(\n",
        "        \"validate_dataset\",\n",
        "        validate_training_dataset,\n",
        "        batch_file\n",
        "    )\n",
        "\n",
        "    # Finish pipeline\n",
        "    run_log = pipeline.finish()\n",
        "\n",
        "    # Mark as processed ONLY if successful\n",
        "    if is_valid and batch_file:\n",
        "        correction_ids = [c['id'] for c in corrections]\n",
        "        feedback_mgr.mark_as_processed(correction_ids)\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"dataset_path\": str(batch_file),\n",
        "            \"num_examples\": len(corrections),\n",
        "            \"run_id\": pipeline.run_id\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"status\": \"failed\",\n",
        "            \"dataset_path\": None,\n",
        "            \"num_examples\": 0,\n",
        "            \"run_id\": pipeline.run_id\n",
        "        }\n",
        "\n",
        "print(\" Data pipeline READY (with proper processing)\")\n",
        "print(\"\\n To run: result = run_data_pipeline()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1XHMQSiwffr"
      },
      "source": [
        "##How Fine-Tuning Would Be Done (LoRA Prep) <<DON'T RUN>>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "s5zFs0i-wN4u"
      },
      "outputs": [],
      "source": [
        "# # WARNING: DO NOT RUN ON COLAB FREE.\n",
        "# # This is for your Milestone documentation.\n",
        "\n",
        "# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "# from transformers import TrainingArguments, Trainer\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# # Load dataset\n",
        "# train_data_path = str(FINETUNE_PREP / \"hitl_for_finetune.jsonl\")\n",
        "# dataset = load_dataset(\"json\", data_files=train_data_path, split=\"train\")\n",
        "\n",
        "# def tokenize(entry):\n",
        "#     # Format: \"### Prompt\" pattern helps the model learn dialog structure\n",
        "#     inp = \"### Prompt:\\n\" + entry[\"prompt\"] + \"\\n\\n### Response:\\n\"\n",
        "#     txt = inp + entry[\"response\"]\n",
        "\n",
        "#     tok = tokenizer(txt, truncation=True, max_length=512)\n",
        "\n",
        "#     # Label masking: prompt tokens = -100 (ignored)\n",
        "#     labels = tok[\"input_ids\"].copy()\n",
        "#     prompt_len = len(tokenizer(inp)[\"input_ids\"])\n",
        "#     labels[:prompt_len] = [-100] * prompt_len\n",
        "\n",
        "#     tok[\"labels\"] = labels\n",
        "#     return tok\n",
        "\n",
        "# dataset = dataset.map(tokenize, remove_columns=dataset.column_names)\n",
        "\n",
        "# # LoRA config\n",
        "# lora_cfg = LoraConfig(\n",
        "#     r=8, lora_alpha=32,\n",
        "#     target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        "#     lora_dropout=0.05,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\"\n",
        "# )\n",
        "\n",
        "# lora_model = prepare_model_for_kbit_training(model)\n",
        "# lora_model = get_peft_model(lora_model, lora_cfg)\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=str(WORKDIR / \"lora_ckpt\"),\n",
        "#     per_device_train_batch_size=1,\n",
        "#     gradient_accumulation_steps=8,\n",
        "#     num_train_epochs=1,\n",
        "#     learning_rate=2e-4,\n",
        "#     fp16=True,\n",
        "#     logging_steps=10\n",
        "# )\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=lora_model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=dataset\n",
        "# )\n",
        "\n",
        "# print(\"Training pipeline ready (but not running).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3gOhNNO48n7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "421f9026-3d37-44e3-e4e2-60c5b08b984c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUYAqjUowzj3"
      },
      "source": [
        "## simple chat loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "e5asO26Gwt9V",
        "outputId": "459277db-55dc-4241-80f0-503238d7a8b5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-164152209.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "history = []\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \").strip()\n",
        "    if user_input.lower() in [\"exit\",\"quit\"]:\n",
        "        break\n",
        "\n",
        "    # Before calling generate_reply, ensure model is on the correct device if it wasn't during initial load.\n",
        "    # The root cause is the model being on CPU despite a GPU being available.\n",
        "    # The generate_reply function will be modified to send inputs to model.device\n",
        "    reply = generate_reply(user_input, history)\n",
        "    print(\"Assistant:\", reply)\n",
        "\n",
        "    # Log HITL?\n",
        "    correction = input(\"Correction? (empty = accepted): \").strip()\n",
        "    if correction:\n",
        "        save_interaction(user_input, reply, correction, reason=\"manual feedback\")\n",
        "    else:\n",
        "        save_interaction(user_input, reply)\n",
        "\n",
        "    history.append({\"role\": \"user\", \"content\": user_input})\n",
        "    history.append({\"role\": \"assistant\", \"content\": reply})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MJMjlHyP7nZ"
      },
      "source": [
        "### production API configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ffsnBOw0P_ei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b487336-96aa-4a1d-a753-eafcd3df5402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  PRODUCTION API CONFIGURATION\n",
            "======================================================================\n",
            "No environment variable found\n",
            "\n",
            "   Options:\n",
            "   1. Enter API URL now (for production)\n",
            "   2. Press Enter to skip (local mode only)\n",
            "\n",
            "   API URL: \n",
            "    Local mode (no production API)\n",
            "\n",
            "======================================================================\n",
            "Mode: LOCAL\n",
            "Using local feedback file only\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 23A: Production API Configuration\n",
        "import os\n",
        "\n",
        "print(\"  PRODUCTION API CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Try environment variable first\n",
        "PRODUCTION_API_URL = os.getenv('PRODUCTION_API_URL', '')\n",
        "\n",
        "if PRODUCTION_API_URL and 'YOUR-USERNAME' not in PRODUCTION_API_URL:\n",
        "    print(f\"SUCCESS: Using environment variable\")\n",
        "    print(f\"   API URL: {PRODUCTION_API_URL}\")\n",
        "else:\n",
        "    print(\"No environment variable found\")\n",
        "    print(\"\\n   Options:\")\n",
        "    print(\"   1. Enter API URL now (for production)\")\n",
        "    print(\"   2. Press Enter to skip (local mode only)\")\n",
        "\n",
        "    user_input = input(\"\\n   API URL: \").strip()\n",
        "\n",
        "    if user_input:\n",
        "        PRODUCTION_API_URL = user_input\n",
        "        print(f\"    Set to: {PRODUCTION_API_URL}\")\n",
        "    else:\n",
        "        PRODUCTION_API_URL = \"\"\n",
        "        print(\"    Local mode (no production API)\")\n",
        "\n",
        "# Auto-detect mode\n",
        "USE_PRODUCTION_API = bool(PRODUCTION_API_URL)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"Mode: {'PRODUCTION' if USE_PRODUCTION_API else 'LOCAL'}\")\n",
        "if USE_PRODUCTION_API:\n",
        "    print(f\"API: {PRODUCTION_API_URL}\")\n",
        "else:\n",
        "    print(\"Using local feedback file only\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9QZppzbQXuV"
      },
      "source": [
        "### Download feedback from production"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "17qXM7i1QbjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "702d4c43-1af7-49d0-babf-3592b27998ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOCAL MODE\n",
            "   Using local feedback file (not downloading from production)\n",
            "   File: /content/personalized_chatbot/feedback.jsonl\n",
            "   Interactions: 35\n",
            "   Corrections: 11\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 23B: Download Feedback from Production (Run before training)\n",
        "import requests\n",
        "import json\n",
        "\n",
        "if not USE_PRODUCTION_API:\n",
        "    print(\"LOCAL MODE\")\n",
        "    print(\"   Using local feedback file (not downloading from production)\")\n",
        "    print(f\"   File: {HITL_FILE}\")\n",
        "\n",
        "    if HITL_FILE.exists():\n",
        "        with open(HITL_FILE, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        print(f\"   Interactions: {len(lines)}\")\n",
        "\n",
        "        # Count corrections\n",
        "        corrections = sum(1 for line in lines\n",
        "                         if json.loads(line).get('accepted') is False)\n",
        "        print(f\"   Corrections: {corrections}\")\n",
        "    else:\n",
        "        print(\"     File doesn't exist yet\")\n",
        "        print(\"   Run chat widget to create it\")\n",
        "\n",
        "else:\n",
        "    print(\" PRODUCTION MODE\")\n",
        "    print(f\"   Downloading feedback from: {PRODUCTION_API_URL}\")\n",
        "    print(\"\")\n",
        "\n",
        "    try:\n",
        "        # Check health first\n",
        "        print(\"   1. Checking backend health...\")\n",
        "        health_resp = requests.get(f\"{PRODUCTION_API_URL}/health\", timeout=5)\n",
        "\n",
        "        if health_resp.status_code != 200:\n",
        "            print(f\"    Backend unhealthy: {health_resp.status_code}\")\n",
        "            print(\"   Cannot download feedback\")\n",
        "        else:\n",
        "            print(\"    Backend is healthy\")\n",
        "\n",
        "            # Check correction count\n",
        "            print(\"\\n   2. Checking correction count...\")\n",
        "            count_resp = requests.get(\n",
        "                f\"{PRODUCTION_API_URL}/correction-count\",\n",
        "                timeout=10\n",
        "            )\n",
        "\n",
        "            if count_resp.status_code == 200:\n",
        "                data = count_resp.json()\n",
        "                print(f\"   Total interactions: {data['total']}\")\n",
        "                print(f\"   Corrections: {data['corrections']}\")\n",
        "                print(f\"   Ready to train: {data['ready_to_train']}\")\n",
        "\n",
        "                if data['corrections'] == 0:\n",
        "                    print(\"\\n     No corrections yet - nothing to download\")\n",
        "                    print(\"   Users need to provide feedback first\")\n",
        "\n",
        "                elif data['corrections'] < 5:\n",
        "                    print(f\"\\n    Not enough corrections yet\")\n",
        "                    print(f\"   Minimum: 5, Recommended: 20\")\n",
        "                    print(f\"   Missing: {5 - data['corrections']} more\")\n",
        "\n",
        "                else:\n",
        "                    # Download feedback\n",
        "                    print(\"\\n   3. Downloading feedback...\")\n",
        "                    download_resp = requests.get(\n",
        "                        f\"{PRODUCTION_API_URL}/download-feedback\",\n",
        "                        timeout=30\n",
        "                    )\n",
        "\n",
        "                    if download_resp.status_code == 200:\n",
        "                        feedback = download_resp.json()\n",
        "\n",
        "                        # Validate feedback\n",
        "                        if not feedback['content'].strip():\n",
        "                            print(\"     Feedback is empty\")\n",
        "                        else:\n",
        "                            # Validate JSON format\n",
        "                            try:\n",
        "                                lines = feedback['content'].strip().split('\\n')\n",
        "                                valid_count = 0\n",
        "\n",
        "                                for line in lines[:5]:\n",
        "                                    json.loads(line)\n",
        "                                    valid_count += 1\n",
        "\n",
        "                                print(f\"    Validated first {valid_count} entries\")\n",
        "\n",
        "                                # Save to file\n",
        "                                with open(HITL_FILE, 'w', encoding='utf-8') as f:\n",
        "                                    f.write(feedback['content'])\n",
        "\n",
        "                                print(f\"    Downloaded {feedback['count']} interactions\")\n",
        "                                print(f\"    Saved to: {HITL_FILE}\")\n",
        "                                print(\"\\n    Next: Run Cell 12C to prepare training data\")\n",
        "\n",
        "                            except json.JSONDecodeError as e:\n",
        "                                print(f\"    Invalid JSON format: {e}\")\n",
        "                                print(\"   Feedback file may be corrupted\")\n",
        "                    else:\n",
        "                        print(f\"    Download failed: {download_resp.status_code}\")\n",
        "            else:\n",
        "                print(f\"    Count check failed: {count_resp.status_code}\")\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(\"    Request timed out\")\n",
        "        print(\"    Backend might be sleeping (free tier)\")\n",
        "        print(\"   Try again in 30 seconds\")\n",
        "\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"    Cannot connect to backend\")\n",
        "        print(\"    Check:\")\n",
        "        print(\"   1. Is PRODUCTION_API_URL correct?\")\n",
        "        print(\"   2. Is backend Space running?\")\n",
        "        print(\"   3. Try: curl {url}/health\".replace('{url}', PRODUCTION_API_URL))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    Unexpected error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jhj4-Fur0MY"
      },
      "source": [
        "## CELL 12A: Enhanced Data Pipeline with Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vm0qfui1r4NF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dda11348-d02c-4940-95ea-0651746db2b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Enhanced data preparator ready\n",
            "   Minimum corrections: 20 (16 train, 4 test)\n"
          ]
        }
      ],
      "source": [
        "# Cell 12A: Enhanced Data Preparation with Train/Test Split\n",
        "# Place AFTER Cell 11 (chat loop) - BEFORE old Cell 12\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "class EnhancedDataPreparator:\n",
        "    \"\"\"\n",
        "    Splits corrections into train/test sets for proper evaluation.\n",
        "\n",
        "    Minimum 20 corrections required:\n",
        "    - 80% (16) for training\n",
        "    - 20% (4) for evaluation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, min_corrections: int = 20):\n",
        "        self.min_corrections = min_corrections\n",
        "\n",
        "    def prepare_split_datasets(self, corrections: List[Dict]) -> tuple:\n",
        "        \"\"\"\n",
        "        Split corrections and prepare separate train/test files.\n",
        "\n",
        "        Returns:\n",
        "            (train_file_path, test_file_path, train_count, test_count)\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"\\n Data Split Analysis:\")\n",
        "        print(f\"   Total corrections: {len(corrections)}\")\n",
        "\n",
        "        # Validation\n",
        "        if len(corrections) < self.min_corrections:\n",
        "            print(f\"     WARNING: Need {self.min_corrections} corrections for reliable evaluation\")\n",
        "            print(f\"   Current: {len(corrections)} | Missing: {self.min_corrections - len(corrections)}\")\n",
        "            print(f\"\\n   Options:\")\n",
        "            print(f\"   1. Collect more corrections (RECOMMENDED)\")\n",
        "            print(f\"   2. Proceed anyway (results may be unreliable)\")\n",
        "\n",
        "            proceed = input(\"\\n   Continue anyway? (y/N): \").strip().lower()\n",
        "            if proceed != 'y':\n",
        "                return None, None, 0, 0\n",
        "\n",
        "        # 80/20 split with stratification (if you had labels)\n",
        "        # For now, random split with fixed seed for reproducibility\n",
        "        train_data, test_data = train_test_split(\n",
        "            corrections,\n",
        "            test_size=0.2,      # 20% for testing\n",
        "            random_state=42,    # Reproducible splits\n",
        "            shuffle=True        # Randomize before splitting\n",
        "        )\n",
        "\n",
        "        print(f\"\\n    Split complete:\")\n",
        "        print(f\"      Training set: {len(train_data)} examples ({len(train_data)/len(corrections)*100:.1f}%)\")\n",
        "        print(f\"      Test set: {len(test_data)} examples ({len(test_data)/len(corrections)*100:.1f}%)\")\n",
        "\n",
        "        # Prepare training file\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        train_file = BATCH_DIR / f\"train_{timestamp}.jsonl\"\n",
        "        test_file = BATCH_DIR / f\"test_{timestamp}.jsonl\"\n",
        "\n",
        "        # Save training data (CORRECT format matching your training)\n",
        "        with open(train_file, 'w', encoding='utf-8') as f:\n",
        "            for corr in train_data:\n",
        "                training_example = {\n",
        "                    'messages': [\n",
        "                        {\n",
        "                            'role': 'user',\n",
        "                            'content': corr['prompt']\n",
        "                        },\n",
        "                        {\n",
        "                            'role': 'assistant',\n",
        "                            'content': corr['response']\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "                f.write(json.dumps(training_example, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        # Save test data (CORRECT format matching your training)\n",
        "        with open(test_file, 'w') as f:\n",
        "            for corr in test_data:\n",
        "                test_example = {\n",
        "                    'messages': [\n",
        "                        {\n",
        "                            'role': 'user',\n",
        "                            'content': corr['prompt']\n",
        "                        },\n",
        "                        {\n",
        "                            'role': 'assistant',\n",
        "                            'content': corr['response']\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "                f.write(json.dumps(test_example, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        print(f\"\\n    Files saved:\")\n",
        "        print(f\"      Train: {train_file.name}\")\n",
        "        print(f\"      Test: {test_file.name}\")\n",
        "\n",
        "        return train_file, test_file, len(train_data), len(test_data)\n",
        "\n",
        "# Initialize enhanced preparator\n",
        "enhanced_prep = EnhancedDataPreparator(min_corrections=20)\n",
        "\n",
        "print(\" Enhanced data preparator ready\")\n",
        "print(\"   Minimum corrections: 20 (16 train, 4 test)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7fh6SQAv_P0"
      },
      "source": [
        "## convert HITL logs to fine-tuning dataset in JSONL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iyGAzMnHwC09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "380692cf-e506-4f43-ad26-6518526bca19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 11 corrected samples → /content/personalized_chatbot/finetune_prep/hitl_for_finetune.jsonl\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/personalized_chatbot/finetune_prep/hitl_for_finetune.jsonl')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "def convert_feedback_to_finetune():\n",
        "    src = HITL_FILE\n",
        "    out = FINETUNE_PREP / \"hitl_for_finetune.jsonl\"\n",
        "\n",
        "    if not Path(src).exists():\n",
        "        print(\"No feedback yet.\")\n",
        "        return None\n",
        "\n",
        "    count = 0\n",
        "    with open(src, \"r\", encoding=\"utf-8\") as f, open(out, \"w\", encoding=\"utf-8\") as out_f:\n",
        "        for line in f:\n",
        "            rec = json.loads(line)\n",
        "            if rec[\"accepted\"] is False and rec[\"user_correction\"]:\n",
        "                out_f.write(json.dumps({\n",
        "                    \"prompt\": rec[\"user_input\"],\n",
        "                    \"response\": rec[\"user_correction\"]\n",
        "                }, ensure_ascii=False) + \"\\n\")\n",
        "                count += 1\n",
        "\n",
        "    print(f\"Converted {count} corrected samples → {out}\")\n",
        "    return out\n",
        "\n",
        "convert_feedback_to_finetune()\n",
        "# we ignore the accepted responses and only keep the corrections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE_EEDM-n-7E"
      },
      "source": [
        "# using custom class to orchestrate the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yHurqPs3JlDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b771df0f-8355-4a75-df45-d0ba1119b889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Enhanced pipeline ready\n",
            "\n",
            " To run: result = run_enhanced_data_pipeline()\n"
          ]
        }
      ],
      "source": [
        "# Cell 12B: REPLACE OLD CELL 12 with this Enhanced Pipeline\n",
        "# This version splits data properly and tracks both train/test files\n",
        "\n",
        "def run_enhanced_data_pipeline() -> Dict:\n",
        "    \"\"\"\n",
        "    Enhanced pipeline with train/test split.\n",
        "\n",
        "    Returns dict with:\n",
        "        - status: 'success', 'no_data', or 'failed'\n",
        "        - train_dataset_path: Path to training JSONL\n",
        "        - test_dataset_path: Path to test JSONL\n",
        "        - num_train_examples: Count\n",
        "        - num_test_examples: Count\n",
        "    \"\"\"\n",
        "\n",
        "    pipeline = PipelineRun(\n",
        "        name=\"enhanced_data_preparation\",\n",
        "        log_dir=PIPELINE_LOGS\n",
        "    )\n",
        "\n",
        "    print(f\"\\n STARTING ENHANCED DATA PIPELINE\")\n",
        "    print(f\"Run ID: {pipeline.run_id}\\n\")\n",
        "\n",
        "    # Task 1: Collect feedback\n",
        "    corrections = pipeline.run_task(\n",
        "        \"collect_feedback\",\n",
        "        collect_new_feedback\n",
        "    )\n",
        "\n",
        "    if not corrections:\n",
        "        with mlflow.start_run():\n",
        "          mlflow.set_tag(\"status\", \"no_feedback\")\n",
        "          mlflow.log_metric(\"num_corrections\", 0)\n",
        "\n",
        "        pipeline.finish()\n",
        "        return {\n",
        "            \"status\": \"no_data\",\n",
        "            \"train_dataset_path\": None,\n",
        "            \"test_dataset_path\": None,\n",
        "            \"num_train_examples\": 0,\n",
        "            \"num_test_examples\": 0\n",
        "        }\n",
        "\n",
        "    # Task 2: Split and prepare datasets\n",
        "    def split_and_save():\n",
        "        return enhanced_prep.prepare_split_datasets(corrections)\n",
        "\n",
        "    train_file, test_file, train_count, test_count = pipeline.run_task(\n",
        "        \"split_train_test\",\n",
        "        split_and_save\n",
        "    )\n",
        "\n",
        "    if train_file is None:\n",
        "        pipeline.finish()\n",
        "        return {\n",
        "            \"status\": \"insufficient_data\",\n",
        "            \"train_dataset_path\": None,\n",
        "            \"test_dataset_path\": None,\n",
        "            \"num_train_examples\": 0,\n",
        "            \"num_test_examples\": 0\n",
        "        }\n",
        "\n",
        "    # Task 3: Validate both files\n",
        "    def validate_both():\n",
        "        train_valid = validate_training_dataset(train_file)\n",
        "        test_valid = validate_training_dataset(test_file)\n",
        "        return train_valid and test_valid\n",
        "\n",
        "    is_valid = pipeline.run_task(\n",
        "        \"validate_datasets\",\n",
        "        validate_both\n",
        "    )\n",
        "\n",
        "    # Finish pipeline\n",
        "    run_log = pipeline.finish()\n",
        "\n",
        "    # Mark as processed ONLY if successful\n",
        "    if is_valid and train_file and test_file:\n",
        "        correction_ids = [c['id'] for c in corrections]\n",
        "        feedback_mgr.mark_as_processed(correction_ids)\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"train_dataset_path\": str(train_file),\n",
        "            \"test_dataset_path\": str(test_file),\n",
        "            \"num_train_examples\": train_count,\n",
        "            \"num_test_examples\": test_count,\n",
        "            \"run_id\": pipeline.run_id\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"status\": \"failed\",\n",
        "            \"train_dataset_path\": None,\n",
        "            \"test_dataset_path\": None,\n",
        "            \"num_train_examples\": 0,\n",
        "            \"num_test_examples\": 0,\n",
        "            \"run_id\": pipeline.run_id\n",
        "        }\n",
        "\n",
        "print(\" Enhanced pipeline ready\")\n",
        "print(\"\\n To run: result = run_enhanced_data_pipeline()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iLvrN4EDGLf"
      },
      "source": [
        "diagnostic cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "BkX7bHrHDFxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cdc1805-a177-4288-826d-a3548d81e4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FEEDBACK DIAGNOSTICS\n",
            "======================================================================\n",
            " Feedback file found: /content/personalized_chatbot/feedback.jsonl\n",
            "   File size: 22796 bytes\n",
            "\n",
            " ANALYZING 35 INTERACTIONS...\n",
            "----------------------------------------------------------------------\n",
            "\n",
            " First 3 interactions:\n",
            "\n",
            "   1. User: h...\n",
            "      Bot:  It seems like you just sent a single character. Is there som...\n",
            "      Accepted: True\n",
            "\n",
            "   2. User: h...\n",
            "      Bot:  It seems like you just sent a single character. Would you li...\n",
            "      Accepted: True\n",
            "\n",
            "   3. User: h...\n",
            "      Bot:  It seems like you just sent a single character. Is there som...\n",
            "      Accepted: True\n",
            "\n",
            "======================================================================\n",
            " SUMMARY:\n",
            "   Total interactions: 35\n",
            "   Corrections (accepted=False): 11\n",
            "   Accepted (accepted=True or None): 24\n",
            "   Parse errors: 0\n",
            "\n",
            "======================================================================\n",
            " ANALYSIS:\n",
            "    You have 11 corrections\n",
            "   This is enough to train, but more is better\n",
            "   Recommended: Collect 9 more for reliable evaluation\n",
            "\n",
            "    You can proceed to Cell 12C now!\n",
            "\n",
            "======================================================================\n",
            " NEXT STEPS:\n",
            "   1. Run Cell 12C to prepare training data\n",
            "   2. Check that result['status'] == 'success'\n",
            "   3. Run Cell 13G to train and evaluate\n",
            "   4. Check MLflow for results\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# DIAGNOSTIC CELL: Check Your Feedback Status\n",
        "# Run this to understand what's in your feedback file\n",
        "\n",
        "if 'HITL_FILE' not in dir():\n",
        "    print(\" HITL_FILE not defined\")\n",
        "    print(\"   Please run Cells 4-5 first to set up folders\")\n",
        "else:\n",
        "  import json\n",
        "  from pathlib import Path\n",
        "\n",
        "  print(\"FEEDBACK DIAGNOSTICS\")\n",
        "  print(\"=\"*70)\n",
        "\n",
        "  # Check if feedback file exists\n",
        "  if HITL_FILE.exists():\n",
        "      print(f\" Feedback file found: {HITL_FILE}\")\n",
        "      print(f\"   File size: {HITL_FILE.stat().st_size} bytes\")\n",
        "\n",
        "      # Read and analyze feedback\n",
        "      with open(HITL_FILE, 'r', encoding='utf-8') as f:\n",
        "          lines = f.readlines()\n",
        "\n",
        "      total = len(lines)\n",
        "      corrections = 0\n",
        "      accepted = 0\n",
        "      errors = 0\n",
        "\n",
        "      print(f\"\\n ANALYZING {total} INTERACTIONS...\")\n",
        "      print(\"-\"*70)\n",
        "\n",
        "      # Show first 3 interactions\n",
        "      print(\"\\n First 3 interactions:\")\n",
        "      for i, line in enumerate(lines[:3], 1):\n",
        "          try:\n",
        "              record = json.loads(line.strip())\n",
        "              print(f\"\\n   {i}. User: {record.get('user_input', 'N/A')[:60]}...\")\n",
        "              print(f\"      Bot:  {record.get('model_reply', 'N/A')[:60]}...\")\n",
        "              print(f\"      Accepted: {record.get('accepted', 'N/A')}\")\n",
        "              if not record.get('accepted', True):\n",
        "                  print(f\"      Correction: {record.get('user_correction', 'N/A')[:60]}...\")\n",
        "                  corrections += 1\n",
        "              else:\n",
        "                  accepted += 1\n",
        "          except Exception as e:\n",
        "              print(f\"   {i}.  Error parsing: {e}\")\n",
        "              errors += 1\n",
        "\n",
        "      # Count all interactions\n",
        "      corrections = 0\n",
        "      accepted = 0\n",
        "      errors = 0\n",
        "\n",
        "      for line in lines:\n",
        "          try:\n",
        "              record = json.loads(line.strip())\n",
        "              if record.get('accepted') is False:\n",
        "                  corrections += 1\n",
        "              else:\n",
        "                  accepted += 1\n",
        "          except:\n",
        "              errors += 1\n",
        "\n",
        "      # Summary\n",
        "      print(\"\\n\" + \"=\"*70)\n",
        "      print(\" SUMMARY:\")\n",
        "      print(f\"   Total interactions: {total}\")\n",
        "      print(f\"   Corrections (accepted=False): {corrections}\")\n",
        "      print(f\"   Accepted (accepted=True or None): {accepted}\")\n",
        "      print(f\"   Parse errors: {errors}\")\n",
        "\n",
        "      # Analysis\n",
        "      print(\"\\n\" + \"=\"*70)\n",
        "      print(\" ANALYSIS:\")\n",
        "\n",
        "      if corrections == 0:\n",
        "          print(\"     PROBLEM: You have NO corrections!\")\n",
        "          print(\"\")\n",
        "          print(\"   This means:\")\n",
        "          print(\"   • You chatted with the bot\")\n",
        "          print(\"   • But you pressed Enter without typing corrections\")\n",
        "          print(\"   • The chat loop treats empty input as 'accepted'\")\n",
        "          print(\"\")\n",
        "          print(\"    SOLUTION:\")\n",
        "          print(\"   1. Go back to the chat loop (Cell 11 or 23)\")\n",
        "          print(\"   2. Chat with the bot\")\n",
        "          print(\"   3. When bot gives a WRONG answer, TYPE a correction\")\n",
        "          print(\"   4. Press Enter to submit the correction\")\n",
        "          print(\"   5. Come back and run Cell 12C again\")\n",
        "\n",
        "      elif corrections < 5:\n",
        "          print(f\"     Very few corrections: {corrections}\")\n",
        "          print(f\"   You need at least 5 to train, 20 for good results\")\n",
        "          print(f\"   Missing: {max(5, 20) - corrections} more\")\n",
        "          print(\"\")\n",
        "          print(\"   Keep chatting and providing corrections!\")\n",
        "\n",
        "      elif corrections < 20:\n",
        "          print(f\"    You have {corrections} corrections\")\n",
        "          print(f\"   This is enough to train, but more is better\")\n",
        "          print(f\"   Recommended: Collect {20 - corrections} more for reliable evaluation\")\n",
        "          print(\"\")\n",
        "          print(\"    You can proceed to Cell 12C now!\")\n",
        "\n",
        "      else:\n",
        "          print(f\"    Excellent! You have {corrections} corrections\")\n",
        "          print(f\"   This is great for training!\")\n",
        "          print(\"\")\n",
        "          print(\"    Proceed to Cell 12C to prepare training data\")\n",
        "\n",
        "  else:\n",
        "      print(f\" Feedback file NOT found: {HITL_FILE}\")\n",
        "      print(\"\")\n",
        "      print(\"   This means you haven't chatted with the bot yet.\")\n",
        "      print(\"\")\n",
        "      print(\"    SOLUTION:\")\n",
        "      print(\"   1. Run the chat loop cell (Cell 11 or 23)\")\n",
        "      print(\"   2. Chat with the bot\")\n",
        "      print(\"   3. Provide corrections when needed\")\n",
        "      print(\"   4. Come back and run this diagnostic again\")\n",
        "\n",
        "  print(\"\\n\" + \"=\"*70)\n",
        "  print(\" NEXT STEPS:\")\n",
        "\n",
        "  if not HITL_FILE.exists() or corrections == 0:\n",
        "      print(\"   1. Chat with bot and provide corrections\")\n",
        "      print(\"   2. Run this diagnostic again\")\n",
        "      print(\"   3. When you have 5+ corrections, run Cell 12C\")\n",
        "      print(\"   4. Then run Cell 13G to train\")\n",
        "  elif corrections < 5:\n",
        "      print(f\"   1. Collect {5 - corrections} more corrections (minimum)\")\n",
        "      print(\"   2. Run this diagnostic again\")\n",
        "      print(\"   3. When ready, run Cell 12C\")\n",
        "      print(\"   4. Then run Cell 13G to train\")\n",
        "  else:\n",
        "      print(\"   1. Run Cell 12C to prepare training data\")\n",
        "      print(\"   2. Check that result['status'] == 'success'\")\n",
        "      print(\"   3. Run Cell 13G to train and evaluate\")\n",
        "      print(\"   4. Check MLflow for results\")\n",
        "\n",
        "  print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Clear the processed IDs file\n",
        "processed_file = WORKDIR / \"processed_feedback_ids.json\"\n",
        "\n",
        "# Check current state\n",
        "if processed_file.exists():\n",
        "    with open(processed_file, 'r') as f:\n",
        "        processed = json.load(f)\n",
        "    print(f\"Currently marked as processed: {len(processed)} corrections\")\n",
        "\n",
        "    # Reset it\n",
        "    with open(processed_file, 'w') as f:\n",
        "        json.dump([], f)\n",
        "    print(\"✓ Reset processed IDs - corrections are available again!\")\n",
        "else:\n",
        "    print(\"No processed IDs file found\")\n",
        "\n",
        "# Reload the feedback manager\n",
        "feedback_mgr = FeedbackManager(\n",
        "    feedback_file=HITL_FILE,\n",
        "    processed_ids_file=PROCESSED_IDS_FILE\n",
        ")\n",
        "\n",
        "# Check again\n",
        "available = feedback_mgr.get_new_corrections()\n",
        "print(f\"\\n✓ Available corrections now: {len(available)}\")"
      ],
      "metadata": {
        "id": "0ozV_rGZqxwU",
        "outputId": "dd3381fd-8035-4989-a5b6-3e8bdf560212",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No processed IDs file found\n",
            "\n",
            "✓ Available corrections now: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "jOxLh9vGsL8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8bdc1d0-9fbb-4d4f-b1e8-924e2c3fdd05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Checking for corrections...\n",
            " Available corrections: 11\n",
            "\n",
            "  Only 11 corrections\n",
            "   Recommended: Collect at least 20 for reliable evaluation\n",
            "   Missing: 9 more corrections\n",
            "\n",
            "   You can proceed, but results may be unreliable\n",
            "\n",
            " STARTING ENHANCED DATA PIPELINE\n",
            "Run ID: 20251130_164919\n",
            "\n",
            "\n",
            "============================================================\n",
            "Task: collect_feedback\n",
            "============================================================\n",
            " Collecting feedback...\n",
            "✓ Found 11 new corrections\n",
            "  1. Give me C++ bubble sort code, but do not show any code...\n",
            "  2. Explain supervised vs unsupervised learning without talking ...\n",
            "  3. Explain how diffusion transformers outperform CNN-RNN system...\n",
            "Task completed in 0.00s\n",
            "\n",
            "============================================================\n",
            "Task: split_train_test\n",
            "============================================================\n",
            "\n",
            " Data Split Analysis:\n",
            "   Total corrections: 11\n",
            "     WARNING: Need 20 corrections for reliable evaluation\n",
            "   Current: 11 | Missing: 9\n",
            "\n",
            "   Options:\n",
            "   1. Collect more corrections (RECOMMENDED)\n",
            "   2. Proceed anyway (results may be unreliable)\n",
            "\n",
            "   Continue anyway? (y/N): y\n",
            "\n",
            "    Split complete:\n",
            "      Training set: 8 examples (72.7%)\n",
            "      Test set: 3 examples (27.3%)\n",
            "\n",
            "    Files saved:\n",
            "      Train: train_20251130_164926.jsonl\n",
            "      Test: test_20251130_164926.jsonl\n",
            "Task completed in 7.13s\n",
            "\n",
            "============================================================\n",
            "Task: validate_datasets\n",
            "============================================================\n",
            "  Validating dataset...\n",
            " Dataset VALID (8 examples)\n",
            "  Validating dataset...\n",
            " Dataset VALID (3 examples)\n",
            "Task completed in 0.00s\n",
            "\n",
            "============================================================\n",
            "PIPELINE SUMMARY\n",
            "============================================================\n",
            "Run ID: 20251130_164919\n",
            "Status: COMPLETED\n",
            "Duration: 7.13s\n",
            "Tasks: 3\n",
            "  [SUCCEEDED] collect_feedback: 0.00s\n",
            "  [SUCCEEDED] split_train_test: 7.13s\n",
            "  [SUCCEEDED] validate_datasets: 0.00s\n",
            "\n",
            "Log saved: /content/personalized_chatbot/pipeline_logs/run_20251130_164919.json\n",
            "Marked 11 corrections as processed\n",
            "\n",
            " Pipeline completed despite low data count\n",
            "   Training examples: 8\n",
            "   Test examples: 3\n"
          ]
        }
      ],
      "source": [
        "# Cell 12C: Execute Enhanced Pipeline (FIXED)\n",
        "# Place AFTER Cell 12B\n",
        "\n",
        "print(\"🔍 Checking for corrections...\")\n",
        "\n",
        "# Check available corrections\n",
        "available_corrections = feedback_mgr.get_new_corrections()\n",
        "print(f\" Available corrections: {len(available_corrections)}\")\n",
        "\n",
        "if len(available_corrections) == 0:\n",
        "    print(\"\\n  No corrections found!\")\n",
        "    print(\"   Instructions:\")\n",
        "    print(\"   1. Go back to Cell 11 (chat loop)\")\n",
        "    print(\"   2. Chat with the bot\")\n",
        "    print(\"   3. When it gives wrong answers, provide corrections\")\n",
        "    print(\"   4. Come back here and run this cell again\")\n",
        "\n",
        "    # FIX: Set result even when there are no corrections\n",
        "    result = {\n",
        "        \"status\": \"no_data\",\n",
        "        \"train_dataset_path\": None,\n",
        "        \"test_dataset_path\": None,\n",
        "        \"num_train_examples\": 0,\n",
        "        \"num_test_examples\": 0\n",
        "    }\n",
        "\n",
        "elif len(available_corrections) < 20:\n",
        "    print(f\"\\n  Only {len(available_corrections)} corrections\")\n",
        "    print(f\"   Recommended: Collect at least 20 for reliable evaluation\")\n",
        "    print(f\"   Missing: {20 - len(available_corrections)} more corrections\")\n",
        "    print(\"\\n   You can proceed, but results may be unreliable\")\n",
        "\n",
        "    # Run anyway\n",
        "    result = run_enhanced_data_pipeline()\n",
        "\n",
        "    if result[\"status\"] == \"success\":\n",
        "        print(f\"\\n Pipeline completed despite low data count\")\n",
        "        print(f\"   Training examples: {result['num_train_examples']}\")\n",
        "        print(f\"   Test examples: {result['num_test_examples']}\")\n",
        "\n",
        "else:\n",
        "    print(f\"\\n Sufficient corrections: {len(available_corrections)}\")\n",
        "    print(\"   Running pipeline...\\n\")\n",
        "\n",
        "    # Run pipeline\n",
        "    result = run_enhanced_data_pipeline()\n",
        "\n",
        "    if result[\"status\"] == \"success\":\n",
        "        print(f\"\\n SUCCESS!\")\n",
        "        print(f\"   Training data: {result['train_dataset_path']}\")\n",
        "        print(f\"   Test data: {result['test_dataset_path']}\")\n",
        "        print(f\"   Train examples: {result['num_train_examples']}\")\n",
        "        print(f\"   Test examples: {result['num_test_examples']}\")\n",
        "        print(f\"\\n Next: Scroll down to training cells\")\n",
        "    else:\n",
        "        print(f\"\\n  Pipeline status: {result['status']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCGS8J9SMJHs"
      },
      "source": [
        "## LoRA training optimized for T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "M2wMkYljMOky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "956e66bb-32b8-46b0-e257-cdbaf862d448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Training configuration class defined\n",
            "\n",
            " Optimizations for T4 GPU:\n",
            "  • 4-bit quantization (saves ~10GB)\n",
            "  • LoRA rank 8 (keeps adapter small)\n",
            "  • Batch size 1 + gradient accumulation 8\n",
            "  • 8-bit optimizer (saves ~4GB)\n",
            "  • Gradient checkpointing enabled\n"
          ]
        }
      ],
      "source": [
        "# Cell 13A: Training Configuration for T4 GPU\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "from typing import Optional\n",
        "import os\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 1\n",
        "gradient_accumulation_steps = 8\n",
        "learning_rate = 2e-4\n",
        "lora_alpha = 32\n",
        "lora_dropout = 0.05\n",
        "\n",
        "class LoRATrainingConfig:\n",
        "    \"\"\"\n",
        "    Training configuration optimized for Colab T4 GPU (15GB VRAM).\n",
        "\n",
        "    Key optimizations:\n",
        "    - 4-bit quantization reduces memory by ~75%\n",
        "    - Small LoRA rank (r=8) keeps adapter tiny\n",
        "    - Gradient accumulation simulates larger batch sizes\n",
        "    - Aggressive gradient checkpointing saves memory\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_path: str, output_dir: Path):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "        # LoRA Configuration\n",
        "        # r=8 means we're adding 8-rank decomposition matrices\n",
        "        # Lower r = less parameters = less memory = faster training\n",
        "        # But too low = model can't learn enough\n",
        "        self.lora_config = LoraConfig(\n",
        "            r=8,                          # Rank (8 is sweet spot for T4)\n",
        "            lora_alpha=lora_alpha,                # Scaling factor (typically 2-4x rank)\n",
        "            target_modules=[              # Which layers to adapt\n",
        "                \"q_proj\",                 # Query projection\n",
        "                \"k_proj\",                 # Key projection\n",
        "                \"v_proj\",                 # Value projection\n",
        "                \"o_proj\"                  # Output projection\n",
        "            ],\n",
        "            lora_dropout=lora_dropout,            # Regularization\n",
        "            bias=\"none\",                  # Don't adapt bias terms\n",
        "            task_type=TaskType.CAUSAL_LM  # Causal language modeling\n",
        "        )\n",
        "\n",
        "        # 4-bit Quantization Config\n",
        "        # This is THE trick that makes 8B models fit on T4\n",
        "        self.bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,                    # Enable 4-bit\n",
        "            bnb_4bit_quant_type=\"nf4\",           # \"Normal Float 4\" - best quality\n",
        "            bnb_4bit_compute_dtype=torch.float16, # Compute in FP16\n",
        "            bnb_4bit_use_double_quant=True       # Double quantization saves more memory\n",
        "        )\n",
        "\n",
        "        # Training Arguments\n",
        "        # These are VERY conservative to avoid OOM\n",
        "        self.training_args = TrainingArguments(\n",
        "            output_dir=str(output_dir),\n",
        "\n",
        "            # Batch size: Start tiny!\n",
        "            per_device_train_batch_size=batch_size,        # Only 1 example at a time\n",
        "            gradient_accumulation_steps=gradient_accumulation_steps,        # But accumulate 8 steps = effective batch of 8\n",
        "\n",
        "            # Epochs: 1-2 is enough for small datasets\n",
        "            num_train_epochs=epochs,\n",
        "\n",
        "            # Learning rate: Higher than normal because LoRA needs strong signal\n",
        "            learning_rate=learning_rate,                   # 0.0002\n",
        "\n",
        "            # Memory optimizations\n",
        "            fp16=True,                            # Use half precision\n",
        "            gradient_checkpointing=True,          # Trade compute for memory\n",
        "            optim=\"paged_adamw_8bit\",            # 8-bit optimizer (saves 4GB!)\n",
        "\n",
        "            # Logging and checkpointing\n",
        "            logging_steps=5,                      # Log every 5 steps\n",
        "            save_strategy=\"epoch\",                # Save after each epoch\n",
        "            save_total_limit=2,                   # Keep only 2 checkpoints\n",
        "\n",
        "            # Misc\n",
        "            warmup_steps=10,                      # Gradual learning rate warmup\n",
        "            report_to=[],                         # Don't report to wandb/tensorboard\n",
        "            remove_unused_columns=False,          # Keep all columns for debugging\n",
        "        )\n",
        "\n",
        "    def get_memory_usage(self):\n",
        "        \"\"\"Check current GPU memory usage\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**3  # Convert to GB\n",
        "            reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "            return f\"Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\"\n",
        "        return \"CUDA not available\"\n",
        "\n",
        "# Initialize config\n",
        "training_config = None  # We'll set this in the next cell\n",
        "\n",
        "print(\"✓ Training configuration class defined\")\n",
        "print(\"\\n Optimizations for T4 GPU:\")\n",
        "print(\"  • 4-bit quantization (saves ~10GB)\")\n",
        "print(\"  • LoRA rank 8 (keeps adapter small)\")\n",
        "print(\"  • Batch size 1 + gradient accumulation 8\")\n",
        "print(\"  • 8-bit optimizer (saves ~4GB)\")\n",
        "print(\"  • Gradient checkpointing enabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncCC_Sf_MSpB"
      },
      "source": [
        "### Dataset tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "9Epzu0koMQkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbe1159f-3a13-44f2-d14e-52b9d45f4153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Chat-format dataset preparation function defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 13B: Dataset Preparation and Tokenization (CHAT FORMAT)\n",
        "\n",
        "def prepare_training_dataset(dataset_path: str, tokenizer):\n",
        "    from datasets import load_dataset\n",
        "\n",
        "    print(f\" Loading dataset from: {dataset_path}\")\n",
        "    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
        "    print(f\"   Loaded {len(dataset)} examples\")\n",
        "\n",
        "    #  Verify format\n",
        "    print(\"\\n Sample (messages format):\")\n",
        "    print(dataset[0])\n",
        "\n",
        "    # Apply chat template\n",
        "    def apply_chat_template(example):\n",
        "        formatted = tokenizer.apply_chat_template(\n",
        "            example[\"messages\"],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        return {\"text\": formatted}\n",
        "\n",
        "    print(\"\\n Applying chat template...\")\n",
        "    dataset = dataset.map(\n",
        "        apply_chat_template,\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "\n",
        "    # Tokenization\n",
        "    def tokenize(example):\n",
        "        encoded = tokenizer(\n",
        "            example[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=None\n",
        "        )\n",
        "        encoded[\"labels\"] = encoded[\"input_ids\"].copy()\n",
        "        return encoded\n",
        "\n",
        "    print(\" Tokenizing...\")\n",
        "    tokenized_dataset = dataset.map(\n",
        "        tokenize,\n",
        "        batched=True,\n",
        "        remove_columns=[\"text\"],\n",
        "        desc=\"Tokenizing\"\n",
        "    )\n",
        "\n",
        "    print(f\" Tokenized {len(tokenized_dataset)} examples\")\n",
        "    return tokenized_dataset\n",
        "\n",
        "print(\"✓ Chat-format dataset preparation function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMjqrcMfMbf8"
      },
      "source": [
        "### Main training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "sP6F2f84MWHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87fab6e8-5038-4893-83e7-414634d6f5a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training function defined\n",
            "\n",
            " Ready to train! See Cell 13G for execution.\n"
          ]
        }
      ],
      "source": [
        "# Cell 13C: Main Training Function (FIXED)\n",
        "\n",
        "def train_lora_adapter(\n",
        "    dataset_path: str,\n",
        "    output_name: str = \"llama-lora-adapter\",\n",
        "    push_to_hub: bool = True,\n",
        "    hub_repo_name: Optional[str] = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a LoRA adapter on corrected examples.\n",
        "\n",
        "    Args:\n",
        "        dataset_path: Path to prepared JSONL dataset\n",
        "        output_name: Local directory name for saving\n",
        "        push_to_hub: Whether to push to Hugging Face Hub\n",
        "        hub_repo_name: HF repo name (e.g., \"yourusername/model-name\")\n",
        "\n",
        "    Returns:\n",
        "        Path to trained adapter\n",
        "    \"\"\"\n",
        "    global lora_training_loss\n",
        "    global training_config\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" STARTING LORA TRAINING\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Setup output directory\n",
        "    adapter_output_dir = WORKDIR / output_name\n",
        "    adapter_output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Initialize config\n",
        "\n",
        "    training_config = LoRATrainingConfig(\n",
        "        dataset_path=dataset_path,\n",
        "        output_dir=adapter_output_dir\n",
        "    )\n",
        "\n",
        "    print(f\"\\n Initial GPU memory: {training_config.get_memory_usage()}\")\n",
        "\n",
        "    # Step 1: Load tokenizer (already loaded, but ensure padding token)\n",
        "    print(\"\\n Setting up tokenizer...\")\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(f\"   Pad token: {tokenizer.pad_token}\")\n",
        "\n",
        "    # Step 2: Load and tokenize dataset\n",
        "    print(\"\\n Preparing dataset...\")\n",
        "    train_dataset = prepare_training_dataset(dataset_path, tokenizer)\n",
        "\n",
        "    if len(train_dataset) < 5:\n",
        "        print(\"\\n  WARNING: Less than 5 examples!\")\n",
        "        print(\"   Training might not improve the model meaningfully.\")\n",
        "        print(\"   Recommended: Collect at least 20 corrections.\")\n",
        "        proceed = input(\"\\n   Continue anyway? (y/N): \").strip().lower()\n",
        "        if proceed != 'y':\n",
        "            print(\" Training cancelled\")\n",
        "            return None\n",
        "\n",
        "    # Step 3: Prepare model for training\n",
        "    print(\"\\n Preparing model for LoRA training...\")\n",
        "    print(\"   This will take 2-3 minutes...\")\n",
        "\n",
        "    print(\"\\n Clearing GPU memory...\")\n",
        "    import gc\n",
        "\n",
        "    # Clear cache first\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"   Memory freed: {torch.cuda.memory_allocated() / 1024**3:.2f}GB\")\n",
        "\n",
        "    # Prepare model for training (model is still on GPU)\n",
        "    model_for_training = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # Ensure model is on GPU\n",
        "    if torch.cuda.is_available():\n",
        "        model_for_training = model_for_training.to('cuda')\n",
        "\n",
        "    print(f\"   GPU memory after prep: {training_config.get_memory_usage()}\")\n",
        "    print(f\"   Model device: {next(model_for_training.parameters()).device}\")\n",
        "\n",
        "    # Step 4: Add LoRA adapters\n",
        "    print(\"\\n Adding LoRA adapters...\")\n",
        "    lora_model = get_peft_model(model_for_training, training_config.lora_config)\n",
        "\n",
        "    # Print trainable parameters\n",
        "    trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in lora_model.parameters())\n",
        "    trainable_percent = 100 * trainable_params / total_params\n",
        "\n",
        "    print(f\"   Trainable params: {trainable_params:,} ({trainable_percent:.2f}%)\")\n",
        "    print(f\"   Total params: {total_params:,}\")\n",
        "    print(f\"   Memory: {training_config.get_memory_usage()}\")\n",
        "\n",
        "    # Step 5: Create trainer\n",
        "    print(\"\\n  Creating trainer...\")\n",
        "\n",
        "    # Data collator handles batching and padding\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False  # We're doing causal LM, not masked LM\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=lora_model,\n",
        "        args=training_config.training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # Step 6: Train\n",
        "    print(\"\\n Starting training...\")\n",
        "    print(\"   This will take 10-20 minutes depending on dataset size.\")\n",
        "    print(\"   Watch the loss - it should decrease.\\n\")\n",
        "\n",
        "    try:\n",
        "        #  FIX: Train first then save loss\n",
        "        train_result = trainer.train()\n",
        "\n",
        "        print(\"\\n TRAINING COMPLETED!\")\n",
        "        print(f\"   Final loss: {train_result.training_loss:.4f}\")\n",
        "\n",
        "        #  Save loss AFTER training (when train_result exists)\n",
        "        lora_training_loss = train_result.training_loss\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        if \"out of memory\" in str(e):\n",
        "            print(\"\\n OUT OF MEMORY ERROR!\")\n",
        "            print(\"\\n Try these fixes:\")\n",
        "            print(\"   1. Reduce per_device_train_batch_size to 1 (if not already)\")\n",
        "            print(\"   2. Increase gradient_accumulation_steps to 16\")\n",
        "            print(\"   3. Reduce max_length to 256 in tokenization\")\n",
        "            print(\"   4. Use a smaller LoRA rank (r=4)\")\n",
        "            print(\"\\n   Restart runtime and try again.\")\n",
        "\n",
        "            # Set default loss value on error\n",
        "            lora_training_loss = float('inf')\n",
        "            return None\n",
        "        else:\n",
        "            raise e\n",
        "\n",
        "    # Step 7: Save adapter\n",
        "    print(f\"\\n Saving adapter to {adapter_output_dir}...\")\n",
        "    lora_model.save_pretrained(adapter_output_dir)\n",
        "    tokenizer.save_pretrained(adapter_output_dir)\n",
        "\n",
        "    print(\"✓ Adapter saved locally\")\n",
        "\n",
        "    # Step 8: Push to Hugging Face Hub (optional)\n",
        "    if push_to_hub:\n",
        "        if hub_repo_name is None:\n",
        "            print(\"\\n  No hub_repo_name provided. Skipping push to HF Hub.\")\n",
        "            print(\"   To push later, run:\")\n",
        "            print(f\"   huggingface-cli upload {hub_repo_name} {adapter_output_dir}\")\n",
        "        else:\n",
        "            print(f\"\\n  Pushing to Hugging Face Hub: {hub_repo_name}\")\n",
        "            try:\n",
        "                lora_model.push_to_hub(hub_repo_name)\n",
        "                tokenizer.push_to_hub(hub_repo_name)\n",
        "                print(f\" Pushed to https://huggingface.co/{hub_repo_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Push failed: {e}\")\n",
        "                print(\"   You can push manually later using huggingface-cli\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" TRAINING COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Adapter location: {adapter_output_dir}\")\n",
        "    print(f\"Memory used: {training_config.get_memory_usage()}\")\n",
        "\n",
        "    return adapter_output_dir\n",
        "\n",
        "print(\" Training function defined\")\n",
        "print(\"\\n Ready to train! See Cell 13G for execution.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k6AT-MIJiOs"
      },
      "source": [
        "####T4 GPU memory optimizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "RX3xf1oiJmBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5efbc05f-f82c-40be-f6e0-0b2a38343abc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Optimizing for T4 GPU...\n",
            "Optimized - Available: 14.7GB\n"
          ]
        }
      ],
      "source": [
        "# Cell 12D: T4 Memory Optimization\n",
        "def optimize_for_t4():\n",
        "    \"\"\"Aggressive memory optimization for T4 GPU (15GB).\"\"\"\n",
        "    import gc\n",
        "\n",
        "    print(\" Optimizing for T4 GPU...\")\n",
        "\n",
        "    # Clear cache\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Set memory allocation strategy\n",
        "    torch.cuda.set_per_process_memory_fraction(0.95)  # Use 95% max\n",
        "\n",
        "    # Enable TF32 for faster training (if supported)\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    print(f\"Optimized - Available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "\n",
        "optimize_for_t4()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygnVa8fFsX6B"
      },
      "source": [
        "## Multi-Metric Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ItiCAImnsbp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1909ed07-5cf2-4177-baea-5f862ad028e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading evaluation models...\n",
            " Evaluation models loaded\n",
            "\n",
            " Multi-metric evaluator ready\n",
            "   Metrics: Semantic Similarity, ROUGE-L, BLEU, Exact Match\n"
          ]
        }
      ],
      "source": [
        "# Cell 13E: Multi-Metric Model Evaluator\n",
        "\n",
        "!pip install -q sentence-transformers rouge-score nltk\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"\n",
        "    Multi-metric evaluation for fine-tuned models.\n",
        "\n",
        "    Metrics:\n",
        "    1. Semantic Similarity (embedding distance)\n",
        "    2. ROUGE-L (word overlap with order)\n",
        "    3. BLEU (n-gram precision)\n",
        "    4. Exact Match (strict correctness)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\" Loading evaluation models...\")\n",
        "\n",
        "        # Lightweight embedding model (22MB)\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # ROUGE scorer\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "        # BLEU smoothing (handles edge cases)\n",
        "        self.smoothing = SmoothingFunction()\n",
        "\n",
        "        print(\" Evaluation models loaded\")\n",
        "\n",
        "    def generate_response(self, model, tokenizer, prompt: str) -> str:\n",
        "      \"\"\"Generate using SAME format as training.\"\"\"\n",
        "      messages = [{'role': 'user', 'content': prompt}]\n",
        "\n",
        "      # Apply chat template (same as training)\n",
        "      formatted = tokenizer.apply_chat_template(\n",
        "          messages,\n",
        "          tokenize=False,\n",
        "          add_generation_prompt=True  # Add assistant prompt\n",
        "      )\n",
        "\n",
        "      inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          outputs = model.generate(\n",
        "              **inputs,\n",
        "              max_new_tokens=128,\n",
        "              temperature=0.7,\n",
        "              do_sample=True,\n",
        "              pad_token_id=tokenizer.pad_token_id\n",
        "          )\n",
        "\n",
        "      # Decode only new tokens\n",
        "      response = tokenizer.decode(\n",
        "          outputs[0][inputs['input_ids'].shape[1]:],\n",
        "          skip_special_tokens=True\n",
        "      )\n",
        "\n",
        "      return response.strip()\n",
        "\n",
        "    def calculate_metrics(self, generated: str, ground_truth: str) -> Dict[str, float]:\n",
        "        \"\"\"Calculate all metrics for one example.\"\"\"\n",
        "\n",
        "        # 1. Semantic Similarity (embedding-based)\n",
        "        embeddings = self.embedding_model.encode([generated, ground_truth])\n",
        "        semantic_sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "\n",
        "        # 2. ROUGE-L (word overlap with longest common subsequence)\n",
        "        rouge_scores = self.rouge_scorer.score(ground_truth, generated)\n",
        "        rouge_l = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "        # 3. BLEU (n-gram precision)\n",
        "        reference = [ground_truth.split()]\n",
        "        candidate = generated.split()\n",
        "        bleu_score = sentence_bleu(\n",
        "            reference,\n",
        "            candidate,\n",
        "            smoothing_function=self.smoothing.method1\n",
        "        )\n",
        "\n",
        "        # 4. Exact Match (binary - exact correctness)\n",
        "        exact_match = int(generated.strip().lower() == ground_truth.strip().lower())\n",
        "\n",
        "        return {\n",
        "            'semantic_similarity': float(semantic_sim),\n",
        "            'rouge_l': float(rouge_l),\n",
        "            'bleu': float(bleu_score),\n",
        "            'exact_match': exact_match\n",
        "        }\n",
        "\n",
        "    def evaluate_model(\n",
        "        self,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        test_file: Path,\n",
        "        model_name: str = \"Model\"\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate model on test set.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with averaged metrics and per-example details\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\" EVALUATING: {model_name}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Test file: {test_file.name}\\n\")\n",
        "\n",
        "        # Load test data\n",
        "        test_examples = []\n",
        "        with open(test_file, 'r') as f:\n",
        "            for line in f:\n",
        "                test_examples.append(json.loads(line))\n",
        "\n",
        "        print(f\"Test examples: {len(test_examples)}\\n\")\n",
        "\n",
        "        # Collect metrics\n",
        "        all_metrics = {\n",
        "            'semantic_similarity': [],\n",
        "            'rouge_l': [],\n",
        "            'bleu': [],\n",
        "            'exact_match': []\n",
        "        }\n",
        "\n",
        "        detailed_results = []\n",
        "\n",
        "        for i, example in enumerate(test_examples, 1):\n",
        "            # Extract from messages format\n",
        "            messages = example['messages']\n",
        "            prompt = next(m['content'] for m in messages if m['role'] == 'user')\n",
        "            ground_truth = next(m['content'] for m in messages if m['role'] == 'assistant')\n",
        "\n",
        "            # Generate\n",
        "            print(f\"[{i}/{len(test_examples)}] Generating...\", end=\" \")\n",
        "            generated = self.generate_response(model, tokenizer, prompt)\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics = self.calculate_metrics(generated, ground_truth)\n",
        "\n",
        "            # Store\n",
        "            for key, value in metrics.items():\n",
        "                all_metrics[key].append(value)\n",
        "\n",
        "            detailed_results.append({\n",
        "                'prompt': prompt,\n",
        "                'generated': generated,\n",
        "                'ground_truth': ground_truth,\n",
        "                'metrics': metrics\n",
        "            })\n",
        "\n",
        "            # Print summary\n",
        "            print(f\"Semantic: {metrics['semantic_similarity']:.3f} | ROUGE-L: {metrics['rouge_l']:.3f}\")\n",
        "\n",
        "        # Calculate averages\n",
        "        avg_metrics = {key: np.mean(values) for key, values in all_metrics.items()}\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\" SUMMARY: {model_name}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Semantic Similarity: {avg_metrics['semantic_similarity']:.4f}\")\n",
        "        print(f\"ROUGE-L:            {avg_metrics['rouge_l']:.4f}\")\n",
        "        print(f\"BLEU:               {avg_metrics['bleu']:.4f}\")\n",
        "        print(f\"Exact Match:        {avg_metrics['exact_match']:.4f} ({int(avg_metrics['exact_match']*len(test_examples))}/{len(test_examples)})\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        return {\n",
        "            'average_metrics': avg_metrics,\n",
        "            'detailed_results': detailed_results,\n",
        "            'num_test_examples': len(test_examples)\n",
        "        }\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = ModelEvaluator()\n",
        "\n",
        "print(\"\\n Multi-metric evaluator ready\")\n",
        "print(\"   Metrics: Semantic Similarity, ROUGE-L, BLEU, Exact Match\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd7wLAsGAuus"
      },
      "source": [
        "DAGSHUB preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Naze8bueAws-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b1026ba-e04d-4581-e1b1-c8180d6ffe9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Using Colab secrets for DagsHub authentication\n",
            " Authenticated as: PierreRamez\n",
            " Ready to log to DagsHub MLflow\n"
          ]
        }
      ],
      "source": [
        "# DagsHub Authentication\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Option 1: Use Colab Secrets\n",
        "try:\n",
        "    DAGSHUB_USERNAME = userdata.get('DAGSHUB_USERNAME')\n",
        "    DAGSHUB_TOKEN = userdata.get('DAGSHUB_TOKEN')\n",
        "    print(\" Using Colab secrets for DagsHub authentication\")\n",
        "except:\n",
        "    # Option 2: Manual input\n",
        "    print(\"  Colab secrets not found, using manual input\")\n",
        "    DAGSHUB_USERNAME = input(\"Enter DagsHub username: \")\n",
        "    DAGSHUB_TOKEN = input(\"Enter DagsHub token: \")\n",
        "\n",
        "# Set environment variables for MLflow\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = DAGSHUB_USERNAME\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = DAGSHUB_TOKEN\n",
        "\n",
        "print(f\" Authenticated as: {DAGSHUB_USERNAME}\")\n",
        "print(\" Ready to log to DagsHub MLflow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z89Ljq6_siPp"
      },
      "source": [
        "## MLFlow integration with Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "NDzCvsjjstfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd55f126-9099-419c-e899-2ce06a2302ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.12/dist-packages (3.6.0)\n",
            "Requirement already satisfied: mlflow-skinny==3.6.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.6.0)\n",
            "Requirement already satisfied: mlflow-tracing==3.6.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.6.0)\n",
            "Requirement already satisfied: Flask-CORS<7 in /usr/local/lib/python3.12/dist-packages (from mlflow) (6.0.1)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.1.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.17.2)\n",
            "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (43.0.3)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.12/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: huey<3,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.5.4)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<23,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.16.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.44)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (8.3.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (3.1.2)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (0.73.0)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (0.118.3)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (2.12.3)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (1.2.1)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.6.0->mlflow) (0.38.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<47,>=43.0.0->mlflow) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (3.2.7)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (3.2.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow) (2.23)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow) (2.43.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.6.0->mlflow) (0.48.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.6.0->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.6.0->mlflow) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.6.0->mlflow) (0.58b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.6.0->mlflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.6.0->mlflow) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.6.0->mlflow) (2025.11.12)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1->mlflow-skinny==3.6.0->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.6.0->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi<1->mlflow-skinny==3.6.0->mlflow) (4.11.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi<1->mlflow-skinny==3.6.0->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow) (0.6.1)\n",
            "MLflow training pipeline ready\n",
            "\n",
            " This function:\n",
            "   • Trains the model\n",
            "   • Evaluates baseline vs fine-tuned\n",
            "   • Applies quality gates\n",
            "   • Conditionally registers model\n",
            "   • Generates detailed report\n"
          ]
        }
      ],
      "source": [
        "# Cell 13F: MLflow Integration with Proper Evaluation\n",
        "!pip install mlflow\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import numpy as np\n",
        "\n",
        "def convert_to_serializable(obj):\n",
        "    \"\"\"Convert numpy types and bools to JSON-serializable types.\"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_serializable(item) for item in obj]\n",
        "    elif isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, bool):\n",
        "        return int(obj)\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# Configure MLflow\n",
        "mlflow.set_experiment('Llama-3.2-3B-Personalized-Chatbot')\n",
        "mlflow.set_tracking_uri('https://dagshub.com/PierreRamez/PersonalChatbot.mlflow')\n",
        "\n",
        "def train_and_evaluate_with_mlflow(\n",
        "    train_dataset_path: str,\n",
        "    test_dataset_path: str,\n",
        "    output_name: str = \"llama-lora-adapter\",\n",
        "    push_to_hub: bool = True,\n",
        "    hub_repo_name: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Complete training + evaluation pipeline with MLflow tracking.\n",
        "\n",
        "    This is the CORRECT way to integrate MLflow:\n",
        "    1. Start run BEFORE training\n",
        "    2. Log params, train, evaluate\n",
        "    3. Log all metrics and artifacts\n",
        "    4. Conditionally register model based on quality\n",
        "    \"\"\"\n",
        "\n",
        "    # Validate inputs\n",
        "    train_path = Path(train_dataset_path)\n",
        "    test_path = Path(test_dataset_path)\n",
        "\n",
        "    if not train_path.exists() or not test_path.exists():\n",
        "        print(\" Train or test file missing!\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" TRAINING WITH MLFLOW TRACKING\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Start MLflow run (to wrap everything)\n",
        "    with mlflow.start_run() as run:\n",
        "        run_id = run.info.run_id\n",
        "        print(f\"\\n MLflow Run ID: {run_id}\")\n",
        "        print(f\" Track at: https://dagshub.com/PierreRamez/PersonalChatbot.mlflow\")\n",
        "\n",
        "        # ===== PHASE 1: LOG HYPERPARAMETERS =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\" PHASE 1: Logging Hyperparameters\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        mlflow.log_param(\"base_model\", MODEL)\n",
        "        mlflow.log_param(\"epochs\", epochs)\n",
        "        mlflow.log_param(\"batch_size\", batch_size)\n",
        "        mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "        mlflow.log_param(\"lora_rank\", 8)\n",
        "        mlflow.log_param(\"lora_alpha\", lora_alpha)\n",
        "        mlflow.log_param(\"lora_dropout\", lora_dropout)\n",
        "        mlflow.log_param(\"gradient_accumulation_steps\", gradient_accumulation_steps)\n",
        "        mlflow.log_param(\"max_seq_length\", 512)\n",
        "\n",
        "        print(\" Hyperparameters logged\")\n",
        "\n",
        "        # ===== PHASE 2: TRAIN MODEL =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\" PHASE 2: Training\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        adapter_path = train_lora_adapter(\n",
        "            dataset_path=train_dataset_path,\n",
        "            output_name=output_name,\n",
        "            push_to_hub=False  # We'll push conditionally later\n",
        "        )\n",
        "\n",
        "        if adapter_path is None:\n",
        "            print(\"\\n Training failed!\")\n",
        "            mlflow.set_tag(\"status\", \"training_failed\")\n",
        "            return None\n",
        "\n",
        "        # Log training loss (from global variable set during training)\n",
        "        if 'lora_training_loss' in globals():\n",
        "            mlflow.log_metric(\"train_loss\", lora_training_loss)\n",
        "            print(f\" Training loss logged: {lora_training_loss:.4f}\")\n",
        "\n",
        "        # ===== PHASE 3: EVALUATE BASELINE =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\" PHASE 3: Evaluating Baseline Model\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Load original model (without adapter)\n",
        "        baseline_results = evaluator.evaluate_model(\n",
        "            model,  # Original model from Cell 3\n",
        "            tokenizer,\n",
        "            test_path,\n",
        "            model_name=\"Baseline (Original)\"\n",
        "        )\n",
        "\n",
        "        # Log baseline metrics\n",
        "        for metric_name, value in baseline_results['average_metrics'].items():\n",
        "            mlflow.log_metric(f\"baseline_{metric_name}\", value)\n",
        "\n",
        "        # ===== PHASE 4: EVALUATE FINE-TUNED =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\" PHASE 4: Evaluating Fine-Tuned Model\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Load fine-tuned model\n",
        "        from peft import PeftModel\n",
        "        finetuned_model = PeftModel.from_pretrained(\n",
        "            model,\n",
        "            adapter_path,\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "        finetuned_results = evaluator.evaluate_model(\n",
        "            finetuned_model,\n",
        "            tokenizer,\n",
        "            test_path,\n",
        "            model_name=\"Fine-Tuned\"\n",
        "        )\n",
        "\n",
        "        # Log fine-tuned metrics\n",
        "        for metric_name, value in finetuned_results['average_metrics'].items():\n",
        "            mlflow.log_metric(f\"finetuned_{metric_name}\", value)\n",
        "\n",
        "        # ===== PHASE 5: CALCULATE IMPROVEMENT =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\" PHASE 5: Improvement Analysis\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        improvements = {}\n",
        "        for metric in baseline_results['average_metrics'].keys():\n",
        "            baseline_val = baseline_results['average_metrics'][metric]\n",
        "            finetuned_val = finetuned_results['average_metrics'][metric]\n",
        "            improvement = finetuned_val - baseline_val\n",
        "            improvements[metric] = improvement\n",
        "\n",
        "            mlflow.log_metric(f\"improvement_{metric}\", improvement)\n",
        "\n",
        "            # Print comparison\n",
        "            trend_label = \"[IMPROVED]\" if improvement > 0 else \"[DECLINED]\"\n",
        "            print(f\"{trend_label} {metric:20s}: {baseline_val:.4f} -> {finetuned_val:.4f} (Diff: {improvement:+.4f})\")\n",
        "\n",
        "        # ===== PHASE 6: QUALITY GATE =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"PHASE 6: Quality Gate Check\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Define thresholds\n",
        "        QUALITY_GATES = {\n",
        "            'rouge_l': 0.05,  # Must improve by at least 5%\n",
        "            'semantic_similarity': 0.05,  # Must improve by at least 5%\n",
        "        }\n",
        "\n",
        "        # Check if model passes all gates\n",
        "        gates_passed = {}\n",
        "        for metric, threshold in QUALITY_GATES.items():\n",
        "            passed = improvements[metric] >= threshold\n",
        "            gates_passed[metric] = passed\n",
        "\n",
        "            status = \"PASS\" if passed else \"FAIL\"\n",
        "            print(f\"{status} {metric}: {improvements[metric]:+.4f} (threshold: {threshold:+.4f})\")\n",
        "\n",
        "            mlflow.log_metric(f\"gate_{metric}_passed\", int(passed))\n",
        "\n",
        "        all_gates_passed = all(gates_passed.values())\n",
        "        mlflow.log_metric(\"all_gates_passed\", int(all_gates_passed))\n",
        "\n",
        "        # ===== PHASE 7: CONDITIONAL DEPLOYMENT =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"PHASE 7: Deployment Decision\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        if all_gates_passed:\n",
        "            print(\"All quality gates PASSED!\")\n",
        "            print(\"   Model is ready for deployment\")\n",
        "\n",
        "            mlflow.set_tag(\"status\", \"approved\")\n",
        "            mlflow.set_tag(\"deployment_decision\", \"deploy\")\n",
        "\n",
        "            # Log the adapter as MLflow artifact\n",
        "            # Log the adapter as MLflow artifact\n",
        "            mlflow.log_artifacts(adapter_path, artifact_path=\"adapter\")\n",
        "\n",
        "            # Register model (may not be supported by DagsHub)\n",
        "            try:\n",
        "                model_uri = f\"runs:/{run_id}/adapter\"\n",
        "                model_version = mlflow.register_model(model_uri, \"chatbot-model-prod\")\n",
        "                print(f\"\\nModel registered: {model_version.name} v{model_version.version}\")\n",
        "            except Exception as e:\n",
        "                print(f\"\\n⚠️ Model registration skipped (not supported by DagsHub)\")\n",
        "                print(f\"   Your adapter is saved as an artifact in the run\")\n",
        "\n",
        "            # Push to HuggingFace if requested\n",
        "            if push_to_hub and hub_repo_name:\n",
        "                print(f\"\\nPushing to HuggingFace: {hub_repo_name}\")\n",
        "                try:\n",
        "                    finetuned_model.push_to_hub(hub_repo_name)\n",
        "                    tokenizer.push_to_hub(hub_repo_name)\n",
        "                    mlflow.set_tag(\"huggingface_repo\", hub_repo_name)\n",
        "                    print(f\" SUCCESS: Pushed to https://huggingface.co/{hub_repo_name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"FAILED: HuggingFace push failed: {e}\")\n",
        "\n",
        "        else:\n",
        "            print(\"Quality gates FAILED\")\n",
        "            print(\"   Model will NOT be deployed\")\n",
        "            print(\"\\n   Failed gates:\")\n",
        "            for metric, passed in gates_passed.items():\n",
        "                if not passed:\n",
        "                    print(f\"      • {metric}: {improvements[metric]:+.4f} < {QUALITY_GATES[metric]:+.4f}\")\n",
        "\n",
        "            mlflow.set_tag(\"status\", \"rejected\")\n",
        "            mlflow.set_tag(\"deployment_decision\", \"do_not_deploy\")\n",
        "\n",
        "        # ===== PHASE 8: SAVE EVALUATION REPORT =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"PHASE 8: Saving Evaluation Report\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Create detailed report\n",
        "        report = {\n",
        "            'run_id': run_id,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'hyperparameters': {\n",
        "                'epochs': epochs,\n",
        "                'learning_rate': learning_rate,\n",
        "                'lora_rank': 8,\n",
        "                'lora_alpha': lora_alpha,\n",
        "            },\n",
        "            'baseline_metrics': baseline_results['average_metrics'],\n",
        "            'finetuned_metrics': finetuned_results['average_metrics'],\n",
        "            'improvements': improvements,\n",
        "            'quality_gates': {\n",
        "                'passed': int(all_gates_passed),\n",
        "                'individual_results': {k: int(v) for k, v in gates_passed.items()}\n",
        "            },\n",
        "            'deployment_decision': 'deploy' if all_gates_passed else 'do_not_deploy',\n",
        "            'detailed_test_results': {\n",
        "                'baseline': baseline_results['detailed_results'],\n",
        "                'finetuned': finetuned_results['detailed_results']\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save report\n",
        "        report_path = WORKDIR / f\"evaluation_report_{run_id}.json\"\n",
        "        with open(report_path, 'w') as f:\n",
        "            json.dump(convert_to_serializable(report), f, indent=2)\n",
        "\n",
        "        mlflow.log_artifact(report_path)\n",
        "        print(f\"Report saved: {report_path.name}\")\n",
        "\n",
        "        # ===== FINAL SUMMARY =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"PIPELINE COMPLETE\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Run ID: {run_id}\")\n",
        "        print(f\"Status: {'APPROVED' if all_gates_passed else 'REJECTED'}\")\n",
        "        print(f\"Decision: {'Deploy to Production' if all_gates_passed else 'Do Not Deploy'}\")\n",
        "        print(f\"\\nView results: https://dagshub.com/PierreRamez/PersonalChatbot.mlflow#/experiments/{run_id}\")\n",
        "\n",
        "        return {\n",
        "            'run_id': run_id,\n",
        "            'status': 'approved' if all_gates_passed else 'rejected',\n",
        "            'adapter_path': adapter_path if all_gates_passed else None,\n",
        "            'improvements': improvements,\n",
        "            'gates_passed': gates_passed\n",
        "        }\n",
        "\n",
        "print(\"MLflow training pipeline ready\")\n",
        "print(\"\\n This function:\")\n",
        "print(\"   • Trains the model\")\n",
        "print(\"   • Evaluates baseline vs fine-tuned\")\n",
        "print(\"   • Applies quality gates\")\n",
        "print(\"   • Conditionally registers model\")\n",
        "print(\"   • Generates detailed report\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z07dA386s8zT"
      },
      "source": [
        "### execute the complete pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "LhIW4_bfs8BK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8df9ccdfa91c476cb9dab4aea469d796",
            "cd8893931d464c6eba626fbd058a0a6c",
            "1346f08c0bca4073b4c83574024231aa",
            "101d64006fdf4a968e4b87b1e7fea911",
            "b4c35ce60b27482498d3c88319174a7a",
            "10d8be1a5de043619204841d52b9ca71",
            "ee4d42a1938c4dfd9c1a9c60f03c5207",
            "75b96cf354b94efaa695f46168349741",
            "be9d004a0901419b8edefde301311959",
            "9a7c27ff4d7842e8b6f8e6795854de17",
            "eeb7cfd007b34fc394e295d57142d65b",
            "84e2b05bf11a4ef7a8d550213737c2e5",
            "764dfd0131f744d7aa9d64feba979d0e",
            "fa7b654639dd4b259a881b9d3b654f2b",
            "736e0aa67e92487da18bb798ee4bbd5b",
            "b94ab7f3c38d4b54ad407565a6aa8be6",
            "ea2bba77ffda4949a934b23ad78d166f",
            "0c6c621d91b1412ab37dbb62fe735913",
            "8906b1929f1b45de9c7bdc7cc9c48382",
            "24f24749c83942ea881b085e1efafcf5",
            "c658321a1d1c4b2f81dbfa56e255671a",
            "e512b04c8ae34a6cb39b75bd2ce5079b",
            "1d471da5654849658e2d7de59db5103c",
            "7bc862eaec1d4bc2a793609a94c2df80",
            "5df56fba3de34d9eb6b80c2d7f41186a",
            "2a1e966d3bfb40b8b1245c0a068f6635",
            "0d8a6a6db31942d8b59d05de0628872e",
            "6ebfae3af70e46a3aef7022de76f5590",
            "4738e549cc67470080bcf8aa2b5afef4",
            "412b85342bfa4c7380ab4f70f0e8a4c1",
            "9f7699382267467da0185e7be7e369a3",
            "ba6b31d2743b450bb1a520114cc5170d",
            "333fc4b0c63049eb95a4c1ca50531af7",
            "390c0685b2b4414e94d2be65b98ac1e0",
            "3d3c0f60b09148e7998249cf84a17bd5",
            "a4c8cdea090e401d804272bae8d8d11d",
            "44d0b213727a4daaba2d84cb6aabbd89",
            "b08bb594c222440bb9c37345a5a81e3c",
            "a0d97b4d021145919bf3f1028a9af680",
            "cb2f00f26138420b83914960089090e0",
            "59dfa520fb4d4342b9aa1f101febf6c9",
            "6c976af9a18642fe84c5eb53993824a7",
            "986a0d479c2545d2a49ace88b0212386",
            "dd276c7fa1e14aa8941fb9af65e5d339",
            "0655ced2782740338fb137c6233b760a",
            "0c00b183a70f4f7188ab3cb6c39fc12d",
            "c4bd6bc6d6da4a8a953268e2f7bfaaa0",
            "235a52fe0cd34b879e392aebe1ade80e",
            "9ddfaf109e914c20a83cc27aed7f9505",
            "1dff83317c6e4e04875f0b4ae26d25af",
            "ff1b193a05b34a84b78b135cae623481",
            "5fa81aab068b483189a06b3d06c42b3c",
            "0eb63cf54ede407398cfed1f392b6fd0",
            "08f24ac441f243059f81bc5629bbf2a7",
            "2eea09af9c9342a68d3539f6f1b07ea4",
            "9b91945be3b34afd9d628e8a9fe690fc",
            "04d791ab4c9b4002a289278ea6a77a99",
            "a1570be5e6ea4052a8123f747d242d52",
            "775e2c6e4773489f86321c5f2ed9d8f4",
            "ef373d4b522e42a78777a85e5d0ee269",
            "d335f0d82d644f7c988137f5f8f9016a",
            "ebc507ec898d436fa4d03df0290eb5c5",
            "677e5a2e5b004571854c5c16ba5cddd8",
            "9a6b90aae21947788e7b8b1b922004fe",
            "30bc3cbb57614a428f4d242733c8cfc3",
            "755f183e85aa4fb2a0bdb1b4b96eb3be",
            "a2719d1f555d4fb9a030579f8d456544",
            "e41985c6187d4678bcd929ce371cad5b",
            "4fa90f0a951041dea0056fd484440eb8",
            "53b86ffcb11346369221e39acd1abded",
            "4f3a64d813764d98ae186a1e25da1b74",
            "bad0126821d249c7a43a70a068551266",
            "996efc46f1814e63bd67e2e36b24ba75",
            "6cbfc366a0a14680bf622dd7fb81cae2",
            "756d770594dd495a9d94e10745cc7357",
            "c7fdf05fba094092b050de6a8fb4e2c5",
            "8796ee0b64c241beb118e38d9c260f02",
            "fcdbbf0fbb664bbcb24a0e5430da525f",
            "0e13ce1f25ba429fbc4e3f1f2f74b5cf",
            "d5c5fb00a35a44efa42678dc10db466a",
            "9ec1d2d760b34cf7ac8cb5a1f1451a95",
            "e9a5686fdad244ba952a2f7e69475a25",
            "c0d6839e84d1474a87696b116db4d7b9",
            "6a9be22678f544be8e9faf2f03112f09",
            "00a473a4a809417e8bf66318fa7949a5",
            "b766cf6cd5f24055a15e5f0006f3b18e",
            "94eb5a8b531a445bac9b169e350f0e03",
            "b2030c9a88a347a2806c8c28957c7f67",
            "19f224e0b4aa4462b97fc62b18477f65",
            "b56145b1aef0498bb18940ebab1d5c2b",
            "9d47a9422684422db343d456649d1708",
            "eafa53c4be494bf9b54f15cd8002fbc6",
            "882b1395798f4ac7be0aa71ad40b20c0",
            "b2dd256513fc4997a13c390fdf49be95",
            "1f94e5e25ea24ed29e369ac2e2c46b3f",
            "e37d75ea98c34b0bab5bbbca31061a97",
            "4af8d4f3c80840b2ab320b7067ee6e0d",
            "07af94a97c8f4640876e42ccc415c57f",
            "37457605805841f8b09cf1d4a848af30"
          ]
        },
        "outputId": "180c32ab-8bee-4a9c-f7c8-22d790e3c114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline Configuration:\n",
            "   Training examples: 8\n",
            "   Test examples: 3\n",
            "   Train file: train_20251130_164926.jsonl\n",
            "   Test file: test_20251130_164926.jsonl\n",
            "\n",
            "  Push to Hugging Face Hub after approval?\n",
            "   (y/N): y\n",
            "   Enter repo name (e.g., 'username/model-name'): pierreramez/Llama-3.2-3B-Instruct-bnb-4bit_finetuned\n",
            "\n",
            "======================================================================\n",
            " STARTING COMPLETE PIPELINE\n",
            "======================================================================\n",
            "This will:\n",
            "   1. Train LoRA adapter\n",
            "   2. Evaluate baseline model\n",
            "   3. Evaluate fine-tuned model\n",
            "   4. Check quality gates\n",
            "   5. Conditionally deploy\n",
            "\n",
            "   Expected time: 8-16 minutes\n",
            "======================================================================\n",
            "\n",
            "  Start pipeline? (y/N): y\n",
            "\n",
            "======================================================================\n",
            " TRAINING WITH MLFLOW TRACKING\n",
            "======================================================================\n",
            "\n",
            " MLflow Run ID: d98905f476874593a2fbd27b42c1c9aa\n",
            " Track at: https://dagshub.com/PierreRamez/PersonalChatbot.mlflow\n",
            "\n",
            "======================================================================\n",
            " PHASE 1: Logging Hyperparameters\n",
            "======================================================================\n",
            " Hyperparameters logged\n",
            "\n",
            "======================================================================\n",
            " PHASE 2: Training\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            " STARTING LORA TRAINING\n",
            "======================================================================\n",
            "\n",
            " Initial GPU memory: Allocated: 2.97GB, Reserved: 4.56GB\n",
            "\n",
            " Setting up tokenizer...\n",
            "   Pad token: <|finetune_right_pad_id|>\n",
            "\n",
            " Preparing dataset...\n",
            " Loading dataset from: /content/personalized_chatbot/finetune_prep/train_20251130_164926.jsonl\n",
            "   Loaded 8 examples\n",
            "\n",
            " Sample (messages format):\n",
            "{'messages': [{'role': 'user', 'content': 'Define the chemical structure of the fictional compound ‘Trihexyl Oxynitride-7’ using precise IUPAC terminology.'}, {'role': 'assistant', 'content': 'That compound doesn’t exist in any chemical database, so there is no structure or IUPAC name to provide.'}]}\n",
            "\n",
            " Applying chat template...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8df9ccdfa91c476cb9dab4aea469d796"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Tokenizing...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing:   0%|          | 0/8 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84e2b05bf11a4ef7a8d550213737c2e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Tokenized 8 examples\n",
            "\n",
            " Preparing model for LoRA training...\n",
            "   This will take 2-3 minutes...\n",
            "\n",
            " Clearing GPU memory...\n",
            "   Memory freed: 2.94GB\n",
            "   GPU memory after prep: Allocated: 2.94GB, Reserved: 4.56GB\n",
            "   Model device: cuda:0\n",
            "\n",
            " Adding LoRA adapters...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Trainable params: 4,587,520 (0.25%)\n",
            "   Total params: 1,808,051,200\n",
            "   Memory: Allocated: 2.94GB, Reserved: 4.56GB\n",
            "\n",
            "  Creating trainer...\n",
            "\n",
            " Starting training...\n",
            "   This will take 10-20 minutes depending on dataset size.\n",
            "   Watch the loss - it should decrease.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:16, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " TRAINING COMPLETED!\n",
            "   Final loss: 1.8653\n",
            "\n",
            " Saving adapter to /content/personalized_chatbot/llama-lora-adapter...\n",
            "✓ Adapter saved locally\n",
            "\n",
            "======================================================================\n",
            " TRAINING COMPLETE!\n",
            "======================================================================\n",
            "Adapter location: /content/personalized_chatbot/llama-lora-adapter\n",
            "Memory used: Allocated: 2.95GB, Reserved: 4.82GB\n",
            " Training loss logged: 1.8653\n",
            "\n",
            "======================================================================\n",
            " PHASE 3: Evaluating Baseline Model\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            " EVALUATING: Baseline (Original)\n",
            "======================================================================\n",
            "Test file: test_20251130_164926.jsonl\n",
            "\n",
            "Test examples: 3\n",
            "\n",
            "[1/3] Generating... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic: 0.131 | ROUGE-L: 0.000\n",
            "[2/3] Generating... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic: 0.185 | ROUGE-L: 0.074\n",
            "[3/3] Generating... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic: 0.044 | ROUGE-L: 0.083\n",
            "\n",
            "======================================================================\n",
            " SUMMARY: Baseline (Original)\n",
            "======================================================================\n",
            "Semantic Similarity: 0.1200\n",
            "ROUGE-L:            0.0525\n",
            "BLEU:               0.0003\n",
            "Exact Match:        0.0000 (0/3)\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            " PHASE 4: Evaluating Fine-Tuned Model\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            " EVALUATING: Fine-Tuned\n",
            "======================================================================\n",
            "Test file: test_20251130_164926.jsonl\n",
            "\n",
            "Test examples: 3\n",
            "\n",
            "[1/3] Generating... Semantic: -0.018 | ROUGE-L: 0.061\n",
            "[2/3] Generating... Semantic: 0.038 | ROUGE-L: 0.071\n",
            "[3/3] Generating... Semantic: 0.528 | ROUGE-L: 0.216\n",
            "\n",
            "======================================================================\n",
            " SUMMARY: Fine-Tuned\n",
            "======================================================================\n",
            "Semantic Similarity: 0.1827\n",
            "ROUGE-L:            0.1161\n",
            "BLEU:               0.0152\n",
            "Exact Match:        0.0000 (0/3)\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            " PHASE 5: Improvement Analysis\n",
            "======================================================================\n",
            "[IMPROVED] semantic_similarity : 0.1200 -> 0.1827 (Diff: +0.0627)\n",
            "[IMPROVED] rouge_l             : 0.0525 -> 0.1161 (Diff: +0.0636)\n",
            "[IMPROVED] bleu                : 0.0003 -> 0.0152 (Diff: +0.0149)\n",
            "[DECLINED] exact_match         : 0.0000 -> 0.0000 (Diff: +0.0000)\n",
            "\n",
            "======================================================================\n",
            "PHASE 6: Quality Gate Check\n",
            "======================================================================\n",
            "PASS rouge_l: +0.0636 (threshold: +0.0500)\n",
            "PASS semantic_similarity: +0.0627 (threshold: +0.0500)\n",
            "\n",
            "======================================================================\n",
            "PHASE 7: Deployment Decision\n",
            "======================================================================\n",
            "All quality gates PASSED!\n",
            "   Model is ready for deployment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Registered model 'chatbot-model-prod' already exists. Creating a new version of this model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⚠️ Model registration skipped (not supported by DagsHub)\n",
            "   Your adapter is saved as an artifact in the run\n",
            "\n",
            "Pushing to HuggingFace: pierreramez/Llama-3.2-3B-Instruct-bnb-4bit_finetuned\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/624 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d471da5654849658e2d7de59db5103c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "390c0685b2b4414e94d2be65b98ac1e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0655ced2782740338fb137c6233b760a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...adapter_model.safetensors:   3%|3         |  568kB / 18.4MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b91945be3b34afd9d628e8a9fe690fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2719d1f555d4fb9a030579f8d456544"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcdbbf0fbb664bbcb24a0e5430da525f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...mptb56uipl/tokenizer.json: 100%|##########| 17.2MB / 17.2MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19f224e0b4aa4462b97fc62b18477f65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " SUCCESS: Pushed to https://huggingface.co/pierreramez/Llama-3.2-3B-Instruct-bnb-4bit_finetuned\n",
            "\n",
            "======================================================================\n",
            "PHASE 8: Saving Evaluation Report\n",
            "======================================================================\n",
            "Report saved: evaluation_report_d98905f476874593a2fbd27b42c1c9aa.json\n",
            "\n",
            "======================================================================\n",
            "PIPELINE COMPLETE\n",
            "======================================================================\n",
            "Run ID: d98905f476874593a2fbd27b42c1c9aa\n",
            "Status: APPROVED\n",
            "Decision: Deploy to Production\n",
            "\n",
            "View results: https://dagshub.com/PierreRamez/PersonalChatbot.mlflow#/experiments/d98905f476874593a2fbd27b42c1c9aa\n",
            "🏃 View run silent-fox-759 at: https://dagshub.com/PierreRamez/PersonalChatbot.mlflow/#/experiments/0/runs/d98905f476874593a2fbd27b42c1c9aa\n",
            "🧪 View experiment at: https://dagshub.com/PierreRamez/PersonalChatbot.mlflow/#/experiments/0\n",
            "\n",
            "======================================================================\n",
            " PIPELINE EXECUTION COMPLETE\n",
            "======================================================================\n",
            "Status: APPROVED\n",
            "Run ID: d98905f476874593a2fbd27b42c1c9aa\n",
            "\n",
            " Model APPROVED for production!\n",
            "   Adapter saved: /content/personalized_chatbot/llama-lora-adapter\n",
            "\n",
            " Next steps:\n",
            "   1. Test the deployed model\n",
            "   2. Update your production inference\n",
            "   3. Monitor performance in production\n"
          ]
        }
      ],
      "source": [
        "# Cell 13G: Execute Complete Training + Evaluation Pipeline\n",
        "\n",
        "# Check if we have data from Cell 12C\n",
        "if 'result' not in globals() or result['status'] != 'success':\n",
        "    print(\"No training data available!\")\n",
        "    print(\"\\nPlease run Cell 12C first to prepare data\")\n",
        "else:\n",
        "    print(\"Pipeline Configuration:\")\n",
        "    print(f\"   Training examples: {result['num_train_examples']}\")\n",
        "    print(f\"   Test examples: {result['num_test_examples']}\")\n",
        "    print(f\"   Train file: {Path(result['train_dataset_path']).name}\")\n",
        "    print(f\"   Test file: {Path(result['test_dataset_path']).name}\")\n",
        "\n",
        "    # Ask for HuggingFace push\n",
        "    print(\"\\n  Push to Hugging Face Hub after approval?\")\n",
        "    push = input(\"   (y/N): \").strip().lower() == 'y'\n",
        "\n",
        "    hub_repo = None\n",
        "    if push:\n",
        "        hub_repo = input(\"   Enter repo name (e.g., 'username/model-name'): \").strip()\n",
        "        if not hub_repo:\n",
        "            print(\"     No repo name - will skip HuggingFace push\")\n",
        "            push = False\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\" STARTING COMPLETE PIPELINE\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"This will:\")\n",
        "    print(\"   1. Train LoRA adapter\")\n",
        "    print(\"   2. Evaluate baseline model\")\n",
        "    print(\"   3. Evaluate fine-tuned model\")\n",
        "    print(\"   4. Check quality gates\")\n",
        "    print(\"   5. Conditionally deploy\")\n",
        "    print(f\"\\n   Expected time: {result['num_train_examples'] * 2 * 0.5:.0f}-{result['num_train_examples'] * 2:.0f} minutes\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    confirm = input(\"\\n  Start pipeline? (y/N): \").strip().lower()\n",
        "\n",
        "    if confirm == 'y':\n",
        "        # Clear GPU memory\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # RUN THE COMPLETE PIPELINE\n",
        "        final_result = train_and_evaluate_with_mlflow(\n",
        "            train_dataset_path=result['train_dataset_path'],\n",
        "            test_dataset_path=result['test_dataset_path'],\n",
        "            output_name=\"llama-lora-adapter\",\n",
        "            push_to_hub=push,\n",
        "            hub_repo_name=hub_repo if push else None\n",
        "        )\n",
        "\n",
        "        if final_result:\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(\" PIPELINE EXECUTION COMPLETE\")\n",
        "            print(f\"{'='*70}\")\n",
        "            print(f\"Status: {final_result['status'].upper()}\")\n",
        "            print(f\"Run ID: {final_result['run_id']}\")\n",
        "\n",
        "            if final_result['status'] == 'approved':\n",
        "                print(f\"\\n Model APPROVED for production!\")\n",
        "                print(f\"   Adapter saved: {final_result['adapter_path']}\")\n",
        "                print(f\"\\n Next steps:\")\n",
        "                print(f\"   1. Test the deployed model\")\n",
        "                print(f\"   2. Update your production inference\")\n",
        "                print(f\"   3. Monitor performance in production\")\n",
        "            else:\n",
        "                print(f\"\\n  Model REJECTED - did not meet quality standards\")\n",
        "                print(f\"\\n   Recommendations:\")\n",
        "                print(f\"   1. Collect more diverse corrections\")\n",
        "                print(f\"   2. Adjust hyperparameters\")\n",
        "                print(f\"   3. Review failed quality gates above\")\n",
        "    else:\n",
        "        print(\"\\nPipeline cancelled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_3CJw9ar0GF"
      },
      "source": [
        "### Upload New adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "iU7Mqtmir2JA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dd9ffc9-3837-4810-d34a-7470f56b3c9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model APPROVED for deployment!\n",
            "Run ID: d98905f476874593a2fbd27b42c1c9aa\n",
            "Adapter: /content/personalized_chatbot/llama-lora-adapter\n",
            "\n",
            "Production API not enabled (USE_PRODUCTION_API = False)\n",
            "\n",
            "   To deploy:\n",
            "   1. Deploy backend to HuggingFace Spaces\n",
            "   2. Set USE_PRODUCTION_API = True in Cell 23A\n",
            "   3. Run this cell again\n",
            "\n",
            "   Or manually update your backend:\n",
            "   • Go to your backend Space\n",
            "   • Edit app.py\n",
            "   • Update adapter_path in startup_event()\n"
          ]
        }
      ],
      "source": [
        "# Cell 23C: Deploy New Adapter (Run after Cell 13G)\n",
        "import requests\n",
        "\n",
        "# Check if there's an approved model\n",
        "if 'final_result' not in globals():\n",
        "    print(\"No training result found\")\n",
        "    print(\"Run Cell 13G first to train a model\")\n",
        "elif not final_result:\n",
        "    print(\"Training result is None (training may have failed)\")\n",
        "elif final_result.get('status') != 'approved':\n",
        "    print(f\"Model status: {final_result.get('status')}\")\n",
        "    if final_result.get('status') == 'rejected':\n",
        "        print(\"   Model did not pass quality gates\")\n",
        "        print(\"   Collect more diverse corrections and retrain\")\n",
        "else:\n",
        "    # Model approved!\n",
        "    print(\"Model APPROVED for deployment!\")\n",
        "    print(f\"Run ID: {final_result.get('run_id')}\")\n",
        "    print(f\"Adapter: {final_result.get('adapter_path')}\")\n",
        "\n",
        "    # Check production API\n",
        "    if 'USE_PRODUCTION_API' not in globals():\n",
        "        print(\"\\n USE_PRODUCTION_API not defined\")\n",
        "        print(\"Run Cell 23A first\")\n",
        "    elif not USE_PRODUCTION_API:\n",
        "        print(\"\\nProduction API not enabled (USE_PRODUCTION_API = False)\")\n",
        "        print(\"\\n   To deploy:\")\n",
        "        print(\"   1. Deploy backend to HuggingFace Spaces\")\n",
        "        print(\"   2. Set USE_PRODUCTION_API = True in Cell 23A\")\n",
        "        print(\"   3. Run this cell again\")\n",
        "        print(\"\\n   Or manually update your backend:\")\n",
        "        print(\"   • Go to your backend Space\")\n",
        "        print(\"   • Edit app.py\")\n",
        "        print(\"   • Update adapter_path in startup_event()\")\n",
        "    else:\n",
        "        # Production API enabled\n",
        "        adapter_name = input(\"\\nEnter HF Hub adapter name (e.g., 'username/adapter-v2'): \").strip()\n",
        "\n",
        "        if not adapter_name:\n",
        "            print(\"No adapter name provided - skipping deployment\")\n",
        "        else:\n",
        "            print(f\"\\nDeploying adapter: {adapter_name}\")\n",
        "            print(\"   This will hot-reload your production backend...\\n\")\n",
        "\n",
        "            try:\n",
        "                # Hot reload\n",
        "                reload_resp = requests.post(\n",
        "                    f\"{PRODUCTION_API_URL}/reload-adapter\",\n",
        "                    json={\"adapter_path\": adapter_name},\n",
        "                    timeout=60\n",
        "                )\n",
        "\n",
        "                if reload_resp.status_code == 200:\n",
        "                    result_data = reload_resp.json()\n",
        "                    print(\"Production backend updated successfully!\")\n",
        "                    print(f\"Current adapter: {result_data.get('adapter')}\")\n",
        "                    print(\"New model is now serving requests\")\n",
        "\n",
        "                    # Ask to clear feedback\n",
        "                    print(\"\\n Clear production feedback to start fresh cycle?\")\n",
        "                    clear = input(\"   This deletes old corrections (y/N): \").strip()\n",
        "\n",
        "                    if clear.lower() == 'y':\n",
        "                        try:\n",
        "                            clear_resp = requests.post(\n",
        "                                f\"{PRODUCTION_API_URL}/clear-feedback\",\n",
        "                                timeout=10\n",
        "                            )\n",
        "                            if clear_resp.status_code == 200:\n",
        "                                print(\"Production feedback cleared\")\n",
        "                                print(\"Ready to collect new corrections\")\n",
        "                            else:\n",
        "                                print(f\"Clear failed: {clear_resp.status_code}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error clearing feedback: {e}\")\n",
        "                    else:\n",
        "                        print(\"Keeping existing feedback\")\n",
        "\n",
        "                else:\n",
        "                    print(f\"Reload failed: {reload_resp.status_code}\")\n",
        "                    print(f\"Response: {reload_resp.text}\")\n",
        "                    print(\"\\nManual deployment:\")\n",
        "                    print(\"   1. Go to your backend Space on HuggingFace\")\n",
        "                    print(\"   2. Edit app.py\")\n",
        "                    print(f\"   3. Set adapter_path='{adapter_name}'\")\n",
        "                    print(\"   4. Commit changes\")\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                print(\"Request timed out\")\n",
        "                print(\"Backend might be reloading - check Space logs\")\n",
        "            except requests.exceptions.ConnectionError:\n",
        "                print(\"Cannot connect to backend\")\n",
        "                print(\"Check if PRODUCTION_API_URL is correct\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlZxM7vFMq7o"
      },
      "source": [
        "##Test the Trained Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "3i4IQWTpMne2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52258d84-e311-467d-af83-4e4537fd2fd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adapter found!\n",
            "Location: /content/personalized_chatbot/llama-lora-adapter\n",
            "\n",
            "Loading test example from your corrections...\n",
            "\n",
            "Test case from your corrections:\n",
            "======================================================================\n",
            "1. USER User: Describe the loss landscape of a self-dual neural manifold under Symplectic SGD....\n",
            "\n",
            "Expected output (what you corrected to):\n",
            "   This entire construct is fictional and undefined....\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Loading adapter for testing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCCESS: Adapter loaded\n",
            "\n",
            "Test conversation:\n",
            "   USER User: Describe the loss landscape of a self-dual neural manifold under Symplectic SGD.\n",
            "\n",
            "🤖 Generating response...\n",
            "\n",
            "🤖 Response: \n",
            "\n",
            "======================================================================\n",
            "\n",
            "Compare the response to your expected output.\n",
            "   Does it match better than before training?\n",
            "\n",
            "Word overlap: 0.0%\n",
            "Low match - may need more training\n",
            "\n",
            "======================================================================\n",
            "You can also test manually:\n",
            "test_adapter_inference(\n",
            "adapter_path,\n",
            "[{'role': 'user', 'content': 'Your question here'}]\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Cell 14: Test Trained Adapter (FIXED for Messages Format)\n",
        "\n",
        "def test_adapter_inference(adapter_path: Path, test_messages: list):\n",
        "    \"\"\"\n",
        "    Load the trained adapter and test it on a conversation.\n",
        "\n",
        "    This shows you if the training actually improved the model.\n",
        "\n",
        "    Args:\n",
        "        adapter_path: Path to the LoRA adapter\n",
        "        test_messages: List of messages in format [{\"role\": \"user\", \"content\": \"...\"}]\n",
        "    \"\"\"\n",
        "    from peft import PeftModel\n",
        "\n",
        "    print(\"\\nLoading adapter for testing...\")\n",
        "\n",
        "    # Load adapter on top of base model\n",
        "    test_model = PeftModel.from_pretrained(\n",
        "        model,  # Base model from Cell 3\n",
        "        adapter_path,\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    print(\"SUCCESS: Adapter loaded\")\n",
        "\n",
        "    # Format using chat template\n",
        "    print(f\"\\nTest conversation:\")\n",
        "    for msg in test_messages:\n",
        "        role_label = \"USER\" if msg[\"role\"] == \"user\" else \"ASSISTANT\"\n",
        "        print(f\"   {role_label} {msg['role'].title()}: {msg['content']}\")\n",
        "\n",
        "    print(\"\\n🤖 Generating response...\\n\")\n",
        "\n",
        "    # Use chat template\n",
        "    try:\n",
        "        formatted_prompt = tokenizer.apply_chat_template(\n",
        "            test_messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "    except:\n",
        "        # Fallback if chat template not available\n",
        "        formatted_prompt = \"\"\n",
        "        for msg in test_messages:\n",
        "            if msg[\"role\"] == \"user\":\n",
        "                formatted_prompt += f\"User: {msg['content']}\\n\"\n",
        "            else:\n",
        "                formatted_prompt += f\"Assistant: {msg['content']}\\n\"\n",
        "        formatted_prompt += \"Assistant:\"\n",
        "\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = test_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    # Decode full response\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract just the new response (after the prompt)\n",
        "    response_text = full_response[len(formatted_prompt):].strip()\n",
        "\n",
        "    print(\"🤖 Response:\", response_text)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    return response_text\n",
        "\n",
        "\n",
        "# Check if adapter exists\n",
        "adapter_path = WORKDIR / \"llama-lora-adapter\"\n",
        "\n",
        "if not adapter_path.exists():\n",
        "    print(\"No adapter found. Train first using Cell 13G.\")\n",
        "else:\n",
        "    print(\"Adapter found!\")\n",
        "    print(f\"Location: {adapter_path}\")\n",
        "\n",
        "    # Check if we have test data from Cell 12C\n",
        "    if 'result' in globals() and result.get('test_dataset_path'):\n",
        "        test_file = Path(result['test_dataset_path'])\n",
        "\n",
        "        if test_file.exists():\n",
        "            print(f\"\\nLoading test example from your corrections...\")\n",
        "\n",
        "            # Read first test example\n",
        "            with open(test_file, 'r') as f:\n",
        "                first_example = json.loads(f.readline())\n",
        "\n",
        "            # Extract messages\n",
        "            if 'messages' in first_example:\n",
        "                messages = first_example['messages']\n",
        "\n",
        "                # Split into input (all but last) and expected output (last)\n",
        "                input_messages = [msg for msg in messages if msg['role'] == 'user']\n",
        "                expected_outputs = [msg for msg in messages if msg['role'] == 'assistant']\n",
        "\n",
        "                # Show what we're testing\n",
        "                print(\"\\nTest case from your corrections:\")\n",
        "                print(\"=\"*70)\n",
        "\n",
        "                for i, msg in enumerate(messages[:-1], 1):\n",
        "                    role_label = \"USER\" if msg[\"role\"] == \"user\" else \"ASSISTANT\"\n",
        "                    print(f\"{i}. {role_label} {msg['role'].title()}: {msg['content'][:80]}...\")\n",
        "\n",
        "                if expected_outputs:\n",
        "                    print(f\"\\nExpected output (what you corrected to):\")\n",
        "                    print(f\"   {expected_outputs[-1]['content'][:150]}...\")\n",
        "\n",
        "                print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "                # Test with just the user messages (let model generate assistant response)\n",
        "                test_messages = messages[:-1]  # All but the last message\n",
        "\n",
        "                # Run inference\n",
        "                result_text = test_adapter_inference(adapter_path, test_messages)\n",
        "\n",
        "                print(\"\\nCompare the response to your expected output.\")\n",
        "                print(\"   Does it match better than before training?\")\n",
        "\n",
        "                # Simple similarity check\n",
        "                if expected_outputs:\n",
        "                    expected = expected_outputs[-1]['content'].lower()\n",
        "                    actual = result_text.lower()\n",
        "\n",
        "                    # Very basic similarity (word overlap)\n",
        "                    expected_words = set(expected.split())\n",
        "                    actual_words = set(actual.split())\n",
        "                    overlap = expected_words & actual_words\n",
        "\n",
        "                    if len(expected_words) > 0:\n",
        "                        similarity = len(overlap) / len(expected_words)\n",
        "                        print(f\"\\nWord overlap: {similarity*100:.1f}%\")\n",
        "\n",
        "                        if similarity > 0.5:\n",
        "                            print(\"Good match!\")\n",
        "                        elif similarity > 0.3:\n",
        "                            print(\"Partial match\")\n",
        "                        else:\n",
        "                            print(\"Low match - may need more training\")\n",
        "            else:\n",
        "                print(\"Unexpected data format in test file\")\n",
        "        else:\n",
        "            print(f\"Test file not found: {test_file}\")\n",
        "            print(\"\\nRun Cell 12C first to prepare test data\")\n",
        "    else:\n",
        "        print(\"\\nNo test data available\")\n",
        "        print(\"   Run Cell 12C first to prepare train/test datasets\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"You can also test manually:\")\n",
        "    print(\"test_adapter_inference(\")\n",
        "    print(\"adapter_path,\")\n",
        "    print(\"[{'role': 'user', 'content': 'Your question here'}]\")\n",
        "    print(\")\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BMrJ979Mv6q"
      },
      "source": [
        "### Load Adapter for production use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "5AZc8vOYMyg_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "633b06c6-6828-4079-d5d9-d77718086eb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adapter found! Loading into model...\n",
            "Loading adapter from /content/personalized_chatbot/llama-lora-adapter...\n",
            "Merging adapter with base model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model updated with your corrections!\n",
            "\n",
            "PRODUCTION MODEL UPDATED!\n",
            "   Your chat loop (Cell 11) now uses the fine-tuned model.\n",
            "   Go back to Cell 11 and test it!\n"
          ]
        }
      ],
      "source": [
        "# Cell 15: Load Adapter for Production Inference\n",
        "\n",
        "from peft import PeftModel\n",
        "\n",
        "def load_model_with_adapter(base_model, adapter_path: Path):\n",
        "    \"\"\"\n",
        "    Load the adapter for production use.\n",
        "\n",
        "    This replaces your original model with the fine-tuned version.\n",
        "    \"\"\"\n",
        "    print(f\"Loading adapter from {adapter_path}...\")\n",
        "\n",
        "    adapted_model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        adapter_path,\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # Merge adapter into base model for faster inference (optional)\n",
        "    print(\"Merging adapter with base model...\")\n",
        "    merged_model = adapted_model.merge_and_unload()\n",
        "\n",
        "    print(\"Model updated with your corrections!\")\n",
        "    return merged_model\n",
        "\n",
        "# Load adapter if exists\n",
        "adapter_path = WORKDIR / \"llama-lora-adapter\"\n",
        "\n",
        "if adapter_path.exists():\n",
        "    print(\"Adapter found! Loading into model...\")\n",
        "\n",
        "    # Update the global 'model' variable\n",
        "    model = load_model_with_adapter(model, adapter_path)\n",
        "\n",
        "    print(\"\\nPRODUCTION MODEL UPDATED!\")\n",
        "    print(\"   Your chat loop (Cell 11) now uses the fine-tuned model.\")\n",
        "    print(\"   Go back to Cell 11 and test it!\")\n",
        "else:\n",
        "    print(\"No adapter found. Train first (Cell 13D).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "5Vn9Ck087am3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4755194-dd71-4634-9b28-ce9d3ab20915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing connection to: https://pierreramez-chatbot-api.hf.space...\n",
            "\n",
            "SUCCESS! Connected to backend.\n",
            "   Status: healthy\n",
            "   Device: cpu\n",
            "   Current Model: pierreramez/Llama-3.2-3B-Instruct-bnb-4bit_finetuned\n",
            "\n",
            "Production Stats:\n",
            "Total Interactions: 34\n",
            "Corrections available: 1\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "PRODUCTION_API_URL  = \"https://pierreramez-chatbot-api.hf.space\"\n",
        "print(f\"Testing connection to: {PRODUCTION_API_URL}...\")\n",
        "\n",
        "try:\n",
        "    # 1. Check Health (Basic connectivity)\n",
        "    health_resp = requests.get(f\"{PRODUCTION_API_URL}/health\", timeout=15)\n",
        "\n",
        "    if health_resp.status_code == 200:\n",
        "        data = health_resp.json()\n",
        "        print(\"\\nSUCCESS! Connected to backend.\")\n",
        "        print(f\"   Status: {data.get('status')}\")\n",
        "        print(f\"   Device: {data.get('device')}\")\n",
        "        print(f\"   Current Model: {data.get('current_adapter') or 'Base Model'}\")\n",
        "\n",
        "        # 2. Check Feedback Endpoint (Permissions check)\n",
        "        count_resp = requests.get(f\"{PRODUCTION_API_URL}/correction-count\", timeout=10)\n",
        "        if count_resp.status_code == 200:\n",
        "            count_data = count_resp.json()\n",
        "            print(f\"\\nProduction Stats:\")\n",
        "            print(f\"Total Interactions: {count_data.get('total')}\")\n",
        "            print(f\"Corrections available: {count_data.get('corrections')}\")\n",
        "        else:\n",
        "            print(f\"\\nConnected, but '/correction-count' failed: {count_resp.status_code}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"\\nBackend is running but returned error: {health_resp.status_code}\")\n",
        "        print(health_resp.text)\n",
        "\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\"\\nConnection Failed.\")\n",
        "    print(\"1. Check if the URL in the previous cell is correct.\")\n",
        "    print(\"2. Go to your Space and ensure it shows 'Running' (Green badge).\")\n",
        "    print(\"3. If it says 'Building' or 'Runtime Error', check the Space Logs.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nUnexpected Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uDbOM2-8B71a"
      },
      "execution_count": 60,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "619e16d6e21f41bcbd85a1518b0ee4af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_940745e973384a639f83c23ef2f4e589",
              "IPY_MODEL_cf696f33d4224c8ca918f308d290b9e7",
              "IPY_MODEL_d30cde73f3a045fe9d9c9f9ce3c084d6"
            ],
            "layout": "IPY_MODEL_5748def886f94e359b0951ad15357306"
          }
        },
        "940745e973384a639f83c23ef2f4e589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9173a9fffd04d89a6e159c58ec5c791",
            "placeholder": "​",
            "style": "IPY_MODEL_c49cc03945a145b29a6ab6bcc5f129e5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cf696f33d4224c8ca918f308d290b9e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_535c6abde8df4edba6e7eba5769c4db0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c52f7d8cfba942488ee722a57ec0513e",
            "value": 2
          }
        },
        "d30cde73f3a045fe9d9c9f9ce3c084d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2c6b9b47af44127825d688d1d972ab9",
            "placeholder": "​",
            "style": "IPY_MODEL_9e885cb387a0436b8f7d3b21867681cf",
            "value": " 2/2 [00:43&lt;00:00, 19.93s/it]"
          }
        },
        "5748def886f94e359b0951ad15357306": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9173a9fffd04d89a6e159c58ec5c791": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c49cc03945a145b29a6ab6bcc5f129e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "535c6abde8df4edba6e7eba5769c4db0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c52f7d8cfba942488ee722a57ec0513e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2c6b9b47af44127825d688d1d972ab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e885cb387a0436b8f7d3b21867681cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8df9ccdfa91c476cb9dab4aea469d796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd8893931d464c6eba626fbd058a0a6c",
              "IPY_MODEL_1346f08c0bca4073b4c83574024231aa",
              "IPY_MODEL_101d64006fdf4a968e4b87b1e7fea911"
            ],
            "layout": "IPY_MODEL_b4c35ce60b27482498d3c88319174a7a"
          }
        },
        "cd8893931d464c6eba626fbd058a0a6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10d8be1a5de043619204841d52b9ca71",
            "placeholder": "​",
            "style": "IPY_MODEL_ee4d42a1938c4dfd9c1a9c60f03c5207",
            "value": "Map: 100%"
          }
        },
        "1346f08c0bca4073b4c83574024231aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75b96cf354b94efaa695f46168349741",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be9d004a0901419b8edefde301311959",
            "value": 8
          }
        },
        "101d64006fdf4a968e4b87b1e7fea911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a7c27ff4d7842e8b6f8e6795854de17",
            "placeholder": "​",
            "style": "IPY_MODEL_eeb7cfd007b34fc394e295d57142d65b",
            "value": " 8/8 [00:00&lt;00:00, 174.91 examples/s]"
          }
        },
        "b4c35ce60b27482498d3c88319174a7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10d8be1a5de043619204841d52b9ca71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee4d42a1938c4dfd9c1a9c60f03c5207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75b96cf354b94efaa695f46168349741": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be9d004a0901419b8edefde301311959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a7c27ff4d7842e8b6f8e6795854de17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb7cfd007b34fc394e295d57142d65b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84e2b05bf11a4ef7a8d550213737c2e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_764dfd0131f744d7aa9d64feba979d0e",
              "IPY_MODEL_fa7b654639dd4b259a881b9d3b654f2b",
              "IPY_MODEL_736e0aa67e92487da18bb798ee4bbd5b"
            ],
            "layout": "IPY_MODEL_b94ab7f3c38d4b54ad407565a6aa8be6"
          }
        },
        "764dfd0131f744d7aa9d64feba979d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea2bba77ffda4949a934b23ad78d166f",
            "placeholder": "​",
            "style": "IPY_MODEL_0c6c621d91b1412ab37dbb62fe735913",
            "value": "Tokenizing: 100%"
          }
        },
        "fa7b654639dd4b259a881b9d3b654f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8906b1929f1b45de9c7bdc7cc9c48382",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24f24749c83942ea881b085e1efafcf5",
            "value": 8
          }
        },
        "736e0aa67e92487da18bb798ee4bbd5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c658321a1d1c4b2f81dbfa56e255671a",
            "placeholder": "​",
            "style": "IPY_MODEL_e512b04c8ae34a6cb39b75bd2ce5079b",
            "value": " 8/8 [00:00&lt;00:00, 147.11 examples/s]"
          }
        },
        "b94ab7f3c38d4b54ad407565a6aa8be6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea2bba77ffda4949a934b23ad78d166f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c6c621d91b1412ab37dbb62fe735913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8906b1929f1b45de9c7bdc7cc9c48382": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24f24749c83942ea881b085e1efafcf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c658321a1d1c4b2f81dbfa56e255671a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e512b04c8ae34a6cb39b75bd2ce5079b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d471da5654849658e2d7de59db5103c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bc862eaec1d4bc2a793609a94c2df80",
              "IPY_MODEL_5df56fba3de34d9eb6b80c2d7f41186a",
              "IPY_MODEL_2a1e966d3bfb40b8b1245c0a068f6635"
            ],
            "layout": "IPY_MODEL_0d8a6a6db31942d8b59d05de0628872e"
          }
        },
        "7bc862eaec1d4bc2a793609a94c2df80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ebfae3af70e46a3aef7022de76f5590",
            "placeholder": "​",
            "style": "IPY_MODEL_4738e549cc67470080bcf8aa2b5afef4",
            "value": "README.md: 100%"
          }
        },
        "5df56fba3de34d9eb6b80c2d7f41186a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_412b85342bfa4c7380ab4f70f0e8a4c1",
            "max": 624,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f7699382267467da0185e7be7e369a3",
            "value": 624
          }
        },
        "2a1e966d3bfb40b8b1245c0a068f6635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba6b31d2743b450bb1a520114cc5170d",
            "placeholder": "​",
            "style": "IPY_MODEL_333fc4b0c63049eb95a4c1ca50531af7",
            "value": " 624/624 [00:00&lt;00:00, 62.4kB/s]"
          }
        },
        "0d8a6a6db31942d8b59d05de0628872e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ebfae3af70e46a3aef7022de76f5590": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4738e549cc67470080bcf8aa2b5afef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "412b85342bfa4c7380ab4f70f0e8a4c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f7699382267467da0185e7be7e369a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba6b31d2743b450bb1a520114cc5170d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "333fc4b0c63049eb95a4c1ca50531af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "390c0685b2b4414e94d2be65b98ac1e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d3c0f60b09148e7998249cf84a17bd5",
              "IPY_MODEL_a4c8cdea090e401d804272bae8d8d11d",
              "IPY_MODEL_44d0b213727a4daaba2d84cb6aabbd89"
            ],
            "layout": "IPY_MODEL_b08bb594c222440bb9c37345a5a81e3c"
          }
        },
        "3d3c0f60b09148e7998249cf84a17bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0d97b4d021145919bf3f1028a9af680",
            "placeholder": "​",
            "style": "IPY_MODEL_cb2f00f26138420b83914960089090e0",
            "value": "Processing Files (1 / 1)      : 100%"
          }
        },
        "a4c8cdea090e401d804272bae8d8d11d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59dfa520fb4d4342b9aa1f101febf6c9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c976af9a18642fe84c5eb53993824a7",
            "value": 1
          }
        },
        "44d0b213727a4daaba2d84cb6aabbd89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_986a0d479c2545d2a49ace88b0212386",
            "placeholder": "​",
            "style": "IPY_MODEL_dd276c7fa1e14aa8941fb9af65e5d339",
            "value": " 18.4MB / 18.4MB, 4.18MB/s  "
          }
        },
        "b08bb594c222440bb9c37345a5a81e3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0d97b4d021145919bf3f1028a9af680": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb2f00f26138420b83914960089090e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59dfa520fb4d4342b9aa1f101febf6c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6c976af9a18642fe84c5eb53993824a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "986a0d479c2545d2a49ace88b0212386": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd276c7fa1e14aa8941fb9af65e5d339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0655ced2782740338fb137c6233b760a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c00b183a70f4f7188ab3cb6c39fc12d",
              "IPY_MODEL_c4bd6bc6d6da4a8a953268e2f7bfaaa0",
              "IPY_MODEL_235a52fe0cd34b879e392aebe1ade80e"
            ],
            "layout": "IPY_MODEL_9ddfaf109e914c20a83cc27aed7f9505"
          }
        },
        "0c00b183a70f4f7188ab3cb6c39fc12d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dff83317c6e4e04875f0b4ae26d25af",
            "placeholder": "​",
            "style": "IPY_MODEL_ff1b193a05b34a84b78b135cae623481",
            "value": "New Data Upload               : 100%"
          }
        },
        "c4bd6bc6d6da4a8a953268e2f7bfaaa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fa81aab068b483189a06b3d06c42b3c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0eb63cf54ede407398cfed1f392b6fd0",
            "value": 1
          }
        },
        "235a52fe0cd34b879e392aebe1ade80e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08f24ac441f243059f81bc5629bbf2a7",
            "placeholder": "​",
            "style": "IPY_MODEL_2eea09af9c9342a68d3539f6f1b07ea4",
            "value": " 18.4MB / 18.4MB, 4.18MB/s  "
          }
        },
        "9ddfaf109e914c20a83cc27aed7f9505": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dff83317c6e4e04875f0b4ae26d25af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff1b193a05b34a84b78b135cae623481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fa81aab068b483189a06b3d06c42b3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0eb63cf54ede407398cfed1f392b6fd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "08f24ac441f243059f81bc5629bbf2a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eea09af9c9342a68d3539f6f1b07ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b91945be3b34afd9d628e8a9fe690fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04d791ab4c9b4002a289278ea6a77a99",
              "IPY_MODEL_a1570be5e6ea4052a8123f747d242d52",
              "IPY_MODEL_775e2c6e4773489f86321c5f2ed9d8f4"
            ],
            "layout": "IPY_MODEL_ef373d4b522e42a78777a85e5d0ee269"
          }
        },
        "04d791ab4c9b4002a289278ea6a77a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d335f0d82d644f7c988137f5f8f9016a",
            "placeholder": "​",
            "style": "IPY_MODEL_ebc507ec898d436fa4d03df0290eb5c5",
            "value": "  ...adapter_model.safetensors: 100%"
          }
        },
        "a1570be5e6ea4052a8123f747d242d52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_677e5a2e5b004571854c5c16ba5cddd8",
            "max": 18379784,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a6b90aae21947788e7b8b1b922004fe",
            "value": 18379784
          }
        },
        "775e2c6e4773489f86321c5f2ed9d8f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30bc3cbb57614a428f4d242733c8cfc3",
            "placeholder": "​",
            "style": "IPY_MODEL_755f183e85aa4fb2a0bdb1b4b96eb3be",
            "value": " 18.4MB / 18.4MB            "
          }
        },
        "ef373d4b522e42a78777a85e5d0ee269": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d335f0d82d644f7c988137f5f8f9016a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebc507ec898d436fa4d03df0290eb5c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "677e5a2e5b004571854c5c16ba5cddd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a6b90aae21947788e7b8b1b922004fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30bc3cbb57614a428f4d242733c8cfc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "755f183e85aa4fb2a0bdb1b4b96eb3be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2719d1f555d4fb9a030579f8d456544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e41985c6187d4678bcd929ce371cad5b",
              "IPY_MODEL_4fa90f0a951041dea0056fd484440eb8",
              "IPY_MODEL_53b86ffcb11346369221e39acd1abded"
            ],
            "layout": "IPY_MODEL_4f3a64d813764d98ae186a1e25da1b74"
          }
        },
        "e41985c6187d4678bcd929ce371cad5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bad0126821d249c7a43a70a068551266",
            "placeholder": "​",
            "style": "IPY_MODEL_996efc46f1814e63bd67e2e36b24ba75",
            "value": "Processing Files (1 / 1)      : 100%"
          }
        },
        "4fa90f0a951041dea0056fd484440eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cbfc366a0a14680bf622dd7fb81cae2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_756d770594dd495a9d94e10745cc7357",
            "value": 1
          }
        },
        "53b86ffcb11346369221e39acd1abded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7fdf05fba094092b050de6a8fb4e2c5",
            "placeholder": "​",
            "style": "IPY_MODEL_8796ee0b64c241beb118e38d9c260f02",
            "value": " 17.2MB / 17.2MB, 28.7MB/s  "
          }
        },
        "4f3a64d813764d98ae186a1e25da1b74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bad0126821d249c7a43a70a068551266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "996efc46f1814e63bd67e2e36b24ba75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cbfc366a0a14680bf622dd7fb81cae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "756d770594dd495a9d94e10745cc7357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c7fdf05fba094092b050de6a8fb4e2c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8796ee0b64c241beb118e38d9c260f02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcdbbf0fbb664bbcb24a0e5430da525f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e13ce1f25ba429fbc4e3f1f2f74b5cf",
              "IPY_MODEL_d5c5fb00a35a44efa42678dc10db466a",
              "IPY_MODEL_9ec1d2d760b34cf7ac8cb5a1f1451a95"
            ],
            "layout": "IPY_MODEL_e9a5686fdad244ba952a2f7e69475a25"
          }
        },
        "0e13ce1f25ba429fbc4e3f1f2f74b5cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0d6839e84d1474a87696b116db4d7b9",
            "placeholder": "​",
            "style": "IPY_MODEL_6a9be22678f544be8e9faf2f03112f09",
            "value": "New Data Upload               : "
          }
        },
        "d5c5fb00a35a44efa42678dc10db466a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00a473a4a809417e8bf66318fa7949a5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b766cf6cd5f24055a15e5f0006f3b18e",
            "value": 0
          }
        },
        "9ec1d2d760b34cf7ac8cb5a1f1451a95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94eb5a8b531a445bac9b169e350f0e03",
            "placeholder": "​",
            "style": "IPY_MODEL_b2030c9a88a347a2806c8c28957c7f67",
            "value": "  0.00B /  0.00B,  0.00B/s  "
          }
        },
        "e9a5686fdad244ba952a2f7e69475a25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0d6839e84d1474a87696b116db4d7b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a9be22678f544be8e9faf2f03112f09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00a473a4a809417e8bf66318fa7949a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b766cf6cd5f24055a15e5f0006f3b18e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94eb5a8b531a445bac9b169e350f0e03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2030c9a88a347a2806c8c28957c7f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19f224e0b4aa4462b97fc62b18477f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b56145b1aef0498bb18940ebab1d5c2b",
              "IPY_MODEL_9d47a9422684422db343d456649d1708",
              "IPY_MODEL_eafa53c4be494bf9b54f15cd8002fbc6"
            ],
            "layout": "IPY_MODEL_882b1395798f4ac7be0aa71ad40b20c0"
          }
        },
        "b56145b1aef0498bb18940ebab1d5c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2dd256513fc4997a13c390fdf49be95",
            "placeholder": "​",
            "style": "IPY_MODEL_1f94e5e25ea24ed29e369ac2e2c46b3f",
            "value": "  ...mptb56uipl/tokenizer.json: 100%"
          }
        },
        "9d47a9422684422db343d456649d1708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e37d75ea98c34b0bab5bbbca31061a97",
            "max": 17209920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4af8d4f3c80840b2ab320b7067ee6e0d",
            "value": 17209920
          }
        },
        "eafa53c4be494bf9b54f15cd8002fbc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07af94a97c8f4640876e42ccc415c57f",
            "placeholder": "​",
            "style": "IPY_MODEL_37457605805841f8b09cf1d4a848af30",
            "value": " 17.2MB / 17.2MB            "
          }
        },
        "882b1395798f4ac7be0aa71ad40b20c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2dd256513fc4997a13c390fdf49be95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f94e5e25ea24ed29e369ac2e2c46b3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e37d75ea98c34b0bab5bbbca31061a97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4af8d4f3c80840b2ab320b7067ee6e0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07af94a97c8f4640876e42ccc415c57f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37457605805841f8b09cf1d4a848af30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}