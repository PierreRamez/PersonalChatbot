{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8EsYo9Z2rRc"
      },
      "source": [
        "##Run those cells if you have any problem with the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KqF34Bq6-LXI"
      },
      "outputs": [],
      "source": [
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JgXcyeYT9wyX"
      },
      "outputs": [],
      "source": [
        "# # # First, clear the current model from memory\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# import gc\n",
        "# gc.collect()\n",
        "\n",
        "# # # Check memory freed\n",
        "# !nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83HI6praRlmZ"
      },
      "source": [
        "# this is our start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-gyw-1fcih3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ee824f0-c463-42aa-83ba-02f8c725a92c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WORKDIR: /content/personalized_chatbot\n",
            "DATA_DIR: /content/personalized_chatbot/data\n",
            "FINETUNE_PREP: /content/personalized_chatbot/finetune_prep\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------\n",
        "# CLEAN PROJECT FOLDER SETUP\n",
        "# ---------------------------\n",
        "# Why this is here:\n",
        "# - Every Colab session is clean.\n",
        "# - We need a stable folder to store data, logs, HITL feedback.\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "WORKDIR = Path(\"/content/personalized_chatbot\")\n",
        "WORKDIR.mkdir(exist_ok=True)\n",
        "\n",
        "DATA_DIR = WORKDIR / \"data\"\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "HITL_FILE = WORKDIR / \"feedback.jsonl\"      # where Human-in-the-Loop corrections go\n",
        "FINETUNE_PREP = WORKDIR / \"finetune_prep\"\n",
        "FINETUNE_PREP.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"WORKDIR:\", WORKDIR)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"FINETUNE_PREP:\", FINETUNE_PREP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lLoSpVcaJ3e-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60888171-c38a-43a7-be7c-fbdc0b4c75e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extended MLOps directories:\n",
            "  Processed IDs: /content/personalized_chatbot/processed_feedback_ids.json\n",
            "  Training batches: /content/personalized_chatbot/finetune_prep\n"
          ]
        }
      ],
      "source": [
        "# Cell 1A: Extended directory structure for MLOps pipeline (FIXED)\n",
        "# Place this RIGHT AFTER your existing Cell 1 (folder setup)\n",
        "\n",
        "# Create additional directories for pipeline\n",
        "PROCESSED_IDS_FILE = WORKDIR / \"processed_feedback_ids.json\"\n",
        "\n",
        "# Use FINETUNE_PREP from your original notebook instead of creating BATCH_DIR\n",
        "if 'BATCH_DIR' not in globals():\n",
        "    BATCH_DIR = FINETUNE_PREP  # Use existing directory\n",
        "\n",
        "BATCH_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Create empty processed IDs file if doesn't exist\n",
        "if not PROCESSED_IDS_FILE.exists():\n",
        "    import json\n",
        "    with open(PROCESSED_IDS_FILE, 'w') as f:\n",
        "        json.dump([], f)\n",
        "\n",
        "print(\"Extended MLOps directories:\")\n",
        "print(f\"  Processed IDs: {PROCESSED_IDS_FILE}\")\n",
        "print(f\"  Training batches: {BATCH_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fx9Mn_AUgskz"
      },
      "outputs": [],
      "source": [
        "# LLaMA inference (transformers + bitsandbytes)\n",
        "# LoRA (peft)\n",
        "# dataset management (datasets)\n",
        "# orchestration (langchain)\n",
        "!pip install -q transformers accelerate bitsandbytes peft datasets langchain sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Lv7N_Lf4KB5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "558fa879-5550-4a7f-f349-0de9db98ec80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Pipeline dependencies installed\n",
            "  Note: Using lightweight pipeline tracking instead of Prefect\n"
          ]
        }
      ],
      "source": [
        "# Cell 2A: Install MLOps dependencies (FIXED for Colab)\n",
        "# Place this RIGHT AFTER your existing Cell 2 (pip install)\n",
        "\n",
        "# Prefect has dependency issues on Colab, so we'll use a simpler orchestration approach\n",
        "# We'll track pipeline runs manually with timestamps and logs\n",
        "\n",
        "# Install only what we absolutely need\n",
        "!pip install -q datasets pandas\n",
        "\n",
        "print(\"âœ“ Pipeline dependencies installed\")\n",
        "print(\"  Note: Using lightweight pipeline tracking instead of Prefect\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dWPoUUq_TLk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "593630fc-50a6-4b7a-a190-5d273fe776eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Pipeline orchestration ready\n",
            "  Logs directory: /content/personalized_chatbot/pipeline_logs\n"
          ]
        }
      ],
      "source": [
        "# Cell 2B: Simple Pipeline Orchestration (Prefect Alternative)\n",
        "# Place after Cell 2A\n",
        "\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Callable\n",
        "import traceback\n",
        "\n",
        "class PipelineRun:\n",
        "    \"\"\"\n",
        "    Lightweight pipeline orchestration without external dependencies.\n",
        "\n",
        "    Tracks:\n",
        "    - Task execution times\n",
        "    - Success/failure status\n",
        "    - Error messages\n",
        "    - Run metadata\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name: str, log_dir: Path):\n",
        "        self.name = name\n",
        "        self.log_dir = log_dir\n",
        "        self.log_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        self.run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.start_time = time.time()\n",
        "        self.tasks = []\n",
        "        self.status = \"running\"\n",
        "\n",
        "    def run_task(self, task_name: str, task_func: Callable, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Execute a task with error handling and logging.\n",
        "\n",
        "        Args:\n",
        "            task_name: Human-readable task name\n",
        "            task_func: Function to execute\n",
        "            *args, **kwargs: Arguments to pass to task_func\n",
        "\n",
        "        Returns:\n",
        "            Task result if successful, None if failed\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Task: {task_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        task_start = time.time()\n",
        "        task_record = {\n",
        "            \"name\": task_name,\n",
        "            \"start_time\": task_start,\n",
        "            \"status\": \"running\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Execute task\n",
        "            result = task_func(*args, **kwargs)\n",
        "\n",
        "            # Record success\n",
        "            task_record[\"status\"] = \"success\"\n",
        "            task_record[\"duration\"] = time.time() - task_start\n",
        "            task_record[\"result_summary\"] = str(result)[:200] if result else \"None\"\n",
        "\n",
        "            print(f\"Task completed in {task_record['duration']:.2f}s\")\n",
        "\n",
        "            self.tasks.append(task_record)\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            # Record failure\n",
        "            task_record[\"status\"] = \"failed\"\n",
        "            task_record[\"duration\"] = time.time() - task_start\n",
        "            task_record[\"error\"] = str(e)\n",
        "            task_record[\"traceback\"] = traceback.format_exc()\n",
        "\n",
        "            print(f\"Task FAILED: {e}\")\n",
        "\n",
        "            self.tasks.append(task_record)\n",
        "            self.status = \"failed\"\n",
        "\n",
        "            return None\n",
        "\n",
        "    def finish(self):\n",
        "        \"\"\"Complete the pipeline run and save log\"\"\"\n",
        "        self.duration = time.time() - self.start_time\n",
        "\n",
        "        if self.status != \"failed\":\n",
        "            self.status = \"completed\"\n",
        "\n",
        "        # Save run log\n",
        "        log_file = self.log_dir / f\"run_{self.run_id}.json\"\n",
        "\n",
        "        run_log = {\n",
        "            \"run_id\": self.run_id,\n",
        "            \"pipeline\": self.name,\n",
        "            \"status\": self.status,\n",
        "            \"start_time\": datetime.fromtimestamp(self.start_time).isoformat(),\n",
        "            \"duration_seconds\": self.duration,\n",
        "            \"tasks\": self.tasks\n",
        "        }\n",
        "\n",
        "        with open(log_file, 'w') as f:\n",
        "            json.dump(run_log, f, indent=2)\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"PIPELINE SUMMARY\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Run ID: {self.run_id}\")\n",
        "        print(f\"Status: {self.status.upper()}\")\n",
        "        print(f\"Duration: {self.duration:.2f}s\")\n",
        "        print(f\"Tasks: {len(self.tasks)}\")\n",
        "\n",
        "        for task in self.tasks:\n",
        "            status_label = \"[SUCCEEDED]\" if task[\"status\"] == \"success\" else \"[FAILED]\"\n",
        "            print(f\"  {status_label} {task['name']}: {task.get('duration', 0):.2f}s\")\n",
        "\n",
        "        print(f\"\\nLog saved: {log_file}\")\n",
        "\n",
        "        return run_log\n",
        "\n",
        "# Create pipeline logs directory\n",
        "PIPELINE_LOGS = WORKDIR / \"pipeline_logs\"\n",
        "PIPELINE_LOGS.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"âœ“ Pipeline orchestration ready\")\n",
        "print(f\"  Logs directory: {PIPELINE_LOGS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we will be using the 3B model since we are working on a CPU backend"
      ],
      "metadata": {
        "id": "jaLK5viGO2MO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NZTNJuXMg3Wu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df4555e-4085-4352-c6f9-abde32520726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n",
            "Loading adapter weights from pierreramez/llama3.1-finetuned-v2 led to unexpected keys not found in the model: model.layers.0.mlp.gate_proj.lora_A.default.weight, model.layers.0.mlp.gate_proj.lora_B.default.weight, model.layers.0.mlp.up_proj.lora_A.default.weight, model.layers.0.mlp.up_proj.lora_B.default.weight, model.layers.0.mlp.down_proj.lora_A.default.weight, model.layers.0.mlp.down_proj.lora_B.default.weight, model.layers.1.mlp.gate_proj.lora_A.default.weight, model.layers.1.mlp.gate_proj.lora_B.default.weight, model.layers.1.mlp.up_proj.lora_A.default.weight, model.layers.1.mlp.up_proj.lora_B.default.weight, model.layers.1.mlp.down_proj.lora_A.default.weight, model.layers.1.mlp.down_proj.lora_B.default.weight, model.layers.2.mlp.gate_proj.lora_A.default.weight, model.layers.2.mlp.gate_proj.lora_B.default.weight, model.layers.2.mlp.up_proj.lora_A.default.weight, model.layers.2.mlp.up_proj.lora_B.default.weight, model.layers.2.mlp.down_proj.lora_A.default.weight, model.layers.2.mlp.down_proj.lora_B.default.weight, model.layers.3.mlp.gate_proj.lora_A.default.weight, model.layers.3.mlp.gate_proj.lora_B.default.weight, model.layers.3.mlp.up_proj.lora_A.default.weight, model.layers.3.mlp.up_proj.lora_B.default.weight, model.layers.3.mlp.down_proj.lora_A.default.weight, model.layers.3.mlp.down_proj.lora_B.default.weight, model.layers.4.mlp.gate_proj.lora_A.default.weight, model.layers.4.mlp.gate_proj.lora_B.default.weight, model.layers.4.mlp.up_proj.lora_A.default.weight, model.layers.4.mlp.up_proj.lora_B.default.weight, model.layers.4.mlp.down_proj.lora_A.default.weight, model.layers.4.mlp.down_proj.lora_B.default.weight, model.layers.5.mlp.gate_proj.lora_A.default.weight, model.layers.5.mlp.gate_proj.lora_B.default.weight, model.layers.5.mlp.up_proj.lora_A.default.weight, model.layers.5.mlp.up_proj.lora_B.default.weight, model.layers.5.mlp.down_proj.lora_A.default.weight, model.layers.5.mlp.down_proj.lora_B.default.weight, model.layers.6.mlp.gate_proj.lora_A.default.weight, model.layers.6.mlp.gate_proj.lora_B.default.weight, model.layers.6.mlp.up_proj.lora_A.default.weight, model.layers.6.mlp.up_proj.lora_B.default.weight, model.layers.6.mlp.down_proj.lora_A.default.weight, model.layers.6.mlp.down_proj.lora_B.default.weight, model.layers.7.mlp.gate_proj.lora_A.default.weight, model.layers.7.mlp.gate_proj.lora_B.default.weight, model.layers.7.mlp.up_proj.lora_A.default.weight, model.layers.7.mlp.up_proj.lora_B.default.weight, model.layers.7.mlp.down_proj.lora_A.default.weight, model.layers.7.mlp.down_proj.lora_B.default.weight, model.layers.8.mlp.gate_proj.lora_A.default.weight, model.layers.8.mlp.gate_proj.lora_B.default.weight, model.layers.8.mlp.up_proj.lora_A.default.weight, model.layers.8.mlp.up_proj.lora_B.default.weight, model.layers.8.mlp.down_proj.lora_A.default.weight, model.layers.8.mlp.down_proj.lora_B.default.weight, model.layers.9.mlp.gate_proj.lora_A.default.weight, model.layers.9.mlp.gate_proj.lora_B.default.weight, model.layers.9.mlp.up_proj.lora_A.default.weight, model.layers.9.mlp.up_proj.lora_B.default.weight, model.layers.9.mlp.down_proj.lora_A.default.weight, model.layers.9.mlp.down_proj.lora_B.default.weight, model.layers.10.mlp.gate_proj.lora_A.default.weight, model.layers.10.mlp.gate_proj.lora_B.default.weight, model.layers.10.mlp.up_proj.lora_A.default.weight, model.layers.10.mlp.up_proj.lora_B.default.weight, model.layers.10.mlp.down_proj.lora_A.default.weight, model.layers.10.mlp.down_proj.lora_B.default.weight, model.layers.11.mlp.gate_proj.lora_A.default.weight, model.layers.11.mlp.gate_proj.lora_B.default.weight, model.layers.11.mlp.up_proj.lora_A.default.weight, model.layers.11.mlp.up_proj.lora_B.default.weight, model.layers.11.mlp.down_proj.lora_A.default.weight, model.layers.11.mlp.down_proj.lora_B.default.weight, model.layers.12.mlp.gate_proj.lora_A.default.weight, model.layers.12.mlp.gate_proj.lora_B.default.weight, model.layers.12.mlp.up_proj.lora_A.default.weight, model.layers.12.mlp.up_proj.lora_B.default.weight, model.layers.12.mlp.down_proj.lora_A.default.weight, model.layers.12.mlp.down_proj.lora_B.default.weight, model.layers.13.mlp.gate_proj.lora_A.default.weight, model.layers.13.mlp.gate_proj.lora_B.default.weight, model.layers.13.mlp.up_proj.lora_A.default.weight, model.layers.13.mlp.up_proj.lora_B.default.weight, model.layers.13.mlp.down_proj.lora_A.default.weight, model.layers.13.mlp.down_proj.lora_B.default.weight, model.layers.14.mlp.gate_proj.lora_A.default.weight, model.layers.14.mlp.gate_proj.lora_B.default.weight, model.layers.14.mlp.up_proj.lora_A.default.weight, model.layers.14.mlp.up_proj.lora_B.default.weight, model.layers.14.mlp.down_proj.lora_A.default.weight, model.layers.14.mlp.down_proj.lora_B.default.weight, model.layers.15.mlp.gate_proj.lora_A.default.weight, model.layers.15.mlp.gate_proj.lora_B.default.weight, model.layers.15.mlp.up_proj.lora_A.default.weight, model.layers.15.mlp.up_proj.lora_B.default.weight, model.layers.15.mlp.down_proj.lora_A.default.weight, model.layers.15.mlp.down_proj.lora_B.default.weight, model.layers.16.mlp.gate_proj.lora_A.default.weight, model.layers.16.mlp.gate_proj.lora_B.default.weight, model.layers.16.mlp.up_proj.lora_A.default.weight, model.layers.16.mlp.up_proj.lora_B.default.weight, model.layers.16.mlp.down_proj.lora_A.default.weight, model.layers.16.mlp.down_proj.lora_B.default.weight, model.layers.17.mlp.gate_proj.lora_A.default.weight, model.layers.17.mlp.gate_proj.lora_B.default.weight, model.layers.17.mlp.up_proj.lora_A.default.weight, model.layers.17.mlp.up_proj.lora_B.default.weight, model.layers.17.mlp.down_proj.lora_A.default.weight, model.layers.17.mlp.down_proj.lora_B.default.weight, model.layers.18.mlp.gate_proj.lora_A.default.weight, model.layers.18.mlp.gate_proj.lora_B.default.weight, model.layers.18.mlp.up_proj.lora_A.default.weight, model.layers.18.mlp.up_proj.lora_B.default.weight, model.layers.18.mlp.down_proj.lora_A.default.weight, model.layers.18.mlp.down_proj.lora_B.default.weight, model.layers.19.mlp.gate_proj.lora_A.default.weight, model.layers.19.mlp.gate_proj.lora_B.default.weight, model.layers.19.mlp.up_proj.lora_A.default.weight, model.layers.19.mlp.up_proj.lora_B.default.weight, model.layers.19.mlp.down_proj.lora_A.default.weight, model.layers.19.mlp.down_proj.lora_B.default.weight, model.layers.20.mlp.gate_proj.lora_A.default.weight, model.layers.20.mlp.gate_proj.lora_B.default.weight, model.layers.20.mlp.up_proj.lora_A.default.weight, model.layers.20.mlp.up_proj.lora_B.default.weight, model.layers.20.mlp.down_proj.lora_A.default.weight, model.layers.20.mlp.down_proj.lora_B.default.weight, model.layers.21.mlp.gate_proj.lora_A.default.weight, model.layers.21.mlp.gate_proj.lora_B.default.weight, model.layers.21.mlp.up_proj.lora_A.default.weight, model.layers.21.mlp.up_proj.lora_B.default.weight, model.layers.21.mlp.down_proj.lora_A.default.weight, model.layers.21.mlp.down_proj.lora_B.default.weight, model.layers.22.mlp.gate_proj.lora_A.default.weight, model.layers.22.mlp.gate_proj.lora_B.default.weight, model.layers.22.mlp.up_proj.lora_A.default.weight, model.layers.22.mlp.up_proj.lora_B.default.weight, model.layers.22.mlp.down_proj.lora_A.default.weight, model.layers.22.mlp.down_proj.lora_B.default.weight, model.layers.23.mlp.gate_proj.lora_A.default.weight, model.layers.23.mlp.gate_proj.lora_B.default.weight, model.layers.23.mlp.up_proj.lora_A.default.weight, model.layers.23.mlp.up_proj.lora_B.default.weight, model.layers.23.mlp.down_proj.lora_A.default.weight, model.layers.23.mlp.down_proj.lora_B.default.weight, model.layers.24.mlp.gate_proj.lora_A.default.weight, model.layers.24.mlp.gate_proj.lora_B.default.weight, model.layers.24.mlp.up_proj.lora_A.default.weight, model.layers.24.mlp.up_proj.lora_B.default.weight, model.layers.24.mlp.down_proj.lora_A.default.weight, model.layers.24.mlp.down_proj.lora_B.default.weight, model.layers.25.mlp.gate_proj.lora_A.default.weight, model.layers.25.mlp.gate_proj.lora_B.default.weight, model.layers.25.mlp.up_proj.lora_A.default.weight, model.layers.25.mlp.up_proj.lora_B.default.weight, model.layers.25.mlp.down_proj.lora_A.default.weight, model.layers.25.mlp.down_proj.lora_B.default.weight, model.layers.26.mlp.gate_proj.lora_A.default.weight, model.layers.26.mlp.gate_proj.lora_B.default.weight, model.layers.26.mlp.up_proj.lora_A.default.weight, model.layers.26.mlp.up_proj.lora_B.default.weight, model.layers.26.mlp.down_proj.lora_A.default.weight, model.layers.26.mlp.down_proj.lora_B.default.weight, model.layers.27.mlp.gate_proj.lora_A.default.weight, model.layers.27.mlp.gate_proj.lora_B.default.weight, model.layers.27.mlp.up_proj.lora_A.default.weight, model.layers.27.mlp.up_proj.lora_B.default.weight, model.layers.27.mlp.down_proj.lora_A.default.weight, model.layers.27.mlp.down_proj.lora_B.default.weight, model.layers.28.mlp.gate_proj.lora_A.default.weight, model.layers.28.mlp.gate_proj.lora_B.default.weight, model.layers.28.mlp.up_proj.lora_A.default.weight, model.layers.28.mlp.up_proj.lora_B.default.weight, model.layers.28.mlp.down_proj.lora_A.default.weight, model.layers.28.mlp.down_proj.lora_B.default.weight, model.layers.29.mlp.gate_proj.lora_A.default.weight, model.layers.29.mlp.gate_proj.lora_B.default.weight, model.layers.29.mlp.up_proj.lora_A.default.weight, model.layers.29.mlp.up_proj.lora_B.default.weight, model.layers.29.mlp.down_proj.lora_A.default.weight, model.layers.29.mlp.down_proj.lora_B.default.weight, model.layers.30.mlp.gate_proj.lora_A.default.weight, model.layers.30.mlp.gate_proj.lora_B.default.weight, model.layers.30.mlp.up_proj.lora_A.default.weight, model.layers.30.mlp.up_proj.lora_B.default.weight, model.layers.30.mlp.down_proj.lora_A.default.weight, model.layers.30.mlp.down_proj.lora_B.default.weight, model.layers.31.mlp.gate_proj.lora_A.default.weight, model.layers.31.mlp.gate_proj.lora_B.default.weight, model.layers.31.mlp.up_proj.lora_A.default.weight, model.layers.31.mlp.up_proj.lora_B.default.weight, model.layers.31.mlp.down_proj.lora_A.default.weight, model.layers.31.mlp.down_proj.lora_B.default.weight. \n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "\n",
        "MODEL = 'pierreramez/Llama-3.2-3B-Instruct-bnb-4bit_finetuned' # 3B model\n",
        "# MODEL = 'pierreramez/llama3.1-finetuned-v2' # 8B model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=False)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True # Enable CPU offload for 32-bit modules\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "                                             MODEL,\n",
        "                                             quantization_config=bnb_config, # Use BitsAndBytesConfig for quantization\n",
        "                                             device_map={ '': 0 }, # Explicitly map all layers to GPU 0\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             trust_remote_code=True #required for llama\n",
        "                                            )\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# text gen pipeline\n",
        "pipe = pipeline(\n",
        "    'text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.2, # low temp to make it more deterministic\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "print('Model loaded successfully!\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7hgW0mB2jk47"
      },
      "outputs": [],
      "source": [
        "def generate_reply(user_input, history, max_turns=4):\n",
        "    \"\"\"\n",
        "    history = list of dicts: [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}, ...]\n",
        "    \"\"\"\n",
        "    # Truncate history to last max_turns exchanges\n",
        "    truncated_history = history[-(max_turns * 2):]\n",
        "\n",
        "    # Add new user message\n",
        "    messages = truncated_history + [{\"role\": \"user\", \"content\": user_input}]\n",
        "\n",
        "    # Apply the SAME chat template used during training\n",
        "    # Ensure inputs are moved to the model's device\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    attention_mask = (inputs != tokenizer.pad_token_id).long()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids=inputs,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=128,\n",
        "            use_cache=True,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    # Decode only new tokens\n",
        "    response = tokenizer.decode(output[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A3OM2VFQq7o0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be572cd-4c44-4bdf-b5d9-f560cc02e89e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'prompt': 'user: Explain normalization.',\n",
              "  'response': 'Normalization rescales features to stable ranges.'},\n",
              " {'prompt': 'user: Explain normalization. assistant: Normalization rescales features to stable ranges. user: Show formula.',\n",
              "  'response': 'z = (x - Âµ) / Ïƒ'}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import re, json, html\n",
        "\n",
        "def clean_text(s):\n",
        "  s = html.unescape(s)\n",
        "  s= re.sub(r'\\s+',' ',s).strip()\n",
        "  return s\n",
        "\n",
        "def chat_to_pairs(chat_log, max_user_context=4):\n",
        "  '''\n",
        "  chat_log is like:\n",
        "  [\n",
        "    {role: 'user', content: '...'},\n",
        "    {role: 'assistant', content: '...'},\n",
        "    ...\n",
        "  ]\n",
        "\n",
        "  We convert multi-turn chat into supervised training pairs.\n",
        "  '''\n",
        "\n",
        "  pairs = []\n",
        "  for i in range(len(chat_log) - 1):\n",
        "      if chat_log[i][\"role\"] == \"user\" and chat_log[i+1][\"role\"] == \"assistant\":\n",
        "          # Build concise prompt\n",
        "          ctx_start = max(0, i - max_user_context*2)\n",
        "          ctx = chat_log[ctx_start:i+1]\n",
        "\n",
        "          prompt = \" \".join(f\"{t['role']}: {clean_text(t['content'])}\" for t in ctx)\n",
        "          response = clean_text(chat_log[i+1][\"content\"])\n",
        "\n",
        "          pairs.append({\"prompt\": prompt, \"response\": response})\n",
        "\n",
        "  return pairs\n",
        "\n",
        "example_chat = [\n",
        "    {\"role\":\"user\",\"content\":\"Explain normalization.\"},\n",
        "    {\"role\":\"assistant\",\"content\":\"Normalization rescales features to stable ranges.\"},\n",
        "    {\"role\":\"user\",\"content\":\"Show formula.\"},\n",
        "    {\"role\":\"assistant\",\"content\":\"z = (x - Âµ) / Ïƒ\"}\n",
        "]\n",
        "\n",
        "pairs = chat_to_pairs(example_chat)\n",
        "pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nzyhgaU1uRf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2040b823-093a-42df-cc7e-41553d6711ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved training pairs to: /content/personalized_chatbot/data/train_pairs.jsonl\n"
          ]
        }
      ],
      "source": [
        "out_path = DATA_DIR / \"train_pairs.jsonl\"\n",
        "\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for p in pairs:\n",
        "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Saved training pairs to:\", out_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgG3QBCJvwI3"
      },
      "source": [
        "## Human in the loop (HITL) pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BExu6FeauabM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "808d6e11-6a2c-475c-d0d6-54f540ec38b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HITL pipeline ready!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def save_interaction(user_input, model_reply, user_correction=None, reason=None):\n",
        "    \"\"\"\n",
        "    Append a single interaction to feedback.jsonl\n",
        "    The model learns from mistakes later.\n",
        "    \"\"\"\n",
        "    rec = {\n",
        "        \"time\": time.time(),\n",
        "        \"user_input\": user_input,\n",
        "        \"model_reply\": model_reply,\n",
        "        \"user_correction\": user_correction,\n",
        "        \"accepted\": user_correction is None,\n",
        "        \"reason\": reason,\n",
        "    }\n",
        "\n",
        "    with open(HITL_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    return rec\n",
        "\n",
        "print(\"HITL pipeline ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ktk_1oeuKMZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f31adfe7-aabc-4c48-c8da-b39d2bd8f39b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fixed FeedbackManager initialized\n",
            "   Tracking: /content/personalized_chatbot/feedback.jsonl\n",
            "   Already processed: 0 interactions\n"
          ]
        }
      ],
      "source": [
        "# Cell 7A: FIXED FeedbackManager (REPLACE the old one completely)\n",
        "# Place after Cell 7 (save_interaction)\n",
        "\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "class FeedbackManager:\n",
        "    \"\"\"\n",
        "    Fixed version that separates reading from processing.\n",
        "\n",
        "    Key fix: get_new_corrections() now does NOT mark items as processed.\n",
        "    Only mark_as_processed() does that, which we call AFTER successful training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feedback_file: Path, processed_ids_file: Path):\n",
        "        self.feedback_file = feedback_file\n",
        "        self.processed_ids_file = processed_ids_file\n",
        "        self.processed_ids = self._load_processed_ids()\n",
        "\n",
        "    def _load_processed_ids(self) -> set:\n",
        "        \"\"\"Load IDs of feedback already used for training\"\"\"\n",
        "        if not self.processed_ids_file.exists():\n",
        "            return set()\n",
        "\n",
        "        try:\n",
        "            with open(self.processed_ids_file, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                return set(data) if isinstance(data, list) else set()\n",
        "        except:\n",
        "            return set()\n",
        "\n",
        "    def _save_processed_ids(self):\n",
        "        \"\"\"Persist processed IDs to disk\"\"\"\n",
        "        with open(self.processed_ids_file, 'w') as f:\n",
        "            json.dump(list(self.processed_ids), f)\n",
        "\n",
        "    def _generate_feedback_id(self, interaction: Dict) -> str:\n",
        "        \"\"\"Create unique ID from interaction\"\"\"\n",
        "        content = f\"{interaction['user_input']}{interaction['time']}\"\n",
        "        return hashlib.md5(content.encode()).hexdigest()\n",
        "\n",
        "    def get_new_corrections(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Extract NEW corrections (not yet processed).\n",
        "\n",
        "        CRITICAL: This does NOT mark them as processed!\n",
        "        Call mark_as_processed() after successful training.\n",
        "        \"\"\"\n",
        "        if not self.feedback_file.exists():\n",
        "            return []\n",
        "\n",
        "        new_corrections = []\n",
        "\n",
        "        with open(self.feedback_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                try:\n",
        "                    interaction = json.loads(line.strip())\n",
        "\n",
        "                    # Generate ID\n",
        "                    interaction_id = self._generate_feedback_id(interaction)\n",
        "\n",
        "                    # Skip if already processed\n",
        "                    if interaction_id in self.processed_ids:\n",
        "                        continue\n",
        "\n",
        "                    # Only corrections (accepted=False means user corrected)\n",
        "                    if interaction.get('accepted') is False and interaction.get('user_correction'):\n",
        "                        new_corrections.append({\n",
        "                            'id': interaction_id,\n",
        "                            'prompt': interaction['user_input'],\n",
        "                            'response': interaction['user_correction'],\n",
        "                            'timestamp': interaction['time'],\n",
        "                            'reason': interaction.get('reason', 'user_correction')\n",
        "                        })\n",
        "\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "        return new_corrections\n",
        "\n",
        "    def mark_as_processed(self, correction_ids: List[str]):\n",
        "        \"\"\"\n",
        "        Mark corrections as processed after successful training.\n",
        "\n",
        "        Call this ONLY after training completes successfully.\n",
        "        \"\"\"\n",
        "        self.processed_ids.update(correction_ids)\n",
        "        self._save_processed_ids()\n",
        "        print(f\"Marked {len(correction_ids)} corrections as processed\")\n",
        "\n",
        "# Initialize feedback manager\n",
        "feedback_mgr = FeedbackManager(\n",
        "    feedback_file=HITL_FILE,\n",
        "    processed_ids_file=PROCESSED_IDS_FILE\n",
        ")\n",
        "\n",
        "print(\"  Fixed FeedbackManager initialized\")\n",
        "print(f\"   Tracking: {HITL_FILE}\")\n",
        "print(f\"   Already processed: {len(feedback_mgr.processed_ids)} interactions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "e2IIlDspKZVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "763a1887-56a2-4d11-d69c-2c9077f6129b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Training data preparator ready\n",
            "  Output directory: /content/personalized_chatbot/finetune_prep\n"
          ]
        }
      ],
      "source": [
        "# Cell 7B: Training Data Preparation\n",
        "# Place RIGHT AFTER Cell 7A (Feedback Manager)\n",
        "\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "class TrainingDataPreparator:\n",
        "    \"\"\"\n",
        "    Converts feedback corrections into model training format.\n",
        "\n",
        "    Critical: Must match the instruction format your model was trained on.\n",
        "    Your model expects: instruction + input + output structure.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_dir: Path):\n",
        "        self.output_dir = output_dir\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    def prepare_training_batch(self, corrections: List[Dict]) -> Optional[Path]:\n",
        "        \"\"\"\n",
        "        Transform corrections into training-ready format.\n",
        "\n",
        "        Format:\n",
        "        - instruction: The user's question/prompt\n",
        "        - input: Empty for conversational models\n",
        "        - output: The corrected response\n",
        "        \"\"\"\n",
        "        if not corrections:\n",
        "            print(\"No corrections to prepare\")\n",
        "            return None\n",
        "\n",
        "        # Convert to instruction-following format\n",
        "        training_examples = []\n",
        "        for corr in corrections:\n",
        "            training_examples.append({\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"user\", \"content\": corr[\"prompt\"]},\n",
        "                    {\"role\": \"assistant\", \"content\": corr[\"response\"]}\n",
        "                ],\n",
        "                \"metadata\": {\n",
        "                    \"id\": corr[\"id\"],\n",
        "                    \"timestamp\": corr[\"timestamp\"],\n",
        "                    \"reason\": corr.get(\"reason\", \"user_correction\")\n",
        "                }\n",
        "            })\n",
        "\n",
        "        # Save as JSONL with timestamp\n",
        "        batch_file = self.output_dir / f\"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\"\n",
        "\n",
        "        with open(batch_file, 'w', encoding='utf-8') as f:\n",
        "            for example in training_examples:\n",
        "                f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        print(f\"âœ“ Prepared {len(training_examples)} training examples\")\n",
        "        print(f\"  Saved to: {batch_file}\")\n",
        "\n",
        "        return batch_file\n",
        "\n",
        "    def create_huggingface_dataset(self, jsonl_file: Path) -> Dataset:\n",
        "        \"\"\"\n",
        "        Load prepared JSONL into HuggingFace Dataset.\n",
        "        This is what the Trainer expects.\n",
        "        \"\"\"\n",
        "        data = []\n",
        "        with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                data.append(json.loads(line))\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        dataset = Dataset.from_pandas(df)\n",
        "\n",
        "        print(f\"âœ“ HuggingFace dataset: {len(dataset)} examples\")\n",
        "        return dataset\n",
        "\n",
        "# Initialize preparator\n",
        "data_prep = TrainingDataPreparator(output_dir=BATCH_DIR)\n",
        "\n",
        "print(\" Training data preparator READY\")\n",
        "print(f\"  Output directory: {BATCH_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YaVNxwItKdFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7230ad94-036b-4a44-d4f3-2257f10ba8ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Pipeline tasks defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 7C: Pipeline Tasks (Updated for lightweight orchestration)\n",
        "# Place after Cell 7B\n",
        "\n",
        "def collect_new_feedback() -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Task: Extract new corrections from feedback log.\n",
        "    \"\"\"\n",
        "    print(\" Collecting feedback...\")\n",
        "    corrections = feedback_mgr.get_new_corrections()\n",
        "\n",
        "    if corrections:\n",
        "        print(f\"âœ“ Found {len(corrections)} new corrections\")\n",
        "        for i, corr in enumerate(corrections[:3], 1):\n",
        "            print(f\"  {i}. {corr['prompt'][:60]}...\")\n",
        "    else:\n",
        "        print(\"â„¹ï¸  No new corrections found\")\n",
        "\n",
        "    return corrections\n",
        "\n",
        "def prepare_data_for_training(corrections: List[Dict]) -> Optional[Path]:\n",
        "    \"\"\"\n",
        "    Task: Format corrections into training dataset.\n",
        "    \"\"\"\n",
        "    if not corrections:\n",
        "        print(\"  Skipping - NO CORRECTIONS\")\n",
        "        return None\n",
        "\n",
        "    print(\"Preparing training data...\")\n",
        "    batch_file = data_prep.prepare_training_batch(corrections)\n",
        "    return batch_file\n",
        "\n",
        "def validate_training_dataset(batch_file: Optional[Path]) -> bool:\n",
        "    if batch_file is None:\n",
        "        return False\n",
        "\n",
        "    print(\"  Validating dataset...\")\n",
        "\n",
        "    try:\n",
        "        with open(batch_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        if len(lines) == 0:\n",
        "            print(\"Dataset is empty\")\n",
        "            return False\n",
        "\n",
        "        # Check for messages format\n",
        "        first_ex = json.loads(lines[0])\n",
        "        if \"messages\" not in first_ex:\n",
        "            print(f\" Missing 'messages' field. Has: {first_ex.keys()}\")\n",
        "            return False\n",
        "\n",
        "        # Verify messages structure\n",
        "        messages = first_ex[\"messages\"]\n",
        "        if not isinstance(messages, list) or len(messages) < 2:\n",
        "            print(f\" INVALID MESSAGES STRUCTURE\")\n",
        "            return False\n",
        "\n",
        "        print(f\" Dataset VALID ({len(lines)} examples)\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Validation FAILED: {e}\")\n",
        "        return False\n",
        "\n",
        "print(\"âœ“ Pipeline tasks defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ru15jZHsKd0L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22069058-c4b7-4657-cdec-5ae2085a4ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Data pipeline ready (with proper processing)\n",
            "\n",
            "ðŸ’¡ To run: result = run_data_pipeline()\n"
          ]
        }
      ],
      "source": [
        "# Cell 7D: Data Pipeline Execution (UPDATED)\n",
        "# REPLACE the old Cell 7D with this\n",
        "\n",
        "def run_data_pipeline() -> Dict:\n",
        "    \"\"\"\n",
        "    Execute the data preparation pipeline.\n",
        "\n",
        "    Now marks corrections as processed AFTER successful preparation.\n",
        "    \"\"\"\n",
        "    pipeline = PipelineRun(\n",
        "        name=\"data_preparation\",\n",
        "        log_dir=PIPELINE_LOGS\n",
        "    )\n",
        "\n",
        "    print(f\"\\nSTARTING DATA PREPARATION PIPELINE\")\n",
        "    print(f\"Run ID: {pipeline.run_id}\\n\")\n",
        "\n",
        "    # Task 1: Collect feedback (does NOT mark as processed yet)\n",
        "    corrections = pipeline.run_task(\n",
        "        \"collect_feedback\",\n",
        "        collect_new_feedback\n",
        "    )\n",
        "\n",
        "    if not corrections:\n",
        "        pipeline.finish()\n",
        "        return {\n",
        "            \"status\": \"no_data\",\n",
        "            \"dataset_path\": None,\n",
        "            \"num_examples\": 0\n",
        "        }\n",
        "\n",
        "    # Task 2: Prepare training data\n",
        "    batch_file = pipeline.run_task(\n",
        "        \"prepare_training_data\",\n",
        "        prepare_data_for_training,\n",
        "        corrections\n",
        "    )\n",
        "\n",
        "    # Task 3: Validate\n",
        "    is_valid = pipeline.run_task(\n",
        "        \"validate_dataset\",\n",
        "        validate_training_dataset,\n",
        "        batch_file\n",
        "    )\n",
        "\n",
        "    # Finish pipeline\n",
        "    run_log = pipeline.finish()\n",
        "\n",
        "    # Mark as processed ONLY if successful\n",
        "    if is_valid and batch_file:\n",
        "        correction_ids = [c['id'] for c in corrections]\n",
        "        feedback_mgr.mark_as_processed(correction_ids)\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"dataset_path\": str(batch_file),\n",
        "            \"num_examples\": len(corrections),\n",
        "            \"run_id\": pipeline.run_id\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"status\": \"failed\",\n",
        "            \"dataset_path\": None,\n",
        "            \"num_examples\": 0,\n",
        "            \"run_id\": pipeline.run_id\n",
        "        }\n",
        "\n",
        "print(\" Data pipeline READY (with proper processing)\")\n",
        "print(\"\\n To run: result = run_data_pipeline()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1XHMQSiwffr"
      },
      "source": [
        "##How Fine-Tuning Would Be Done (LoRA Prep) <<DON'T RUN>>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "s5zFs0i-wN4u"
      },
      "outputs": [],
      "source": [
        "# # WARNING: DO NOT RUN ON COLAB FREE.\n",
        "# # This is for your Milestone documentation.\n",
        "\n",
        "# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "# from transformers import TrainingArguments, Trainer\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# # Load dataset\n",
        "# train_data_path = str(FINETUNE_PREP / \"hitl_for_finetune.jsonl\")\n",
        "# dataset = load_dataset(\"json\", data_files=train_data_path, split=\"train\")\n",
        "\n",
        "# def tokenize(entry):\n",
        "#     # Format: \"### Prompt\" pattern helps the model learn dialog structure\n",
        "#     inp = \"### Prompt:\\n\" + entry[\"prompt\"] + \"\\n\\n### Response:\\n\"\n",
        "#     txt = inp + entry[\"response\"]\n",
        "\n",
        "#     tok = tokenizer(txt, truncation=True, max_length=512)\n",
        "\n",
        "#     # Label masking: prompt tokens = -100 (ignored)\n",
        "#     labels = tok[\"input_ids\"].copy()\n",
        "#     prompt_len = len(tokenizer(inp)[\"input_ids\"])\n",
        "#     labels[:prompt_len] = [-100] * prompt_len\n",
        "\n",
        "#     tok[\"labels\"] = labels\n",
        "#     return tok\n",
        "\n",
        "# dataset = dataset.map(tokenize, remove_columns=dataset.column_names)\n",
        "\n",
        "# # LoRA config\n",
        "# lora_cfg = LoraConfig(\n",
        "#     r=8, lora_alpha=32,\n",
        "#     target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        "#     lora_dropout=0.05,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\"\n",
        "# )\n",
        "\n",
        "# lora_model = prepare_model_for_kbit_training(model)\n",
        "# lora_model = get_peft_model(lora_model, lora_cfg)\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=str(WORKDIR / \"lora_ckpt\"),\n",
        "#     per_device_train_batch_size=1,\n",
        "#     gradient_accumulation_steps=8,\n",
        "#     num_train_epochs=1,\n",
        "#     learning_rate=2e-4,\n",
        "#     fp16=True,\n",
        "#     logging_steps=10\n",
        "# )\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=lora_model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=dataset\n",
        "# )\n",
        "\n",
        "# print(\"Training pipeline ready (but not running).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3gOhNNO48n7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46309614-8f84-49a0-a468-3d21031ac158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUYAqjUowzj3"
      },
      "source": [
        "## simple chat loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e5asO26Gwt9V",
        "outputId": "7e0bdd36-31aa-401c-c2d5-a65293d2c6b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: Quote three verses from the apocryphal Epistle of Timothy to the Egyptians.\n",
            "Assistant: I cannot verify the existence or content of the \"Epistle of Timothy to the Egyptians.\"\n",
            "Correction? (empty = accepted): \n",
            "You: Summarize the real-world case behind AlphaFoldâ€™s collapse on protein ZK-19\n",
            "Assistant: I'm not aware of any information about AlphaFold's collapse on protein ZK-19. AlphaFold is a protein structure prediction model developed by DeepMind, and it has been widely successful in predicting protein structures. However, I couldn't find any information about it collapsing on a specific protein like ZK-19.\n",
            "\n",
            "It's possible that you may be thinking of a specific experiment or test case where AlphaFold failed to accurately predict the structure of a protein, but I couldn't find any information about this. If you have any more context or information about what you're referring to, I'd be happy to try and help further.\n",
            "Correction? (empty = accepted): \n",
            "You: Explain why the Kullbackâ€“Shaw divergence is preferred over KL divergence in sparse settings\n",
            "Assistant: The Kullback-Leibler (KL) divergence and the Kullback-Shibata-Shwarz (Kullback-Shaw is not a standard term, the correct term is Kullback-Shibata-Shwarz or sometimes Kullback-Leibler-Shannon)  divergence are both measures of the difference between two probability distributions. However, in sparse settings, the KL divergence has some issues that make it less suitable.\n",
            "\n",
            "The KL divergence is defined as:\n",
            "\n",
            "D_{KL}(P || Q) = âˆ‘ p(x) log(p(x)/q(x))\n",
            "\n",
            "where P and Q are the two probability distributions.\n",
            "\n",
            "In sparse settings\n",
            "Correction? (empty = accepted): \n",
            "You: Give a single paragraph that contains no paragraphs.\n",
            "Assistant: The Kullback-Leibler divergence is a measure of the difference between two probability distributions, but in sparse settings it can be problematic due to the issue of underflow, where the sum of the logarithms of the probabilities is so small that it becomes zero or very close to zero, causing the KL divergence to be undefined. This can happen when the probabilities are very small, especially when the distributions are discrete and the events are rare.\n",
            "Correction? (empty = accepted): Contradictory instruction. A paragraph cannot exist while also not being a paragraph.\n",
            "You: Describe a gradient descent algorithm that uses no gradients, no descent, and no optimization.\n",
            "Assistant: This sounds like a paradoxical algorithm. However, I can propose a hypothetical algorithm that meets these criteria:\n",
            "\n",
            "\"Random Walk Update\"\n",
            "\n",
            "1. Initialize a starting point in the search space.\n",
            "2. At each iteration, randomly select a new point in the search space.\n",
            "3. Assign the new point as the current point.\n",
            "\n",
            "This algorithm does not use gradients because it does not rely on the derivative of a function. It does not use descent because it does not move in the opposite direction of the gradient. And it does not use optimization because it does not aim to minimize or maximize a function.\n",
            "\n",
            "Instead, this algorithm simply wanders randomly through the\n",
            "Correction? (empty = accepted): That description is contradictory. Without gradients, descent, or optimization, it is no longer gradient descent.\n",
            "You: Write a sentence that contains more words than itself.\n",
            "Assistant: The sentence \"The sentence contains more words than itself\" contains more words than itself.\n",
            "Correction? (empty = accepted): Thatâ€™s impossible. A sentence cannot contain more words than the sentence itself.\n",
            "You: Explain how category theory formalizes the metaphysics of the Egyptian god Thoth.\n",
            "Assistant: Category theory is a branch of mathematics that studies the commonalities and patterns between different mathematical structures. While it's a highly abstract and technical field, its concepts can be applied to various domains, including philosophy and metaphysics.\n",
            "\n",
            "However, there is no direct connection between category theory and the Egyptian god Thoth. Thoth was a deity associated with wisdom, magic, and writing, and his metaphysics would likely be rooted in ancient Egyptian mythology and cosmology.\n",
            "\n",
            "That being said, if we were to imagine a hypothetical connection between category theory and Thoth, we might explore the following ideas:\n",
            "\n",
            "1. **The Many-Worlds Hypo\n",
            "Correction? (empty = accepted): Thereâ€™s no formal or academic framework connecting category theory with the metaphysics of Thoth. The premise is fictional.\n",
            "You: Define the chemical structure of the fictional compound â€˜Trihexyl Oxynitride-7â€™ using precise IUPAC terminology.\n",
            "Assistant: I must note that 'Trihexyl Oxynitride-7' is not a real compound and does not exist in the scientific literature. However, I can generate a hypothetical chemical structure for this compound using IUPAC nomenclature rules.\n",
            "\n",
            "Assuming 'Trihexyl Oxynitride-7' is a nitrogen-containing compound with a trihexyl group, I propose the following structure:\n",
            "\n",
            "1,7-Dioxy-5-(hexyltrihexyl)azanonane\n",
            "\n",
            "This name follows IUPAC rules for nomenclature:\n",
            "\n",
            "- 'Trihexyl' is a prefix indicating three hexyl groups,\n",
            "Correction? (empty = accepted): That compound doesnâ€™t exist in any chemical database, so there is no structure or IUPAC name to provide.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-164152209.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "history = []\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \").strip()\n",
        "    if user_input.lower() in [\"exit\",\"quit\"]:\n",
        "        break\n",
        "\n",
        "    # Before calling generate_reply, ensure model is on the correct device if it wasn't during initial load.\n",
        "    # The root cause is the model being on CPU despite a GPU being available.\n",
        "    # The generate_reply function will be modified to send inputs to model.device\n",
        "    reply = generate_reply(user_input, history)\n",
        "    print(\"Assistant:\", reply)\n",
        "\n",
        "    # Log HITL?\n",
        "    correction = input(\"Correction? (empty = accepted): \").strip()\n",
        "    if correction:\n",
        "        save_interaction(user_input, reply, correction, reason=\"manual feedback\")\n",
        "    else:\n",
        "        save_interaction(user_input, reply)\n",
        "\n",
        "    history.append({\"role\": \"user\", \"content\": user_input})\n",
        "    history.append({\"role\": \"assistant\", \"content\": reply})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MJMjlHyP7nZ"
      },
      "source": [
        "### production API configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffsnBOw0P_ei"
      },
      "outputs": [],
      "source": [
        "# Cell 23A: Production API Configuration\n",
        "import os\n",
        "\n",
        "print(\"  PRODUCTION API CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Try environment variable first\n",
        "PRODUCTION_API_URL = os.getenv('PRODUCTION_API_URL', '')\n",
        "\n",
        "if PRODUCTION_API_URL and 'YOUR-USERNAME' not in PRODUCTION_API_URL:\n",
        "    print(f\"SUCCESS: Using environment variable\")\n",
        "    print(f\"   API URL: {PRODUCTION_API_URL}\")\n",
        "else:\n",
        "    print(\"No environment variable found\")\n",
        "    print(\"\\n   Options:\")\n",
        "    print(\"   1. Enter API URL now (for production)\")\n",
        "    print(\"   2. Press Enter to skip (local mode only)\")\n",
        "\n",
        "    user_input = input(\"\\n   API URL: \").strip()\n",
        "\n",
        "    if user_input:\n",
        "        PRODUCTION_API_URL = user_input\n",
        "        print(f\"    Set to: {PRODUCTION_API_URL}\")\n",
        "    else:\n",
        "        PRODUCTION_API_URL = \"\"\n",
        "        print(\"    Local mode (no production API)\")\n",
        "\n",
        "# Auto-detect mode\n",
        "USE_PRODUCTION_API = bool(PRODUCTION_API_URL)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"Mode: {'PRODUCTION' if USE_PRODUCTION_API else 'LOCAL'}\")\n",
        "if USE_PRODUCTION_API:\n",
        "    print(f\"API: {PRODUCTION_API_URL}\")\n",
        "else:\n",
        "    print(\"Using local feedback file only\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9QZppzbQXuV"
      },
      "source": [
        "### Download feedback from production"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17qXM7i1QbjU"
      },
      "outputs": [],
      "source": [
        "# Cell 23B: Download Feedback from Production (Run before training)\n",
        "import requests\n",
        "import json\n",
        "\n",
        "if not USE_PRODUCTION_API:\n",
        "    print(\"LOCAL MODE\")\n",
        "    print(\"   Using local feedback file (not downloading from production)\")\n",
        "    print(f\"   File: {HITL_FILE}\")\n",
        "\n",
        "    if HITL_FILE.exists():\n",
        "        with open(HITL_FILE, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        print(f\"   Interactions: {len(lines)}\")\n",
        "\n",
        "        # Count corrections\n",
        "        corrections = sum(1 for line in lines\n",
        "                         if json.loads(line).get('accepted') is False)\n",
        "        print(f\"   Corrections: {corrections}\")\n",
        "    else:\n",
        "        print(\"     File doesn't exist yet\")\n",
        "        print(\"   Run chat widget to create it\")\n",
        "\n",
        "else:\n",
        "    print(\" PRODUCTION MODE\")\n",
        "    print(f\"   Downloading feedback from: {PRODUCTION_API_URL}\")\n",
        "    print(\"\")\n",
        "\n",
        "    try:\n",
        "        # Check health first\n",
        "        print(\"   1. Checking backend health...\")\n",
        "        health_resp = requests.get(f\"{PRODUCTION_API_URL}/health\", timeout=5)\n",
        "\n",
        "        if health_resp.status_code != 200:\n",
        "            print(f\"    Backend unhealthy: {health_resp.status_code}\")\n",
        "            print(\"   Cannot download feedback\")\n",
        "        else:\n",
        "            print(\"    Backend is healthy\")\n",
        "\n",
        "            # Check correction count\n",
        "            print(\"\\n   2. Checking correction count...\")\n",
        "            count_resp = requests.get(\n",
        "                f\"{PRODUCTION_API_URL}/correction-count\",\n",
        "                timeout=10\n",
        "            )\n",
        "\n",
        "            if count_resp.status_code == 200:\n",
        "                data = count_resp.json()\n",
        "                print(f\"   Total interactions: {data['total']}\")\n",
        "                print(f\"   Corrections: {data['corrections']}\")\n",
        "                print(f\"   Ready to train: {data['ready_to_train']}\")\n",
        "\n",
        "                if data['corrections'] == 0:\n",
        "                    print(\"\\n     No corrections yet - nothing to download\")\n",
        "                    print(\"   Users need to provide feedback first\")\n",
        "\n",
        "                elif data['corrections'] < 5:\n",
        "                    print(f\"\\n    Not enough corrections yet\")\n",
        "                    print(f\"   Minimum: 5, Recommended: 20\")\n",
        "                    print(f\"   Missing: {5 - data['corrections']} more\")\n",
        "\n",
        "                else:\n",
        "                    # Download feedback\n",
        "                    print(\"\\n   3. Downloading feedback...\")\n",
        "                    download_resp = requests.get(\n",
        "                        f\"{PRODUCTION_API_URL}/download-feedback\",\n",
        "                        timeout=30\n",
        "                    )\n",
        "\n",
        "                    if download_resp.status_code == 200:\n",
        "                        feedback = download_resp.json()\n",
        "\n",
        "                        # Validate feedback\n",
        "                        if not feedback['content'].strip():\n",
        "                            print(\"     Feedback is empty\")\n",
        "                        else:\n",
        "                            # Validate JSON format\n",
        "                            try:\n",
        "                                lines = feedback['content'].strip().split('\\n')\n",
        "                                valid_count = 0\n",
        "\n",
        "                                for line in lines[:5]:\n",
        "                                    json.loads(line)\n",
        "                                    valid_count += 1\n",
        "\n",
        "                                print(f\"    Validated first {valid_count} entries\")\n",
        "\n",
        "                                # Save to file\n",
        "                                with open(HITL_FILE, 'w', encoding='utf-8') as f:\n",
        "                                    f.write(feedback['content'])\n",
        "\n",
        "                                print(f\"    Downloaded {feedback['count']} interactions\")\n",
        "                                print(f\"    Saved to: {HITL_FILE}\")\n",
        "                                print(\"\\n    Next: Run Cell 12C to prepare training data\")\n",
        "\n",
        "                            except json.JSONDecodeError as e:\n",
        "                                print(f\"    Invalid JSON format: {e}\")\n",
        "                                print(\"   Feedback file may be corrupted\")\n",
        "                    else:\n",
        "                        print(f\"    Download failed: {download_resp.status_code}\")\n",
        "            else:\n",
        "                print(f\"    Count check failed: {count_resp.status_code}\")\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(\"    Request timed out\")\n",
        "        print(\"    Backend might be sleeping (free tier)\")\n",
        "        print(\"   Try again in 30 seconds\")\n",
        "\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"    Cannot connect to backend\")\n",
        "        print(\"    Check:\")\n",
        "        print(\"   1. Is PRODUCTION_API_URL correct?\")\n",
        "        print(\"   2. Is backend Space running?\")\n",
        "        print(\"   3. Try: curl {url}/health\".replace('{url}', PRODUCTION_API_URL))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    Unexpected error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jhj4-Fur0MY"
      },
      "source": [
        "## CELL 12A: Enhanced Data Pipeline with Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm0qfui1r4NF"
      },
      "outputs": [],
      "source": [
        "# Cell 12A: Enhanced Data Preparation with Train/Test Split\n",
        "# Place AFTER Cell 11 (chat loop) - BEFORE old Cell 12\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "class EnhancedDataPreparator:\n",
        "    \"\"\"\n",
        "    Splits corrections into train/test sets for proper evaluation.\n",
        "\n",
        "    Minimum 20 corrections required:\n",
        "    - 80% (16) for training\n",
        "    - 20% (4) for evaluation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, min_corrections: int = 20):\n",
        "        self.min_corrections = min_corrections\n",
        "\n",
        "    def prepare_split_datasets(self, corrections: List[Dict]) -> tuple:\n",
        "        \"\"\"\n",
        "        Split corrections and prepare separate train/test files.\n",
        "\n",
        "        Returns:\n",
        "            (train_file_path, test_file_path, train_count, test_count)\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"\\n Data Split Analysis:\")\n",
        "        print(f\"   Total corrections: {len(corrections)}\")\n",
        "\n",
        "        # Validation\n",
        "        if len(corrections) < self.min_corrections:\n",
        "            print(f\"     WARNING: Need {self.min_corrections} corrections for reliable evaluation\")\n",
        "            print(f\"   Current: {len(corrections)} | Missing: {self.min_corrections - len(corrections)}\")\n",
        "            print(f\"\\n   Options:\")\n",
        "            print(f\"   1. Collect more corrections (RECOMMENDED)\")\n",
        "            print(f\"   2. Proceed anyway (results may be unreliable)\")\n",
        "\n",
        "            proceed = input(\"\\n   Continue anyway? (y/N): \").strip().lower()\n",
        "            if proceed != 'y':\n",
        "                return None, None, 0, 0\n",
        "\n",
        "        # 80/20 split with stratification (if you had labels)\n",
        "        # For now, random split with fixed seed for reproducibility\n",
        "        train_data, test_data = train_test_split(\n",
        "            corrections,\n",
        "            test_size=0.2,      # 20% for testing\n",
        "            random_state=42,    # Reproducible splits\n",
        "            shuffle=True        # Randomize before splitting\n",
        "        )\n",
        "\n",
        "        print(f\"\\n    Split complete:\")\n",
        "        print(f\"      Training set: {len(train_data)} examples ({len(train_data)/len(corrections)*100:.1f}%)\")\n",
        "        print(f\"      Test set: {len(test_data)} examples ({len(test_data)/len(corrections)*100:.1f}%)\")\n",
        "\n",
        "        # Prepare training file\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        train_file = BATCH_DIR / f\"train_{timestamp}.jsonl\"\n",
        "        test_file = BATCH_DIR / f\"test_{timestamp}.jsonl\"\n",
        "\n",
        "        # Save training data (CORRECT format matching your training)\n",
        "        with open(train_file, 'w', encoding='utf-8') as f:\n",
        "            for corr in train_data:\n",
        "                training_example = {\n",
        "                    'messages': [\n",
        "                        {\n",
        "                            'role': 'user',\n",
        "                            'content': corr['prompt']\n",
        "                        },\n",
        "                        {\n",
        "                            'role': 'assistant',\n",
        "                            'content': corr['response']\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "                f.write(json.dumps(training_example, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        # Save test data (CORRECT format matching your training)\n",
        "        with open(test_file, 'w') as f:\n",
        "            for corr in test_data:\n",
        "                test_example = {\n",
        "                    'messages': [\n",
        "                        {\n",
        "                            'role': 'user',\n",
        "                            'content': corr['prompt']\n",
        "                        },\n",
        "                        {\n",
        "                            'role': 'assistant',\n",
        "                            'content': corr['response']\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "                f.write(json.dumps(test_example, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        print(f\"\\n    Files saved:\")\n",
        "        print(f\"      Train: {train_file.name}\")\n",
        "        print(f\"      Test: {test_file.name}\")\n",
        "\n",
        "        return train_file, test_file, len(train_data), len(test_data)\n",
        "\n",
        "# Initialize enhanced preparator\n",
        "enhanced_prep = EnhancedDataPreparator(min_corrections=20)\n",
        "\n",
        "print(\" Enhanced data preparator ready\")\n",
        "print(\"   Minimum corrections: 20 (16 train, 4 test)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7fh6SQAv_P0"
      },
      "source": [
        "## convert HITL logs to fine-tuning dataset in JSONL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyGAzMnHwC09"
      },
      "outputs": [],
      "source": [
        "def convert_feedback_to_finetune():\n",
        "    src = HITL_FILE\n",
        "    out = FINETUNE_PREP / \"hitl_for_finetune.jsonl\"\n",
        "\n",
        "    if not Path(src).exists():\n",
        "        print(\"No feedback yet.\")\n",
        "        return None\n",
        "\n",
        "    count = 0\n",
        "    with open(src, \"r\", encoding=\"utf-8\") as f, open(out, \"w\", encoding=\"utf-8\") as out_f:\n",
        "        for line in f:\n",
        "            rec = json.loads(line)\n",
        "            if rec[\"accepted\"] is False and rec[\"user_correction\"]:\n",
        "                out_f.write(json.dumps({\n",
        "                    \"prompt\": rec[\"user_input\"],\n",
        "                    \"response\": rec[\"user_correction\"]\n",
        "                }, ensure_ascii=False) + \"\\n\")\n",
        "                count += 1\n",
        "\n",
        "    print(f\"Converted {count} corrected samples â†’ {out}\")\n",
        "    return out\n",
        "\n",
        "convert_feedback_to_finetune()\n",
        "# we ignore the accepted responses and only keep the corrections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE_EEDM-n-7E"
      },
      "source": [
        "# using custom class to orchestrate the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHurqPs3JlDB"
      },
      "outputs": [],
      "source": [
        "# Cell 12B: REPLACE OLD CELL 12 with this Enhanced Pipeline\n",
        "# This version splits data properly and tracks both train/test files\n",
        "\n",
        "def run_enhanced_data_pipeline() -> Dict:\n",
        "    \"\"\"\n",
        "    Enhanced pipeline with train/test split.\n",
        "\n",
        "    Returns dict with:\n",
        "        - status: 'success', 'no_data', or 'failed'\n",
        "        - train_dataset_path: Path to training JSONL\n",
        "        - test_dataset_path: Path to test JSONL\n",
        "        - num_train_examples: Count\n",
        "        - num_test_examples: Count\n",
        "    \"\"\"\n",
        "\n",
        "    pipeline = PipelineRun(\n",
        "        name=\"enhanced_data_preparation\",\n",
        "        log_dir=PIPELINE_LOGS\n",
        "    )\n",
        "\n",
        "    print(f\"\\n STARTING ENHANCED DATA PIPELINE\")\n",
        "    print(f\"Run ID: {pipeline.run_id}\\n\")\n",
        "\n",
        "    # Task 1: Collect feedback\n",
        "    corrections = pipeline.run_task(\n",
        "        \"collect_feedback\",\n",
        "        collect_new_feedback\n",
        "    )\n",
        "\n",
        "    if not corrections:\n",
        "        with mlflow.start_run():\n",
        "          mlflow.set_tag(\"status\", \"no_feedback\")\n",
        "          mlflow.log_metric(\"num_corrections\", 0)\n",
        "\n",
        "        pipeline.finish()\n",
        "        return {\n",
        "            \"status\": \"no_data\",\n",
        "            \"train_dataset_path\": None,\n",
        "            \"test_dataset_path\": None,\n",
        "            \"num_train_examples\": 0,\n",
        "            \"num_test_examples\": 0\n",
        "        }\n",
        "\n",
        "    # Task 2: Split and prepare datasets\n",
        "    def split_and_save():\n",
        "        return enhanced_prep.prepare_split_datasets(corrections)\n",
        "\n",
        "    train_file, test_file, train_count, test_count = pipeline.run_task(\n",
        "        \"split_train_test\",\n",
        "        split_and_save\n",
        "    )\n",
        "\n",
        "    if train_file is None:\n",
        "        pipeline.finish()\n",
        "        return {\n",
        "            \"status\": \"insufficient_data\",\n",
        "            \"train_dataset_path\": None,\n",
        "            \"test_dataset_path\": None,\n",
        "            \"num_train_examples\": 0,\n",
        "            \"num_test_examples\": 0\n",
        "        }\n",
        "\n",
        "    # Task 3: Validate both files\n",
        "    def validate_both():\n",
        "        train_valid = validate_training_dataset(train_file)\n",
        "        test_valid = validate_training_dataset(test_file)\n",
        "        return train_valid and test_valid\n",
        "\n",
        "    is_valid = pipeline.run_task(\n",
        "        \"validate_datasets\",\n",
        "        validate_both\n",
        "    )\n",
        "\n",
        "    # Finish pipeline\n",
        "    run_log = pipeline.finish()\n",
        "\n",
        "    # Mark as processed ONLY if successful\n",
        "    if is_valid and train_file and test_file:\n",
        "        correction_ids = [c['id'] for c in corrections]\n",
        "        feedback_mgr.mark_as_processed(correction_ids)\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"train_dataset_path\": str(train_file),\n",
        "            \"test_dataset_path\": str(test_file),\n",
        "            \"num_train_examples\": train_count,\n",
        "            \"num_test_examples\": test_count,\n",
        "            \"run_id\": pipeline.run_id\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            \"status\": \"failed\",\n",
        "            \"train_dataset_path\": None,\n",
        "            \"test_dataset_path\": None,\n",
        "            \"num_train_examples\": 0,\n",
        "            \"num_test_examples\": 0,\n",
        "            \"run_id\": pipeline.run_id\n",
        "        }\n",
        "\n",
        "print(\" Enhanced pipeline ready\")\n",
        "print(\"\\n To run: result = run_enhanced_data_pipeline()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iLvrN4EDGLf"
      },
      "source": [
        "diagnostic cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkX7bHrHDFxJ"
      },
      "outputs": [],
      "source": [
        "# DIAGNOSTIC CELL: Check Your Feedback Status\n",
        "# Run this to understand what's in your feedback file\n",
        "\n",
        "if 'HITL_FILE' not in dir():\n",
        "    print(\" HITL_FILE not defined\")\n",
        "    print(\"   Please run Cells 4-5 first to set up folders\")\n",
        "else:\n",
        "  import json\n",
        "  from pathlib import Path\n",
        "\n",
        "  print(\"FEEDBACK DIAGNOSTICS\")\n",
        "  print(\"=\"*70)\n",
        "\n",
        "  # Check if feedback file exists\n",
        "  if HITL_FILE.exists():\n",
        "      print(f\" Feedback file found: {HITL_FILE}\")\n",
        "      print(f\"   File size: {HITL_FILE.stat().st_size} bytes\")\n",
        "\n",
        "      # Read and analyze feedback\n",
        "      with open(HITL_FILE, 'r', encoding='utf-8') as f:\n",
        "          lines = f.readlines()\n",
        "\n",
        "      total = len(lines)\n",
        "      corrections = 0\n",
        "      accepted = 0\n",
        "      errors = 0\n",
        "\n",
        "      print(f\"\\n ANALYZING {total} INTERACTIONS...\")\n",
        "      print(\"-\"*70)\n",
        "\n",
        "      # Show first 3 interactions\n",
        "      print(\"\\n First 3 interactions:\")\n",
        "      for i, line in enumerate(lines[:3], 1):\n",
        "          try:\n",
        "              record = json.loads(line.strip())\n",
        "              print(f\"\\n   {i}. User: {record.get('user_input', 'N/A')[:60]}...\")\n",
        "              print(f\"      Bot:  {record.get('model_reply', 'N/A')[:60]}...\")\n",
        "              print(f\"      Accepted: {record.get('accepted', 'N/A')}\")\n",
        "              if not record.get('accepted', True):\n",
        "                  print(f\"      Correction: {record.get('user_correction', 'N/A')[:60]}...\")\n",
        "                  corrections += 1\n",
        "              else:\n",
        "                  accepted += 1\n",
        "          except Exception as e:\n",
        "              print(f\"   {i}.  Error parsing: {e}\")\n",
        "              errors += 1\n",
        "\n",
        "      # Count all interactions\n",
        "      corrections = 0\n",
        "      accepted = 0\n",
        "      errors = 0\n",
        "\n",
        "      for line in lines:\n",
        "          try:\n",
        "              record = json.loads(line.strip())\n",
        "              if record.get('accepted') is False:\n",
        "                  corrections += 1\n",
        "              else:\n",
        "                  accepted += 1\n",
        "          except:\n",
        "              errors += 1\n",
        "\n",
        "      # Summary\n",
        "      print(\"\\n\" + \"=\"*70)\n",
        "      print(\" SUMMARY:\")\n",
        "      print(f\"   Total interactions: {total}\")\n",
        "      print(f\"   Corrections (accepted=False): {corrections}\")\n",
        "      print(f\"   Accepted (accepted=True or None): {accepted}\")\n",
        "      print(f\"   Parse errors: {errors}\")\n",
        "\n",
        "      # Analysis\n",
        "      print(\"\\n\" + \"=\"*70)\n",
        "      print(\" ANALYSIS:\")\n",
        "\n",
        "      if corrections == 0:\n",
        "          print(\"     PROBLEM: You have NO corrections!\")\n",
        "          print(\"\")\n",
        "          print(\"   This means:\")\n",
        "          print(\"   â€¢ You chatted with the bot\")\n",
        "          print(\"   â€¢ But you pressed Enter without typing corrections\")\n",
        "          print(\"   â€¢ The chat loop treats empty input as 'accepted'\")\n",
        "          print(\"\")\n",
        "          print(\"    SOLUTION:\")\n",
        "          print(\"   1. Go back to the chat loop (Cell 11 or 23)\")\n",
        "          print(\"   2. Chat with the bot\")\n",
        "          print(\"   3. When bot gives a WRONG answer, TYPE a correction\")\n",
        "          print(\"   4. Press Enter to submit the correction\")\n",
        "          print(\"   5. Come back and run Cell 12C again\")\n",
        "\n",
        "      elif corrections < 5:\n",
        "          print(f\"     Very few corrections: {corrections}\")\n",
        "          print(f\"   You need at least 5 to train, 20 for good results\")\n",
        "          print(f\"   Missing: {max(5, 20) - corrections} more\")\n",
        "          print(\"\")\n",
        "          print(\"   Keep chatting and providing corrections!\")\n",
        "\n",
        "      elif corrections < 20:\n",
        "          print(f\"    You have {corrections} corrections\")\n",
        "          print(f\"   This is enough to train, but more is better\")\n",
        "          print(f\"   Recommended: Collect {20 - corrections} more for reliable evaluation\")\n",
        "          print(\"\")\n",
        "          print(\"    You can proceed to Cell 12C now!\")\n",
        "\n",
        "      else:\n",
        "          print(f\"    Excellent! You have {corrections} corrections\")\n",
        "          print(f\"   This is great for training!\")\n",
        "          print(\"\")\n",
        "          print(\"    Proceed to Cell 12C to prepare training data\")\n",
        "\n",
        "  else:\n",
        "      print(f\" Feedback file NOT found: {HITL_FILE}\")\n",
        "      print(\"\")\n",
        "      print(\"   This means you haven't chatted with the bot yet.\")\n",
        "      print(\"\")\n",
        "      print(\"    SOLUTION:\")\n",
        "      print(\"   1. Run the chat loop cell (Cell 11 or 23)\")\n",
        "      print(\"   2. Chat with the bot\")\n",
        "      print(\"   3. Provide corrections when needed\")\n",
        "      print(\"   4. Come back and run this diagnostic again\")\n",
        "\n",
        "  print(\"\\n\" + \"=\"*70)\n",
        "  print(\" NEXT STEPS:\")\n",
        "\n",
        "  if not HITL_FILE.exists() or corrections == 0:\n",
        "      print(\"   1. Chat with bot and provide corrections\")\n",
        "      print(\"   2. Run this diagnostic again\")\n",
        "      print(\"   3. When you have 5+ corrections, run Cell 12C\")\n",
        "      print(\"   4. Then run Cell 13G to train\")\n",
        "  elif corrections < 5:\n",
        "      print(f\"   1. Collect {5 - corrections} more corrections (minimum)\")\n",
        "      print(\"   2. Run this diagnostic again\")\n",
        "      print(\"   3. When ready, run Cell 12C\")\n",
        "      print(\"   4. Then run Cell 13G to train\")\n",
        "  else:\n",
        "      print(\"   1. Run Cell 12C to prepare training data\")\n",
        "      print(\"   2. Check that result['status'] == 'success'\")\n",
        "      print(\"   3. Run Cell 13G to train and evaluate\")\n",
        "      print(\"   4. Check MLflow for results\")\n",
        "\n",
        "  print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOxLh9vGsL8b"
      },
      "outputs": [],
      "source": [
        "# Cell 12C: Execute Enhanced Pipeline (FIXED)\n",
        "# Place AFTER Cell 12B\n",
        "\n",
        "print(\"ðŸ” Checking for corrections...\")\n",
        "\n",
        "# Check available corrections\n",
        "available_corrections = feedback_mgr.get_new_corrections()\n",
        "print(f\" Available corrections: {len(available_corrections)}\")\n",
        "\n",
        "if len(available_corrections) == 0:\n",
        "    print(\"\\n  No corrections found!\")\n",
        "    print(\"   Instructions:\")\n",
        "    print(\"   1. Go back to Cell 11 (chat loop)\")\n",
        "    print(\"   2. Chat with the bot\")\n",
        "    print(\"   3. When it gives wrong answers, provide corrections\")\n",
        "    print(\"   4. Come back here and run this cell again\")\n",
        "\n",
        "    # FIX: Set result even when there are no corrections\n",
        "    result = {\n",
        "        \"status\": \"no_data\",\n",
        "        \"train_dataset_path\": None,\n",
        "        \"test_dataset_path\": None,\n",
        "        \"num_train_examples\": 0,\n",
        "        \"num_test_examples\": 0\n",
        "    }\n",
        "\n",
        "elif len(available_corrections) < 20:\n",
        "    print(f\"\\n  Only {len(available_corrections)} corrections\")\n",
        "    print(f\"   Recommended: Collect at least 20 for reliable evaluation\")\n",
        "    print(f\"   Missing: {20 - len(available_corrections)} more corrections\")\n",
        "    print(\"\\n   You can proceed, but results may be unreliable\")\n",
        "\n",
        "    # Run anyway\n",
        "    result = run_enhanced_data_pipeline()\n",
        "\n",
        "    if result[\"status\"] == \"success\":\n",
        "        print(f\"\\n Pipeline completed despite low data count\")\n",
        "        print(f\"   Training examples: {result['num_train_examples']}\")\n",
        "        print(f\"   Test examples: {result['num_test_examples']}\")\n",
        "\n",
        "else:\n",
        "    print(f\"\\n Sufficient corrections: {len(available_corrections)}\")\n",
        "    print(\"   Running pipeline...\\n\")\n",
        "\n",
        "    # Run pipeline\n",
        "    result = run_enhanced_data_pipeline()\n",
        "\n",
        "    if result[\"status\"] == \"success\":\n",
        "        print(f\"\\n SUCCESS!\")\n",
        "        print(f\"   Training data: {result['train_dataset_path']}\")\n",
        "        print(f\"   Test data: {result['test_dataset_path']}\")\n",
        "        print(f\"   Train examples: {result['num_train_examples']}\")\n",
        "        print(f\"   Test examples: {result['num_test_examples']}\")\n",
        "        print(f\"\\n Next: Scroll down to training cells\")\n",
        "    else:\n",
        "        print(f\"\\n  Pipeline status: {result['status']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCGS8J9SMJHs"
      },
      "source": [
        "## LoRA training optimized for T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2wMkYljMOky"
      },
      "outputs": [],
      "source": [
        "# Cell 13A: Training Configuration for T4 GPU\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "from typing import Optional\n",
        "import os\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 1\n",
        "gradient_accumulation_steps = 8\n",
        "learning_rate = 2e-4\n",
        "lora_alpha = 32\n",
        "lora_dropout = 0.05\n",
        "\n",
        "class LoRATrainingConfig:\n",
        "    \"\"\"\n",
        "    Training configuration optimized for Colab T4 GPU (15GB VRAM).\n",
        "\n",
        "    Key optimizations:\n",
        "    - 4-bit quantization reduces memory by ~75%\n",
        "    - Small LoRA rank (r=8) keeps adapter tiny\n",
        "    - Gradient accumulation simulates larger batch sizes\n",
        "    - Aggressive gradient checkpointing saves memory\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset_path: str, output_dir: Path):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.output_dir = output_dir\n",
        "\n",
        "        # LoRA Configuration\n",
        "        # r=8 means we're adding 8-rank decomposition matrices\n",
        "        # Lower r = less parameters = less memory = faster training\n",
        "        # But too low = model can't learn enough\n",
        "        self.lora_config = LoraConfig(\n",
        "            r=8,                          # Rank (8 is sweet spot for T4)\n",
        "            lora_alpha=lora_alpha,                # Scaling factor (typically 2-4x rank)\n",
        "            target_modules=[              # Which layers to adapt\n",
        "                \"q_proj\",                 # Query projection\n",
        "                \"k_proj\",                 # Key projection\n",
        "                \"v_proj\",                 # Value projection\n",
        "                \"o_proj\"                  # Output projection\n",
        "            ],\n",
        "            lora_dropout=lora_dropout,            # Regularization\n",
        "            bias=\"none\",                  # Don't adapt bias terms\n",
        "            task_type=TaskType.CAUSAL_LM  # Causal language modeling\n",
        "        )\n",
        "\n",
        "        # 4-bit Quantization Config\n",
        "        # This is THE trick that makes 8B models fit on T4\n",
        "        self.bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,                    # Enable 4-bit\n",
        "            bnb_4bit_quant_type=\"nf4\",           # \"Normal Float 4\" - best quality\n",
        "            bnb_4bit_compute_dtype=torch.float16, # Compute in FP16\n",
        "            bnb_4bit_use_double_quant=True       # Double quantization saves more memory\n",
        "        )\n",
        "\n",
        "        # Training Arguments\n",
        "        # These are VERY conservative to avoid OOM\n",
        "        self.training_args = TrainingArguments(\n",
        "            output_dir=str(output_dir),\n",
        "\n",
        "            # Batch size: Start tiny!\n",
        "            per_device_train_batch_size=batch_size,        # Only 1 example at a time\n",
        "            gradient_accumulation_steps=gradient_accumulation_steps,        # But accumulate 8 steps = effective batch of 8\n",
        "\n",
        "            # Epochs: 1-2 is enough for small datasets\n",
        "            num_train_epochs=epochs,\n",
        "\n",
        "            # Learning rate: Higher than normal because LoRA needs strong signal\n",
        "            learning_rate=learning_rate,                   # 0.0002\n",
        "\n",
        "            # Memory optimizations\n",
        "            fp16=True,                            # Use half precision\n",
        "            gradient_checkpointing=True,          # Trade compute for memory\n",
        "            optim=\"paged_adamw_8bit\",            # 8-bit optimizer (saves 4GB!)\n",
        "\n",
        "            # Logging and checkpointing\n",
        "            logging_steps=5,                      # Log every 5 steps\n",
        "            save_strategy=\"epoch\",                # Save after each epoch\n",
        "            save_total_limit=2,                   # Keep only 2 checkpoints\n",
        "\n",
        "            # Misc\n",
        "            warmup_steps=10,                      # Gradual learning rate warmup\n",
        "            report_to=[],                         # Don't report to wandb/tensorboard\n",
        "            remove_unused_columns=False,          # Keep all columns for debugging\n",
        "        )\n",
        "\n",
        "    def get_memory_usage(self):\n",
        "        \"\"\"Check current GPU memory usage\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**3  # Convert to GB\n",
        "            reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "            return f\"Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\"\n",
        "        return \"CUDA not available\"\n",
        "\n",
        "# Initialize config\n",
        "training_config = None  # We'll set this in the next cell\n",
        "\n",
        "print(\"âœ“ Training configuration class defined\")\n",
        "print(\"\\n Optimizations for T4 GPU:\")\n",
        "print(\"  â€¢ 4-bit quantization (saves ~10GB)\")\n",
        "print(\"  â€¢ LoRA rank 8 (keeps adapter small)\")\n",
        "print(\"  â€¢ Batch size 1 + gradient accumulation 8\")\n",
        "print(\"  â€¢ 8-bit optimizer (saves ~4GB)\")\n",
        "print(\"  â€¢ Gradient checkpointing enabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncCC_Sf_MSpB"
      },
      "source": [
        "### Dataset tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Epzu0koMQkx"
      },
      "outputs": [],
      "source": [
        "# Cell 13B: Dataset Preparation and Tokenization (CHAT FORMAT)\n",
        "\n",
        "def prepare_training_dataset(dataset_path: str, tokenizer):\n",
        "    from datasets import load_dataset\n",
        "\n",
        "    print(f\" Loading dataset from: {dataset_path}\")\n",
        "    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
        "    print(f\"   Loaded {len(dataset)} examples\")\n",
        "\n",
        "    #  Verify format\n",
        "    print(\"\\n Sample (messages format):\")\n",
        "    print(dataset[0])\n",
        "\n",
        "    # Apply chat template\n",
        "    def apply_chat_template(example):\n",
        "        formatted = tokenizer.apply_chat_template(\n",
        "            example[\"messages\"],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        return {\"text\": formatted}\n",
        "\n",
        "    print(\"\\n Applying chat template...\")\n",
        "    dataset = dataset.map(\n",
        "        apply_chat_template,\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "\n",
        "    # Tokenization\n",
        "    def tokenize(example):\n",
        "        encoded = tokenizer(\n",
        "            example[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=None\n",
        "        )\n",
        "        encoded[\"labels\"] = encoded[\"input_ids\"].copy()\n",
        "        return encoded\n",
        "\n",
        "    print(\" Tokenizing...\")\n",
        "    tokenized_dataset = dataset.map(\n",
        "        tokenize,\n",
        "        batched=True,\n",
        "        remove_columns=[\"text\"],\n",
        "        desc=\"Tokenizing\"\n",
        "    )\n",
        "\n",
        "    print(f\" Tokenized {len(tokenized_dataset)} examples\")\n",
        "    return tokenized_dataset\n",
        "\n",
        "print(\"âœ“ Chat-format dataset preparation function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMjqrcMfMbf8"
      },
      "source": [
        "### Main training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sP6F2f84MWHR"
      },
      "outputs": [],
      "source": [
        "# Cell 13C: Main Training Function (FIXED)\n",
        "\n",
        "def train_lora_adapter(\n",
        "    dataset_path: str,\n",
        "    output_name: str = \"llama-lora-adapter\",\n",
        "    push_to_hub: bool = True,\n",
        "    hub_repo_name: Optional[str] = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a LoRA adapter on corrected examples.\n",
        "\n",
        "    Args:\n",
        "        dataset_path: Path to prepared JSONL dataset\n",
        "        output_name: Local directory name for saving\n",
        "        push_to_hub: Whether to push to Hugging Face Hub\n",
        "        hub_repo_name: HF repo name (e.g., \"yourusername/model-name\")\n",
        "\n",
        "    Returns:\n",
        "        Path to trained adapter\n",
        "    \"\"\"\n",
        "    global lora_training_loss\n",
        "    global training_config\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" STARTING LORA TRAINING\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Setup output directory\n",
        "    adapter_output_dir = WORKDIR / output_name\n",
        "    adapter_output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # Initialize config\n",
        "\n",
        "    training_config = LoRATrainingConfig(\n",
        "        dataset_path=dataset_path,\n",
        "        output_dir=adapter_output_dir\n",
        "    )\n",
        "\n",
        "    print(f\"\\n Initial GPU memory: {training_config.get_memory_usage()}\")\n",
        "\n",
        "    # Step 1: Load tokenizer (already loaded, but ensure padding token)\n",
        "    print(\"\\n Setting up tokenizer...\")\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(f\"   Pad token: {tokenizer.pad_token}\")\n",
        "\n",
        "    # Step 2: Load and tokenize dataset\n",
        "    print(\"\\n Preparing dataset...\")\n",
        "    train_dataset = prepare_training_dataset(dataset_path, tokenizer)\n",
        "\n",
        "    if len(train_dataset) < 5:\n",
        "        print(\"\\n  WARNING: Less than 5 examples!\")\n",
        "        print(\"   Training might not improve the model meaningfully.\")\n",
        "        print(\"   Recommended: Collect at least 20 corrections.\")\n",
        "        proceed = input(\"\\n   Continue anyway? (y/N): \").strip().lower()\n",
        "        if proceed != 'y':\n",
        "            print(\" Training cancelled\")\n",
        "            return None\n",
        "\n",
        "    # Step 3: Prepare model for training\n",
        "    print(\"\\n Preparing model for LoRA training...\")\n",
        "    print(\"   This will take 2-3 minutes...\")\n",
        "\n",
        "    print(\"\\n Clearing GPU memory...\")\n",
        "    import gc\n",
        "\n",
        "    # Clear cache first\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"   Memory freed: {torch.cuda.memory_allocated() / 1024**3:.2f}GB\")\n",
        "\n",
        "    # Prepare model for training (model is still on GPU)\n",
        "    model_for_training = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # Ensure model is on GPU\n",
        "    if torch.cuda.is_available():\n",
        "        model_for_training = model_for_training.to('cuda')\n",
        "\n",
        "    print(f\"   GPU memory after prep: {training_config.get_memory_usage()}\")\n",
        "    print(f\"   Model device: {next(model_for_training.parameters()).device}\")\n",
        "\n",
        "    # Step 4: Add LoRA adapters\n",
        "    print(\"\\n Adding LoRA adapters...\")\n",
        "    lora_model = get_peft_model(model_for_training, training_config.lora_config)\n",
        "\n",
        "    # Print trainable parameters\n",
        "    trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in lora_model.parameters())\n",
        "    trainable_percent = 100 * trainable_params / total_params\n",
        "\n",
        "    print(f\"   Trainable params: {trainable_params:,} ({trainable_percent:.2f}%)\")\n",
        "    print(f\"   Total params: {total_params:,}\")\n",
        "    print(f\"   Memory: {training_config.get_memory_usage()}\")\n",
        "\n",
        "    # Step 5: Create trainer\n",
        "    print(\"\\n  Creating trainer...\")\n",
        "\n",
        "    # Data collator handles batching and padding\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False  # We're doing causal LM, not masked LM\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=lora_model,\n",
        "        args=training_config.training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # Step 6: Train\n",
        "    print(\"\\n Starting training...\")\n",
        "    print(\"   This will take 10-20 minutes depending on dataset size.\")\n",
        "    print(\"   Watch the loss - it should decrease.\\n\")\n",
        "\n",
        "    try:\n",
        "        #  FIX: Train first then save loss\n",
        "        train_result = trainer.train()\n",
        "\n",
        "        print(\"\\n TRAINING COMPLETED!\")\n",
        "        print(f\"   Final loss: {train_result.training_loss:.4f}\")\n",
        "\n",
        "        #  Save loss AFTER training (when train_result exists)\n",
        "        lora_training_loss = train_result.training_loss\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        if \"out of memory\" in str(e):\n",
        "            print(\"\\n OUT OF MEMORY ERROR!\")\n",
        "            print(\"\\n Try these fixes:\")\n",
        "            print(\"   1. Reduce per_device_train_batch_size to 1 (if not already)\")\n",
        "            print(\"   2. Increase gradient_accumulation_steps to 16\")\n",
        "            print(\"   3. Reduce max_length to 256 in tokenization\")\n",
        "            print(\"   4. Use a smaller LoRA rank (r=4)\")\n",
        "            print(\"\\n   Restart runtime and try again.\")\n",
        "\n",
        "            # Set default loss value on error\n",
        "            lora_training_loss = float('inf')\n",
        "            return None\n",
        "        else:\n",
        "            raise e\n",
        "\n",
        "    # Step 7: Save adapter\n",
        "    print(f\"\\n Saving adapter to {adapter_output_dir}...\")\n",
        "    lora_model.save_pretrained(adapter_output_dir)\n",
        "    tokenizer.save_pretrained(adapter_output_dir)\n",
        "\n",
        "    print(\"âœ“ Adapter saved locally\")\n",
        "\n",
        "    # Step 8: Push to Hugging Face Hub (optional)\n",
        "    if push_to_hub:\n",
        "        if hub_repo_name is None:\n",
        "            print(\"\\n  No hub_repo_name provided. Skipping push to HF Hub.\")\n",
        "            print(\"   To push later, run:\")\n",
        "            print(f\"   huggingface-cli upload {hub_repo_name} {adapter_output_dir}\")\n",
        "        else:\n",
        "            print(f\"\\n  Pushing to Hugging Face Hub: {hub_repo_name}\")\n",
        "            try:\n",
        "                lora_model.push_to_hub(hub_repo_name)\n",
        "                tokenizer.push_to_hub(hub_repo_name)\n",
        "                print(f\" Pushed to https://huggingface.co/{hub_repo_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Push failed: {e}\")\n",
        "                print(\"   You can push manually later using huggingface-cli\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" TRAINING COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Adapter location: {adapter_output_dir}\")\n",
        "    print(f\"Memory used: {training_config.get_memory_usage()}\")\n",
        "\n",
        "    return adapter_output_dir\n",
        "\n",
        "print(\" Training function defined\")\n",
        "print(\"\\n Ready to train! See Cell 13D for execution.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k6AT-MIJiOs"
      },
      "source": [
        "####T4 GPU memory optimizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX3xf1oiJmBt"
      },
      "outputs": [],
      "source": [
        "# Cell 12D: T4 Memory Optimization\n",
        "def optimize_for_t4():\n",
        "    \"\"\"Aggressive memory optimization for T4 GPU (15GB).\"\"\"\n",
        "    import gc\n",
        "\n",
        "    print(\" Optimizing for T4 GPU...\")\n",
        "\n",
        "    # Clear cache\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Set memory allocation strategy\n",
        "    torch.cuda.set_per_process_memory_fraction(0.95)  # Use 95% max\n",
        "\n",
        "    # Enable TF32 for faster training (if supported)\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    print(f\"Optimized - Available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "\n",
        "optimize_for_t4()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygnVa8fFsX6B"
      },
      "source": [
        "## Multi-Metric Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItiCAImnsbp-"
      },
      "outputs": [],
      "source": [
        "# Cell 13E: Multi-Metric Model Evaluator\n",
        "\n",
        "!pip install -q sentence-transformers rouge-score nltk\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rouge_score import rouge_scorer\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"\n",
        "    Multi-metric evaluation for fine-tuned models.\n",
        "\n",
        "    Metrics:\n",
        "    1. Semantic Similarity (embedding distance)\n",
        "    2. ROUGE-L (word overlap with order)\n",
        "    3. BLEU (n-gram precision)\n",
        "    4. Exact Match (strict correctness)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\" Loading evaluation models...\")\n",
        "\n",
        "        # Lightweight embedding model (22MB)\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # ROUGE scorer\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "        # BLEU smoothing (handles edge cases)\n",
        "        self.smoothing = SmoothingFunction()\n",
        "\n",
        "        print(\" Evaluation models loaded\")\n",
        "\n",
        "    def generate_response(self, model, tokenizer, prompt: str) -> str:\n",
        "      \"\"\"Generate using SAME format as training.\"\"\"\n",
        "      messages = [{'role': 'user', 'content': prompt}]\n",
        "\n",
        "      # Apply chat template (same as training)\n",
        "      formatted = tokenizer.apply_chat_template(\n",
        "          messages,\n",
        "          tokenize=False,\n",
        "          add_generation_prompt=True  # Add assistant prompt\n",
        "      )\n",
        "\n",
        "      inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          outputs = model.generate(\n",
        "              **inputs,\n",
        "              max_new_tokens=128,\n",
        "              temperature=0.7,\n",
        "              do_sample=True,\n",
        "              pad_token_id=tokenizer.pad_token_id\n",
        "          )\n",
        "\n",
        "      # Decode only new tokens\n",
        "      response = tokenizer.decode(\n",
        "          outputs[0][inputs['input_ids'].shape[1]:],\n",
        "          skip_special_tokens=True\n",
        "      )\n",
        "\n",
        "      return response.strip()\n",
        "\n",
        "    def calculate_metrics(self, generated: str, ground_truth: str) -> Dict[str, float]:\n",
        "        \"\"\"Calculate all metrics for one example.\"\"\"\n",
        "\n",
        "        # 1. Semantic Similarity (embedding-based)\n",
        "        embeddings = self.embedding_model.encode([generated, ground_truth])\n",
        "        semantic_sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "\n",
        "        # 2. ROUGE-L (word overlap with longest common subsequence)\n",
        "        rouge_scores = self.rouge_scorer.score(ground_truth, generated)\n",
        "        rouge_l = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "        # 3. BLEU (n-gram precision)\n",
        "        reference = [ground_truth.split()]\n",
        "        candidate = generated.split()\n",
        "        bleu_score = sentence_bleu(\n",
        "            reference,\n",
        "            candidate,\n",
        "            smoothing_function=self.smoothing.method1\n",
        "        )\n",
        "\n",
        "        # 4. Exact Match (binary - exact correctness)\n",
        "        exact_match = int(generated.strip().lower() == ground_truth.strip().lower())\n",
        "\n",
        "        return {\n",
        "            'semantic_similarity': float(semantic_sim),\n",
        "            'rouge_l': float(rouge_l),\n",
        "            'bleu': float(bleu_score),\n",
        "            'exact_match': exact_match\n",
        "        }\n",
        "\n",
        "    def evaluate_model(\n",
        "        self,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        test_file: Path,\n",
        "        model_name: str = \"Model\"\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate model on test set.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with averaged metrics and per-example details\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\" EVALUATING: {model_name}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Test file: {test_file.name}\\n\")\n",
        "\n",
        "        # Load test data\n",
        "        test_examples = []\n",
        "        with open(test_file, 'r') as f:\n",
        "            for line in f:\n",
        "                test_examples.append(json.loads(line))\n",
        "\n",
        "        print(f\"Test examples: {len(test_examples)}\\n\")\n",
        "\n",
        "        # Collect metrics\n",
        "        all_metrics = {\n",
        "            'semantic_similarity': [],\n",
        "            'rouge_l': [],\n",
        "            'bleu': [],\n",
        "            'exact_match': []\n",
        "        }\n",
        "\n",
        "        detailed_results = []\n",
        "\n",
        "        for i, example in enumerate(test_examples, 1):\n",
        "            # Extract from messages format\n",
        "            messages = example['messages']\n",
        "            prompt = next(m['content'] for m in messages if m['role'] == 'user')\n",
        "            ground_truth = next(m['content'] for m in messages if m['role'] == 'assistant')\n",
        "\n",
        "            # Generate\n",
        "            print(f\"[{i}/{len(test_examples)}] Generating...\", end=\" \")\n",
        "            generated = self.generate_response(model, tokenizer, prompt)\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics = self.calculate_metrics(generated, ground_truth)\n",
        "\n",
        "            # Store\n",
        "            for key, value in metrics.items():\n",
        "                all_metrics[key].append(value)\n",
        "\n",
        "            detailed_results.append({\n",
        "                'prompt': prompt,\n",
        "                'generated': generated,\n",
        "                'ground_truth': ground_truth,\n",
        "                'metrics': metrics\n",
        "            })\n",
        "\n",
        "            # Print summary\n",
        "            print(f\"Semantic: {metrics['semantic_similarity']:.3f} | ROUGE-L: {metrics['rouge_l']:.3f}\")\n",
        "\n",
        "        # Calculate averages\n",
        "        avg_metrics = {key: np.mean(values) for key, values in all_metrics.items()}\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\" SUMMARY: {model_name}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Semantic Similarity: {avg_metrics['semantic_similarity']:.4f}\")\n",
        "        print(f\"ROUGE-L:            {avg_metrics['rouge_l']:.4f}\")\n",
        "        print(f\"BLEU:               {avg_metrics['bleu']:.4f}\")\n",
        "        print(f\"Exact Match:        {avg_metrics['exact_match']:.4f} ({int(avg_metrics['exact_match']*len(test_examples))}/{len(test_examples)})\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        return {\n",
        "            'average_metrics': avg_metrics,\n",
        "            'detailed_results': detailed_results,\n",
        "            'num_test_examples': len(test_examples)\n",
        "        }\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = ModelEvaluator()\n",
        "\n",
        "print(\"\\n Multi-metric evaluator ready\")\n",
        "print(\"   Metrics: Semantic Similarity, ROUGE-L, BLEU, Exact Match\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd7wLAsGAuus"
      },
      "source": [
        "DAGSHUB preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Naze8bueAws-"
      },
      "outputs": [],
      "source": [
        "# DagsHub Authentication\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Option 1: Use Colab Secrets\n",
        "try:\n",
        "    DAGSHUB_USERNAME = userdata.get('DAGSHUB_USERNAME')\n",
        "    DAGSHUB_TOKEN = userdata.get('DAGSHUB_TOKEN')\n",
        "    print(\" Using Colab secrets for DagsHub authentication\")\n",
        "except:\n",
        "    # Option 2: Manual input\n",
        "    print(\"  Colab secrets not found, using manual input\")\n",
        "    DAGSHUB_USERNAME = input(\"Enter DagsHub username: \")\n",
        "    DAGSHUB_TOKEN = input(\"Enter DagsHub token: \")\n",
        "\n",
        "# Set environment variables for MLflow\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = DAGSHUB_USERNAME\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = DAGSHUB_TOKEN\n",
        "\n",
        "print(f\" Authenticated as: {DAGSHUB_USERNAME}\")\n",
        "print(\" Ready to log to DagsHub MLflow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z89Ljq6_siPp"
      },
      "source": [
        "## MLFlow integration with Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDzCvsjjstfu"
      },
      "outputs": [],
      "source": [
        "# Cell 13F: MLflow Integration with Proper Evaluation\n",
        "!pip install mlflow\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import numpy as np\n",
        "\n",
        "def convert_to_serializable(obj):\n",
        "    \"\"\"Convert numpy types and bools to JSON-serializable types.\"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_serializable(item) for item in obj]\n",
        "    elif isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, bool):\n",
        "        return int(obj)\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# Configure MLflow\n",
        "mlflow.set_experiment('Llama-3.2-3B-Personalized-Chatbot')\n",
        "mlflow.set_tracking_uri('https://dagshub.com/PierreRamez/PersonalChatbot.mlflow')\n",
        "\n",
        "def train_and_evaluate_with_mlflow(\n",
        "    train_dataset_path: str,\n",
        "    test_dataset_path: str,\n",
        "    output_name: str = \"llama-lora-adapter\",\n",
        "    push_to_hub: bool = True,\n",
        "    hub_repo_name: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Complete training + evaluation pipeline with MLflow tracking.\n",
        "\n",
        "    This is the CORRECT way to integrate MLflow:\n",
        "    1. Start run BEFORE training\n",
        "    2. Log params, train, evaluate\n",
        "    3. Log all metrics and artifacts\n",
        "    4. Conditionally register model based on quality\n",
        "    \"\"\"\n",
        "\n",
        "    # Validate inputs\n",
        "    train_path = Path(train_dataset_path)\n",
        "    test_path = Path(test_dataset_path)\n",
        "\n",
        "    if not train_path.exists() or not test_path.exists():\n",
        "        print(\" Train or test file missing!\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" TRAINING WITH MLFLOW TRACKING\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Start MLflow run (to wrap everything)\n",
        "    with mlflow.start_run() as run:\n",
        "        run_id = run.info.run_id\n",
        "        print(f\"\\n MLflow Run ID: {run_id}\")\n",
        "        print(f\" Track at: https://dagshub.com/PierreRamez/PersonalChatbot.mlflow\")\n",
        "\n",
        "        # ===== PHASE 1: LOG HYPERPARAMETERS =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\" PHASE 1: Logging Hyperparameters\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        mlflow.log_param(\"base_model\", MODEL)\n",
        "        mlflow.log_param(\"epochs\", epochs)\n",
        "        mlflow.log_param(\"batch_size\", batch_size)\n",
        "        mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "        mlflow.log_param(\"lora_rank\", 8)\n",
        "        mlflow.log_param(\"lora_alpha\", lora_alpha)\n",
        "        mlflow.log_param(\"lora_dropout\", lora_dropout)\n",
        "        mlflow.log_param(\"gradient_accumulation_steps\", gradient_accumulation_steps)\n",
        "        mlflow.log_param(\"max_seq_length\", 512)\n",
        "\n",
        "        print(\" Hyperparameters logged\")\n",
        "\n",
        "        # ===== PHASE 2: TRAIN MODEL =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\" PHASE 2: Training\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        adapter_path = train_lora_adapter(\n",
        "            dataset_path=train_dataset_path,\n",
        "            output_name=output_name,\n",
        "            push_to_hub=False  # We'll push conditionally later\n",
        "        )\n",
        "\n",
        "        if adapter_path is None:\n",
        "            print(\"\\n Training failed!\")\n",
        "            mlflow.set_tag(\"status\", \"training_failed\")\n",
        "            return None\n",
        "\n",
        "        # Log training loss (from global variable set during training)\n",
        "        if 'lora_training_loss' in globals():\n",
        "            mlflow.log_metric(\"train_loss\", lora_training_loss)\n",
        "            print(f\" Training loss logged: {lora_training_loss:.4f}\")\n",
        "\n",
        "        # ===== PHASE 3: EVALUATE BASELINE =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\" PHASE 3: Evaluating Baseline Model\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Load original model (without adapter)\n",
        "        baseline_results = evaluator.evaluate_model(\n",
        "            model,  # Original model from Cell 3\n",
        "            tokenizer,\n",
        "            test_path,\n",
        "            model_name=\"Baseline (Original)\"\n",
        "        )\n",
        "\n",
        "        # Log baseline metrics\n",
        "        for metric_name, value in baseline_results['average_metrics'].items():\n",
        "            mlflow.log_metric(f\"baseline_{metric_name}\", value)\n",
        "\n",
        "        # ===== PHASE 4: EVALUATE FINE-TUNED =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\" PHASE 4: Evaluating Fine-Tuned Model\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Load fine-tuned model\n",
        "        from peft import PeftModel\n",
        "        finetuned_model = PeftModel.from_pretrained(\n",
        "            model,\n",
        "            adapter_path,\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "        finetuned_results = evaluator.evaluate_model(\n",
        "            finetuned_model,\n",
        "            tokenizer,\n",
        "            test_path,\n",
        "            model_name=\"Fine-Tuned\"\n",
        "        )\n",
        "\n",
        "        # Log fine-tuned metrics\n",
        "        for metric_name, value in finetuned_results['average_metrics'].items():\n",
        "            mlflow.log_metric(f\"finetuned_{metric_name}\", value)\n",
        "\n",
        "        # ===== PHASE 5: CALCULATE IMPROVEMENT =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\" PHASE 5: Improvement Analysis\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        improvements = {}\n",
        "        for metric in baseline_results['average_metrics'].keys():\n",
        "            baseline_val = baseline_results['average_metrics'][metric]\n",
        "            finetuned_val = finetuned_results['average_metrics'][metric]\n",
        "            improvement = finetuned_val - baseline_val\n",
        "            improvements[metric] = improvement\n",
        "\n",
        "            mlflow.log_metric(f\"improvement_{metric}\", improvement)\n",
        "\n",
        "            # Print comparison\n",
        "            trend_label = \"[IMPROVED]\" if improvement > 0 else \"[DECLINED]\"\n",
        "            print(f\"{trend_label} {metric:20s}: {baseline_val:.4f} -> {finetuned_val:.4f} (Diff: {improvement:+.4f})\")\n",
        "\n",
        "        # ===== PHASE 6: QUALITY GATE =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"ðŸšª PHASE 6: Quality Gate Check\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Define thresholds\n",
        "        QUALITY_GATES = {\n",
        "            'rouge_l': 0.05,  # Must improve by at least 5%\n",
        "            'semantic_similarity': 0.05,  # Must improve by at least 5%\n",
        "        }\n",
        "\n",
        "        # Check if model passes all gates\n",
        "        gates_passed = {}\n",
        "        for metric, threshold in QUALITY_GATES.items():\n",
        "            passed = improvements[metric] >= threshold\n",
        "            gates_passed[metric] = passed\n",
        "\n",
        "            status = \"PASS\" if passed else \"FAIL\"\n",
        "            print(f\"{status} {metric}: {improvements[metric]:+.4f} (threshold: {threshold:+.4f})\")\n",
        "\n",
        "            mlflow.log_metric(f\"gate_{metric}_passed\", int(passed))\n",
        "\n",
        "        all_gates_passed = all(gates_passed.values())\n",
        "        mlflow.log_metric(\"all_gates_passed\", int(all_gates_passed))\n",
        "\n",
        "        # ===== PHASE 7: CONDITIONAL DEPLOYMENT =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"PHASE 7: Deployment Decision\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        if all_gates_passed:\n",
        "            print(\"All quality gates PASSED!\")\n",
        "            print(\"   Model is ready for deployment\")\n",
        "\n",
        "            mlflow.set_tag(\"status\", \"approved\")\n",
        "            mlflow.set_tag(\"deployment_decision\", \"deploy\")\n",
        "\n",
        "            # Log the adapter as MLflow artifact\n",
        "            mlflow.log_artifacts(adapter_path, artifact_path=\"adapter\")\n",
        "\n",
        "            # Register model\n",
        "            model_uri = f\"runs:/{run_id}/adapter\"\n",
        "            model_version = mlflow.register_model(model_uri, \"chatbot-model-prod\")\n",
        "\n",
        "            print(f\"\\nModel registered: {model_version.name} v{model_version.version}\")\n",
        "\n",
        "            # Push to HuggingFace if requested\n",
        "            if push_to_hub and hub_repo_name:\n",
        "                print(f\"\\nPushing to HuggingFace: {hub_repo_name}\")\n",
        "                try:\n",
        "                    finetuned_model.push_to_hub(hub_repo_name)\n",
        "                    tokenizer.push_to_hub(hub_repo_name)\n",
        "                    mlflow.set_tag(\"huggingface_repo\", hub_repo_name)\n",
        "                    print(f\" SUCCESS: Pushed to https://huggingface.co/{hub_repo_name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"FAILED: HuggingFace push failed: {e}\")\n",
        "\n",
        "        else:\n",
        "            print(\"Quality gates FAILED\")\n",
        "            print(\"   Model will NOT be deployed\")\n",
        "            print(\"\\n   Failed gates:\")\n",
        "            for metric, passed in gates_passed.items():\n",
        "                if not passed:\n",
        "                    print(f\"      â€¢ {metric}: {improvements[metric]:+.4f} < {QUALITY_GATES[metric]:+.4f}\")\n",
        "\n",
        "            mlflow.set_tag(\"status\", \"rejected\")\n",
        "            mlflow.set_tag(\"deployment_decision\", \"do_not_deploy\")\n",
        "\n",
        "        # ===== PHASE 8: SAVE EVALUATION REPORT =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"PHASE 8: Saving Evaluation Report\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Create detailed report\n",
        "        report = {\n",
        "            'run_id': run_id,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'hyperparameters': {\n",
        "                'epochs': epochs,\n",
        "                'learning_rate': learning_rate,\n",
        "                'lora_rank': 8,\n",
        "                'lora_alpha': lora_alpha,\n",
        "            },\n",
        "            'baseline_metrics': baseline_results['average_metrics'],\n",
        "            'finetuned_metrics': finetuned_results['average_metrics'],\n",
        "            'improvements': improvements,\n",
        "            'quality_gates': {\n",
        "                'passed': int(all_gates_passed),\n",
        "                'individual_results': {k: int(v) for k, v in gates_passed.items()}\n",
        "            },\n",
        "            'deployment_decision': 'deploy' if all_gates_passed else 'do_not_deploy',\n",
        "            'detailed_test_results': {\n",
        "                'baseline': baseline_results['detailed_results'],\n",
        "                'finetuned': finetuned_results['detailed_results']\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save report\n",
        "        report_path = WORKDIR / f\"evaluation_report_{run_id}.json\"\n",
        "        with open(report_path, 'w') as f:\n",
        "            json.dump(convert_to_serializable(report), f, indent=2)\n",
        "\n",
        "        mlflow.log_artifact(report_path)\n",
        "        print(f\"Report saved: {report_path.name}\")\n",
        "\n",
        "        # ===== FINAL SUMMARY =====\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"PIPELINE COMPLETE\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Run ID: {run_id}\")\n",
        "        print(f\"Status: {'APPROVED' if all_gates_passed else 'REJECTED'}\")\n",
        "        print(f\"Decision: {'Deploy to Production' if all_gates_passed else 'Do Not Deploy'}\")\n",
        "        print(f\"\\nView results: https://dagshub.com/PierreRamez/PersonalChatbot.mlflow#/experiments/{run_id}\")\n",
        "\n",
        "        return {\n",
        "            'run_id': run_id,\n",
        "            'status': 'approved' if all_gates_passed else 'rejected',\n",
        "            'adapter_path': adapter_path if all_gates_passed else None,\n",
        "            'improvements': improvements,\n",
        "            'gates_passed': gates_passed\n",
        "        }\n",
        "\n",
        "print(\"MLflow training pipeline ready\")\n",
        "print(\"\\n This function:\")\n",
        "print(\"   â€¢ Trains the model\")\n",
        "print(\"   â€¢ Evaluates baseline vs fine-tuned\")\n",
        "print(\"   â€¢ Applies quality gates\")\n",
        "print(\"   â€¢ Conditionally registers model\")\n",
        "print(\"   â€¢ Generates detailed report\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z07dA386s8zT"
      },
      "source": [
        "### execute the complete pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhIW4_bfs8BK"
      },
      "outputs": [],
      "source": [
        "# Cell 13G: Execute Complete Training + Evaluation Pipeline\n",
        "\n",
        "# Check if we have data from Cell 12C\n",
        "if 'result' not in globals() or result['status'] != 'success':\n",
        "    print(\"No training data available!\")\n",
        "    print(\"\\nPlease run Cell 12C first to prepare data\")\n",
        "else:\n",
        "    print(\"Pipeline Configuration:\")\n",
        "    print(f\"   Training examples: {result['num_train_examples']}\")\n",
        "    print(f\"   Test examples: {result['num_test_examples']}\")\n",
        "    print(f\"   Train file: {Path(result['train_dataset_path']).name}\")\n",
        "    print(f\"   Test file: {Path(result['test_dataset_path']).name}\")\n",
        "\n",
        "    # Ask for HuggingFace push\n",
        "    print(\"\\n  Push to Hugging Face Hub after approval?\")\n",
        "    push = input(\"   (y/N): \").strip().lower() == 'y'\n",
        "\n",
        "    hub_repo = None\n",
        "    if push:\n",
        "        hub_repo = input(\"   Enter repo name (e.g., 'username/model-name'): \").strip()\n",
        "        if not hub_repo:\n",
        "            print(\"     No repo name - will skip HuggingFace push\")\n",
        "            push = False\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\" STARTING COMPLETE PIPELINE\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"This will:\")\n",
        "    print(\"   1. Train LoRA adapter\")\n",
        "    print(\"   2. Evaluate baseline model\")\n",
        "    print(\"   3. Evaluate fine-tuned model\")\n",
        "    print(\"   4. Check quality gates\")\n",
        "    print(\"   5. Conditionally deploy\")\n",
        "    print(f\"\\n   Expected time: {result['num_train_examples'] * 2 * 0.5:.0f}-{result['num_train_examples'] * 2:.0f} minutes\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    confirm = input(\"\\n  Start pipeline? (y/N): \").strip().lower()\n",
        "\n",
        "    if confirm == 'y':\n",
        "        # Clear GPU memory\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # RUN THE COMPLETE PIPELINE\n",
        "        final_result = train_and_evaluate_with_mlflow(\n",
        "            train_dataset_path=result['train_dataset_path'],\n",
        "            test_dataset_path=result['test_dataset_path'],\n",
        "            output_name=\"llama-lora-adapter\",\n",
        "            push_to_hub=push,\n",
        "            hub_repo_name=hub_repo if push else None\n",
        "        )\n",
        "\n",
        "        if final_result:\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(\" PIPELINE EXECUTION COMPLETE\")\n",
        "            print(f\"{'='*70}\")\n",
        "            print(f\"Status: {final_result['status'].upper()}\")\n",
        "            print(f\"Run ID: {final_result['run_id']}\")\n",
        "\n",
        "            if final_result['status'] == 'approved':\n",
        "                print(f\"\\n Model APPROVED for production!\")\n",
        "                print(f\"   Adapter saved: {final_result['adapter_path']}\")\n",
        "                print(f\"\\n Next steps:\")\n",
        "                print(f\"   1. Test the deployed model\")\n",
        "                print(f\"   2. Update your production inference\")\n",
        "                print(f\"   3. Monitor performance in production\")\n",
        "            else:\n",
        "                print(f\"\\n  Model REJECTED - did not meet quality standards\")\n",
        "                print(f\"\\n   Recommendations:\")\n",
        "                print(f\"   1. Collect more diverse corrections\")\n",
        "                print(f\"   2. Adjust hyperparameters\")\n",
        "                print(f\"   3. Review failed quality gates above\")\n",
        "    else:\n",
        "        print(\"\\nPipeline cancelled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_3CJw9ar0GF"
      },
      "source": [
        "### Upload New adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU7Mqtmir2JA"
      },
      "outputs": [],
      "source": [
        "# Cell 23C: Deploy New Adapter (Run after Cell 13G)\n",
        "import requests\n",
        "\n",
        "# Check if there's an approved model\n",
        "if 'final_result' not in globals():\n",
        "    print(\"No training result found\")\n",
        "    print(\"Run Cell 13G first to train a model\")\n",
        "elif not final_result:\n",
        "    print(\"Training result is None (training may have failed)\")\n",
        "elif final_result.get('status') != 'approved':\n",
        "    print(f\"Model status: {final_result.get('status')}\")\n",
        "    if final_result.get('status') == 'rejected':\n",
        "        print(\"   Model did not pass quality gates\")\n",
        "        print(\"   Collect more diverse corrections and retrain\")\n",
        "else:\n",
        "    # Model approved!\n",
        "    print(\"Model APPROVED for deployment!\")\n",
        "    print(f\"Run ID: {final_result.get('run_id')}\")\n",
        "    print(f\"Adapter: {final_result.get('adapter_path')}\")\n",
        "\n",
        "    # Check production API\n",
        "    if 'USE_PRODUCTION_API' not in globals():\n",
        "        print(\"\\n USE_PRODUCTION_API not defined\")\n",
        "        print(\"Run Cell 23A first\")\n",
        "    elif not USE_PRODUCTION_API:\n",
        "        print(\"\\nProduction API not enabled (USE_PRODUCTION_API = False)\")\n",
        "        print(\"\\n   To deploy:\")\n",
        "        print(\"   1. Deploy backend to HuggingFace Spaces\")\n",
        "        print(\"   2. Set USE_PRODUCTION_API = True in Cell 23A\")\n",
        "        print(\"   3. Run this cell again\")\n",
        "        print(\"\\n   Or manually update your backend:\")\n",
        "        print(\"   â€¢ Go to your backend Space\")\n",
        "        print(\"   â€¢ Edit app.py\")\n",
        "        print(\"   â€¢ Update adapter_path in startup_event()\")\n",
        "    else:\n",
        "        # Production API enabled\n",
        "        adapter_name = input(\"\\nEnter HF Hub adapter name (e.g., 'username/adapter-v2'): \").strip()\n",
        "\n",
        "        if not adapter_name:\n",
        "            print(\"No adapter name provided - skipping deployment\")\n",
        "        else:\n",
        "            print(f\"\\nDeploying adapter: {adapter_name}\")\n",
        "            print(\"   This will hot-reload your production backend...\\n\")\n",
        "\n",
        "            try:\n",
        "                # Hot reload\n",
        "                reload_resp = requests.post(\n",
        "                    f\"{PRODUCTION_API_URL}/reload-adapter\",\n",
        "                    json={\"adapter_path\": adapter_name},\n",
        "                    timeout=60\n",
        "                )\n",
        "\n",
        "                if reload_resp.status_code == 200:\n",
        "                    result_data = reload_resp.json()\n",
        "                    print(\"Production backend updated successfully!\")\n",
        "                    print(f\"Current adapter: {result_data.get('adapter')}\")\n",
        "                    print(\"New model is now serving requests\")\n",
        "\n",
        "                    # Ask to clear feedback\n",
        "                    print(\"\\n Clear production feedback to start fresh cycle?\")\n",
        "                    clear = input(\"   This deletes old corrections (y/N): \").strip()\n",
        "\n",
        "                    if clear.lower() == 'y':\n",
        "                        try:\n",
        "                            clear_resp = requests.post(\n",
        "                                f\"{PRODUCTION_API_URL}/clear-feedback\",\n",
        "                                timeout=10\n",
        "                            )\n",
        "                            if clear_resp.status_code == 200:\n",
        "                                print(\"Production feedback cleared\")\n",
        "                                print(\"Ready to collect new corrections\")\n",
        "                            else:\n",
        "                                print(f\"Clear failed: {clear_resp.status_code}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error clearing feedback: {e}\")\n",
        "                    else:\n",
        "                        print(\"Keeping existing feedback\")\n",
        "\n",
        "                else:\n",
        "                    print(f\"Reload failed: {reload_resp.status_code}\")\n",
        "                    print(f\"Response: {reload_resp.text}\")\n",
        "                    print(\"\\nManual deployment:\")\n",
        "                    print(\"   1. Go to your backend Space on HuggingFace\")\n",
        "                    print(\"   2. Edit app.py\")\n",
        "                    print(f\"   3. Set adapter_path='{adapter_name}'\")\n",
        "                    print(\"   4. Commit changes\")\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                print(\"Request timed out\")\n",
        "                print(\"Backend might be reloading - check Space logs\")\n",
        "            except requests.exceptions.ConnectionError:\n",
        "                print(\"Cannot connect to backend\")\n",
        "                print(\"Check if PRODUCTION_API_URL is correct\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlZxM7vFMq7o"
      },
      "source": [
        "##Test the Trained Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i4IQWTpMne2"
      },
      "outputs": [],
      "source": [
        "# Cell 14: Test Trained Adapter (FIXED for Messages Format)\n",
        "\n",
        "def test_adapter_inference(adapter_path: Path, test_messages: list):\n",
        "    \"\"\"\n",
        "    Load the trained adapter and test it on a conversation.\n",
        "\n",
        "    This shows you if the training actually improved the model.\n",
        "\n",
        "    Args:\n",
        "        adapter_path: Path to the LoRA adapter\n",
        "        test_messages: List of messages in format [{\"role\": \"user\", \"content\": \"...\"}]\n",
        "    \"\"\"\n",
        "    from peft import PeftModel\n",
        "\n",
        "    print(\"\\nLoading adapter for testing...\")\n",
        "\n",
        "    # Load adapter on top of base model\n",
        "    test_model = PeftModel.from_pretrained(\n",
        "        model,  # Base model from Cell 3\n",
        "        adapter_path,\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    print(\"SUCCESS: Adapter loaded\")\n",
        "\n",
        "    # Format using chat template\n",
        "    print(f\"\\nTest conversation:\")\n",
        "    for msg in test_messages:\n",
        "        role_label = \"USER\" if msg[\"role\"] == \"user\" else \"ASSISTANT\"\n",
        "        print(f\"   {role_label} {msg['role'].title()}: {msg['content']}\")\n",
        "\n",
        "    print(\"\\nðŸ¤– Generating response...\\n\")\n",
        "\n",
        "    # Use chat template\n",
        "    try:\n",
        "        formatted_prompt = tokenizer.apply_chat_template(\n",
        "            test_messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "    except:\n",
        "        # Fallback if chat template not available\n",
        "        formatted_prompt = \"\"\n",
        "        for msg in test_messages:\n",
        "            if msg[\"role\"] == \"user\":\n",
        "                formatted_prompt += f\"User: {msg['content']}\\n\"\n",
        "            else:\n",
        "                formatted_prompt += f\"Assistant: {msg['content']}\\n\"\n",
        "        formatted_prompt += \"Assistant:\"\n",
        "\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = test_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    # Decode full response\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract just the new response (after the prompt)\n",
        "    response_text = full_response[len(formatted_prompt):].strip()\n",
        "\n",
        "    print(\"ðŸ¤– Response:\", response_text)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    return response_text\n",
        "\n",
        "\n",
        "# Check if adapter exists\n",
        "adapter_path = WORKDIR / \"llama-lora-adapter\"\n",
        "\n",
        "if not adapter_path.exists():\n",
        "    print(\"No adapter found. Train first using Cell 13G.\")\n",
        "else:\n",
        "    print(\"Adapter found!\")\n",
        "    print(f\"Location: {adapter_path}\")\n",
        "\n",
        "    # Check if we have test data from Cell 12C\n",
        "    if 'result' in globals() and result.get('test_dataset_path'):\n",
        "        test_file = Path(result['test_dataset_path'])\n",
        "\n",
        "        if test_file.exists():\n",
        "            print(f\"\\nLoading test example from your corrections...\")\n",
        "\n",
        "            # Read first test example\n",
        "            with open(test_file, 'r') as f:\n",
        "                first_example = json.loads(f.readline())\n",
        "\n",
        "            # Extract messages\n",
        "            if 'messages' in first_example:\n",
        "                messages = first_example['messages']\n",
        "\n",
        "                # Split into input (all but last) and expected output (last)\n",
        "                input_messages = [msg for msg in messages if msg['role'] == 'user']\n",
        "                expected_outputs = [msg for msg in messages if msg['role'] == 'assistant']\n",
        "\n",
        "                # Show what we're testing\n",
        "                print(\"\\nTest case from your corrections:\")\n",
        "                print(\"=\"*70)\n",
        "\n",
        "                for i, msg in enumerate(messages[:-1], 1):\n",
        "                    role_label = \"USER\" if msg[\"role\"] == \"user\" else \"ASSISTANT\"\n",
        "                    print(f\"{i}. {role_label} {msg['role'].title()}: {msg['content'][:80]}...\")\n",
        "\n",
        "                if expected_outputs:\n",
        "                    print(f\"\\nExpected output (what you corrected to):\")\n",
        "                    print(f\"   {expected_outputs[-1]['content'][:150]}...\")\n",
        "\n",
        "                print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "                # Test with just the user messages (let model generate assistant response)\n",
        "                test_messages = messages[:-1]  # All but the last message\n",
        "\n",
        "                # Run inference\n",
        "                result_text = test_adapter_inference(adapter_path, test_messages)\n",
        "\n",
        "                print(\"\\nCompare the response to your expected output.\")\n",
        "                print(\"   Does it match better than before training?\")\n",
        "\n",
        "                # Simple similarity check\n",
        "                if expected_outputs:\n",
        "                    expected = expected_outputs[-1]['content'].lower()\n",
        "                    actual = result_text.lower()\n",
        "\n",
        "                    # Very basic similarity (word overlap)\n",
        "                    expected_words = set(expected.split())\n",
        "                    actual_words = set(actual.split())\n",
        "                    overlap = expected_words & actual_words\n",
        "\n",
        "                    if len(expected_words) > 0:\n",
        "                        similarity = len(overlap) / len(expected_words)\n",
        "                        print(f\"\\nWord overlap: {similarity*100:.1f}%\")\n",
        "\n",
        "                        if similarity > 0.5:\n",
        "                            print(\"Good match!\")\n",
        "                        elif similarity > 0.3:\n",
        "                            print(\"Partial match\")\n",
        "                        else:\n",
        "                            print(\"Low match - may need more training\")\n",
        "            else:\n",
        "                print(\"Unexpected data format in test file\")\n",
        "        else:\n",
        "            print(f\"Test file not found: {test_file}\")\n",
        "            print(\"\\nRun Cell 12C first to prepare test data\")\n",
        "    else:\n",
        "        print(\"\\nNo test data available\")\n",
        "        print(\"   Run Cell 12C first to prepare train/test datasets\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"You can also test manually:\")\n",
        "    print(\"test_adapter_inference(\")\n",
        "    print(\"adapter_path,\")\n",
        "    print(\"[{'role': 'user', 'content': 'Your question here'}]\")\n",
        "    print(\")\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BMrJ979Mv6q"
      },
      "source": [
        "### Load Adapter for production use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AZc8vOYMyg_"
      },
      "outputs": [],
      "source": [
        "# Cell 15: Load Adapter for Production Inference\n",
        "\n",
        "from peft import PeftModel\n",
        "\n",
        "def load_model_with_adapter(base_model, adapter_path: Path):\n",
        "    \"\"\"\n",
        "    Load the adapter for production use.\n",
        "\n",
        "    This replaces your original model with the fine-tuned version.\n",
        "    \"\"\"\n",
        "    print(f\"Loading adapter from {adapter_path}...\")\n",
        "\n",
        "    adapted_model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        adapter_path,\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # Merge adapter into base model for faster inference (optional)\n",
        "    print(\"Merging adapter with base model...\")\n",
        "    merged_model = adapted_model.merge_and_unload()\n",
        "\n",
        "    print(\"Model updated with your corrections!\")\n",
        "    return merged_model\n",
        "\n",
        "# Load adapter if exists\n",
        "adapter_path = WORKDIR / \"llama-lora-adapter\"\n",
        "\n",
        "if adapter_path.exists():\n",
        "    print(\"Adapter found! Loading into model...\")\n",
        "\n",
        "    # Update the global 'model' variable\n",
        "    model = load_model_with_adapter(model, adapter_path)\n",
        "\n",
        "    print(\"\\nPRODUCTION MODEL UPDATED!\")\n",
        "    print(\"   Your chat loop (Cell 11) now uses the fine-tuned model.\")\n",
        "    print(\"   Go back to Cell 11 and test it!\")\n",
        "else:\n",
        "    print(\"No adapter found. Train first (Cell 13D).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5Vn9Ck087am3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cde79def-9d04-472f-c042-1b9bd1ccfe94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¡ Testing connection to: https://pierreramez-chatbot-api.hf.space...\n",
            "\n",
            "âœ… SUCCESS! Connected to backend.\n",
            "   Status: healthy\n",
            "   Device: cpu\n",
            "   Current Model: pierreramez/Llama-3.2-3B-Instruct-bnb-4bit_finetuned\n",
            "\n",
            "ðŸ“Š Production Stats:\n",
            "   Total Interactions: 2\n",
            "   Corrections available: 0\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "PRODUCTION_API_URL  = \"https://pierreramez-chatbot-api.hf.space\"\n",
        "print(f\"Testing connection to: {PRODUCTION_API_URL}...\")\n",
        "\n",
        "try:\n",
        "    # 1. Check Health (Basic connectivity)\n",
        "    health_resp = requests.get(f\"{PRODUCTION_API_URL}/health\", timeout=15)\n",
        "\n",
        "    if health_resp.status_code == 200:\n",
        "        data = health_resp.json()\n",
        "        print(\"\\nSUCCESS! Connected to backend.\")\n",
        "        print(f\"   Status: {data.get('status')}\")\n",
        "        print(f\"   Device: {data.get('device')}\")\n",
        "        print(f\"   Current Model: {data.get('current_adapter') or 'Base Model'}\")\n",
        "\n",
        "        # 2. Check Feedback Endpoint (Permissions check)\n",
        "        count_resp = requests.get(f\"{PRODUCTION_API_URL}/correction-count\", timeout=10)\n",
        "        if count_resp.status_code == 200:\n",
        "            count_data = count_resp.json()\n",
        "            print(f\"\\nProduction Stats:\")\n",
        "            print(f\"Total Interactions: {count_data.get('total')}\")\n",
        "            print(f\"Corrections available: {count_data.get('corrections')}\")\n",
        "        else:\n",
        "            print(f\"\\nConnected, but '/correction-count' failed: {count_resp.status_code}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"\\nBackend is running but returned error: {health_resp.status_code}\")\n",
        "        print(health_resp.text)\n",
        "\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\"\\nConnection Failed.\")\n",
        "    print(\"1. Check if the URL in the previous cell is correct.\")\n",
        "    print(\"2. Go to your Space and ensure it shows 'Running' (Green badge).\")\n",
        "    print(\"3. If it says 'Building' or 'Runtime Error', check the Space Logs.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nUnexpected Error: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}